{"text": " Hello everybody and welcome back to the second tutorial session of I2DM. My name is Francesca and hopefully the last week's tutorial session gave you a good overview of an electress structure and got your familiar with our communication platform, Piazza, and our submission system on our website. Since our first submission exercise will start next week, in case you have any problems left, please reach out on Piazza to our teaching assistants to get some help so that you're good to go next week for the first exercise submission. Our program today will be a math recap and I will give you an overview of the most important mathematical concepts that will be needed in our lecture. For those students of you with a strong mathematical background, this tutorial session won't be too exciting, but for us it's important to get all of you to the same level and also to give you an overview of the mathematical concepts that will be needed. We also uploaded an exercise sheet with five exercises on our website. These exercises will give you the opportunity to practice your understanding but we highly recommend you to work on the exercises this week as we choose them closely to the content on the lecture. So all of the exercises at some point in the future will come back to you. So a good understanding of them will definitely help you in our lecture. We didn't include so far any solutions but we will do so next week. This also gives you the opportunity to work on the exercises on your own this week and compare your own work to the solution next week. Okay, so this was everything from the organization aside so let us start with the math recap. Okay, before we now dive into the mathematics, I first of all want to give you a short overview of how this tutorial session is structured. So we subdivided it into three parts. The first one being linear algebra, the second calculus, and the third one probability theory. So I summarized the contents that we will have a look at and let me just go through it with you together. So linear algebra, we will have a look at vector and matrices and at the most important basic operations on them. So that won't be too difficult. Also, we will introduce tensils which are essentially just a generalization of matrices and which are very useful in the setting of deep learning and computer vision. Finally, I will talk about norms and loss functions which will be used heavily in our lecture. So that's also good to see them again. The second part is calculus. So there we will mainly talk about derivatives. We start with a scale at the derivative and then we will go up to the higher dimensions. So we will look at gradients and at the Jacobian matrix. Finally, in calculus, we also want to have a deeper look at the chain world which will play a very important role in the setting of deep learning and new networks. So that's important as well. The third part is probability theory. So there we will start with the main concepts and we will have a look at their definition. So probability spaces, random variables, and also the corresponding distribution functions. Furthermore, we also review the concept of mean invariances and we will give you a short overview of the most important probability distributions of the random variable. For example, the Gaussian distribution or the uniform distribution. So let us have a look at the first part, linear algebra. So before we give the definition, I want to have a look at the basic notations here before we go further. So when we talk about a vector, we talk about an element of Rn mostly. So it's a vector. It's just a two-wheel-width n elements. In our case, we're looking at Rn. Of course, the vector is not always an element of Rn. But in our case, we are just restricting ourselves to Rn here. So when we choose a vector v in Rn, we can refer to the i-th element by using the index. So the e-th element of our vector v in Rn is then given by v in the index i, which is then an element in R, of course. Equivalent D. Matrix is an element now of the set R to the power of n times n. So it's a matrix with n rows and m columns. And here, if we take arbitrary matrix A and our set R to the power of n times m, we denote an element in this matrix. For example, in the i-th row and the j-th column by the element a, a, a, i, j, where i and j is in the index. And this element, as well as before in the vector, is an element in R. OK, so this is the notation that we will use. Another notation that I want to shortly review is the transpose of a matrix. And the transpose of a matrix essentially is just flipping columns and rows. So you just flip them one around. And then we denote a transpose matrix, which has been the matrix A. We denote it by A to the power of t, which is referring to transpose. And when the matrix A is an element of R to the power n times m, then the transpose matrix, as we flip rows and columns, is then an element of the set R to the power m times n. So we flip also the dimensions. And this concept of transpose is used similarly for vectors. So you can also flip rows and columns in a vector. So a vector like this will just be kind of a lying vector. And you don't maybe have a vector in n or R to the power n times n. But rather in the element when the set R to the power one time times n. So yeah, you can also transpose vectors is what I want to say here. OK, so vectors, let's have a look at that. As I said, a vector is an element in an n dimensional space. And now, OK, we look at the space Rn. And you can just denote it by a tupper with n elements. And the elements are now given by v1, v2, up to vn. And the figure here is showing a three-dimensional space. And now, OK, for example, R3, where each of the little pictures that you see is one vector in our three-dimensional space R3. And for vectors, we can define four basic operations. And these are given by addition, subtraction, scalar multiplication, and the dot product. And we will have a short view over these operations. So addition of two vectors. So we start by taking two vectors arbitrary. So a and b are two elements of Rn. And then you find the addition of these two vectors by the tupper, where you component by add the elements of the vector a and b. So the first element is given by a1 plus b1. The last element is given by a and plus bn. And of course, the result of the addition of two vectors in Rn is again an element in Rn. And the figure and the lower right of this slide shows you the geometric interpretation of what does it mean to add two vectors in the space Rn. So here, for example, we have the vectors a and b. And addition of these vectors geometrically means that we just concatenate. So first of all, we take the vector of a. And then after that, we concatenate the vector of b, which then results in the red vector here given by a plus b. And you can also do it the other way around, start by taking the vector b. And then concatenate the vector a, which gives you exactly the same. And because these are exactly the same, we can define or we can just see that the addition is a co-communative operation. Okay, so let us have a look at subtraction. The subtraction operation is exactly the same as the addition, but we're just going to replace the plus sign by a minus sign. So it's nothing special. So again, we start by a and b being two vectors in Rn, and then the subtraction of these two vectors a minus b is defined by the vector when you subtract component vice. So the first element then or the first element in the vector a minus b is given by a one minus b one. And the last element is given by a and minus bn. So the resulting vector is an element in Rn again. If you look at the figure on the lower right, you can see again the geometric interpretation. And here you have like a blue the vectors a and b. And then red you then have denoted or represented the vector a minus b, which is connecting the ends of the vector a and b. Okay. Scala multiplication. This is maybe more interesting than addition and subtraction, but I think it's also pretty simple. So here we don't start having two vectors in the beginning a and b, but we only take one vector a and Rn. And then on top of that, we take a scalar c and R, which will then be used to multiply the vector by this scalar c. So the scalar multiplication is defined by c times a and then it's given by the vector by each component or each element in the vector a is multiplied by this scalar c. So the first element in our vector c times a is given by c times a one and the last element is given by c times a and and the result of scalar multiplication of a scalar with the vector in Rn is again an element in Rn. And the figure here on the lower right of the slide again gives you a geometric interpretation of what the scalar multiplication is doing to a vector and essentially that just means that we can stretch our vector a. So we can stretch it by the scalar c. If you have a scalar which is positive and bigger than one, then we stretch it and make the vector longer. If you have a scalar which is between 0 and 1, you can just make the vector smaller or shorter. And if you now choose a negative scalar, you can also reverse the direction of the vector. So this is geometrically speaking what scalar multiplication is. Okay, so finally let's have a look at the dot product which is maybe the most important operation of all of these. Okay, so for the dot product again we start with two vectors. So taking a and b as elements from Rn and then the dot product is defined as you just use the multiplication side for a times b. But essentially it is like the multiplication between the vector, the transposed vector of a and the vector b. And in this case that just means that I take component bias to product the elements of the two vectors. And then I take the sum over all of the entries of this resulting component bias multiplication of vector a and b. So the result of the dot product is given here in the second and third line. So you can just see it's the sum over a i times b i. So the sum over component bias multiplication of the elements of the vector a and b. Which is important to mention is now that the result of the dot product of two vectors in Rn is not any more element in Rn itself. But now we obtain an element in R so scalar. So it's important to remember for the dot product. The dot product now has some interesting properties. So the first one is that the dot product is a commutative operation. That means that if I take two vectors a and b, then the dot product of a and b so a times b equals to b times a. So the dot product between b and a. This is interesting. We also have that for addition and the scalar multiplication as well. So all of them are commutative. Another one is the geometric interpretation, which is in this case it is a bit more complicated than the cases before. So let's have a look at the figure that I included on this slide. There you see two vectors x and y and we usually denote that the vectors by a and b so that's kind of the same. And you see that between these vectors x and y you can define the angle theta between these two vectors. So the dot product is now in relation to this angle and it's given by the second line geometric interpretation that I gave a formula which is given by a times b. So the dot product of a and b is equal to the length of a times the length of b times cosine of theta. So cosine of the angle between our two vectors a and b. So this is a property and interpretation of the dot product which is really interesting because it refers to this more complex concept of the angle between two vectors. You can never look at orthogonal vectors. What does it mean to be orthogonal for two vectors? Two vectors are orthogonal if the angle between them is exactly 90 degrees. So we have a right angle like this. And one property in relation to our dot product is now that two non zero vectors are orthogonal to each other. So if I know the dot product is zero, I can conclude that they are orthogonal to each other and the other way around. So this is a nice interpretation geometrically of the dot product. Next slide, I included a figure where you can nicely see as well this interpretation with the angle between two vectors. I just start the animation and you can see that if the angle between two vectors is bigger than 90 degrees, you obtain a negative dot product. So the angle is exactly 90 degrees than our dot product is zero. And if the angle is smaller than 90 degrees, then we have a positive dot product. So this is something which you should know about the dot product which is quite interesting. So let's continue with matrices. As I mentioned before, matrix is an element of the set or the space R to the power n times m. So we have noted with the matrix notation, which is essentially matrix, including all the elements starting with the element a one one up to the element a and m. And on the matrix, we can define also basic operations. In our case, we will have a look at matrix vector multiplication matrix matrix multiplication and the hardmer product. And I think the first two are well known from all of you, the harder mud product is something which will not often define a mathematical lecture, but it's a basic operation, which is really heavily used in the I to the setting, and which is also not very difficult. So we also have a look at this one. Let's start with the matrix vector multiplication. The name is already mentioning or saying we have here a multiplication between the matrix and the vector. So we start with a matrix a and we take it from a set R to the power n times m and a vector from the set R to the power m. And now the multiplication between this matrix and this vector like a times b is given in this formula that I included in our slide here. So we have our matrix a we have our vector b and essentially what we are doing is we lay our we have our matrix and we lay our vector on top of the matrix. And what we then do is we do kind of a dot product with each row of the matrix. So the first element of the result, the first component, the first row. And it's the dot product of the first row of the matrix a together with the vector b. And then you continue doing that the last rows, then the dot product between the last row of the matrix a together with a vector b. And of course, as you see here the outcome of the matrix vector multiplication between a and b is here an element of R and so has dimension n. And this is important if you do matrix vector multiplication and also all the other multiplications on matrices, it's important that you always take care of the dimension. And you have to be fitting to each other because otherwise it's not well defined. So in our case, like you see here on the second point, it's important to make sure that in our case a is of dimension n times m and our vector has to be of dimension m times one. And just the dimension, the second dimension of our matrix has to be equal to the first dimension of our vector, otherwise it's not defined. And this is easily seen by the definition of the matrix vector of multiplication. And as I said before, the outcome is now an element, a vector of dimension n times one, so a vector of dimension n. And the lower part of the slide, you can see an example which uses a matrix of dimension three times one and a vector of dimension two. And then you get the matrix multiplication delivers an element of dimension three. And here you can just test your understanding of this multiplication by just doing this exercise on your own and comparing your solution to the solution down here. And then we just continue with the matrix matrix multiplication. And as the name the same, this is just a generalization of the multiplication that we saw before of the matrix of the vector multiplication. In this case, we don't take a vector B, but we take a second matrix and then we multiply multiply the two matrices with these data. And then we multiply the two matrices with the matrix A, which is from the set R to the power n times m and B, which is an element of the set R to the power m times L. Again, have a look at the dimensions here because that's important. And the multiplication is now given by the multiplication between A and B, between these two matrices, where each element in the outcome, the outcome is again a matrix of dimension n times L. And each element in the outcome of this multiplication is essentially just the dot product between one row in the matrix A and the column in the matrix B. So if you have a look at the second line of equations here on the slide, C ij, which is just one element in the resulting matrix is now the dot product between the Ith row of the matrix A and the J's column of the matrix B. So that's the definition. And then you just build up this whole resulting matrix with n elements or n times L elements. And that gives you then the matrix matrix multiplication result. And again, I also want to point at the dimensions again, we started here with an element A from dimension n times m and B has dimension m times L. And that's important for this multiplication that the second dimension of the matrix A equals the first dimension of the matrix B. Because otherwise you can just see it by the definition that multiplication wouldn't be identified. So always make sure that you check the dimensions and we also have an exercise which is pointing to this dimension problem of the multiplication. And one last thing before I mentioned that the operations that we have been looking at for the vectors are commutative in this case matrix matrix multiplication is not a commutative operation. So you're not allowed to just switch the order of the multiplication and say that A times B is the same than B times A because that's here not the case. So that's important to say it's always pretty easy if you have something which is commutative in this case matrix matrix multiplication and also matrix vector multiplication is not commutative. So last but not least, the other part of the product I've been talking about that in the introduction of the basic operation on matrices is a really easy operation because it's not using the product in the multiplication, but it's just the component wise multiplication of two matrices. So we start with two matrices A and B in this case now we have the same dimension for both of the matrices. So both of them are an element of the set R to the power n times M and what we then do we define this heart am I product which is denoted by a multiplication sign and the circle around it. So we say the result of the heart am I product is a matrix where we just have component wise multiplication. So if you look at the first entry at space 1 1 you can just see that the entry in resulting matrix is A 1 1 times B 1 1 and then you do that component wise for all the entries in the matrices. So it's giving you a matrix of dimension n times M and this is the heart am I product so just component wise multiplication. And I think that's maybe the most simple operations on matrices, but as I said, it's highly important for our I2DL lecture. So make sure that you know the difference between other multiplication on matrices and always make sure that the dimensions are fitting because that's kind of the most important thing when you work with matrices that you always take care of the dimensions and that they fit together. As I promised before we also want to look at tensors. Then those are essentially a multi-dimensional array and they are a generalization of the concept of vectors and matrices. So on the figure that I included on this slide you can just see the relationship to scalars, vectors and matrices. So let's have a look at scalars first. The scalar is just an element from a set. In our case we look at the set R so the set of really numbers so scalar is just one of the real numbers that we choose arbitrarily. The vector now can be seen either as a real vector or column vector. And this is essentially just a generalization of the concept of a scalar because we just concatenate several scalars in one structure. Here's the structure of vectors. The same is happening for matrix. A matrix is a generalization of the concept of vectors because it concatenates or includes combines several vectors in one structure. So now here we not only have one array but we have like in a matrix rows and columns which are combining several vectors. And this is then the same step that we are going from matrix to tensor. A tensor is a generalization of a matrix. So it's just combining concatenating several matrices in one structure. So what we see here on the image you see that we are not only having rows and columns but we also start having channels. Channels are describing the depth of our structure here. So in this case we have three rows, three columns and three channels describing the depth. So let's have a look where we can use these and where we're going to use it in it to the end. So tensors are really common to use in computer vision and it's really easy to just say one application because each image that we want to process is essentially a tensor. Images are often represented by RGB images. RGB stands for red, green and blue. So we are kind of including the information of the red colors, the green colors and the blue colors and three different challenges over the image over the pixels. So one image can be described as a dimension of age times W times RGB which is standing for three in this case because we have three channels, RGB. And on the image here or on the figure that is included on this slide you can just see it again. We have this funny picture of a cat which is then described as a tensor where we have like the size of the image is made and with. And then we have the channels red, green and blue. And in these channels we can then include all the information on the red colors, green colors and blue colors. So this is where tensors are an application and because images are represented there tensors they are really, really important for the whole setting of I2D. Because when we process an image in a neural network of course we have to work with tensors as well. So just have in mind that each image is represented as a tensor and the tensors just a generalization of the matrix. Okay, so last but not least I want to talk about norms and laws functions. So a norm mathematically seen you can describe it as the measure of the length of a vector. And I included the definition here so norm is a non negative function which is always denoted by these vertical lines which is time going from a vector space which we do in T by V into the space R of the real numbers. And it's non negative as I said. So we always have a positive outcome or a zero outcome. And this function has to has three properties. If it satisfies these properties then we can call it an R. And these properties probably you all of you have already seen them is first of all a very important one is the triangle inequality. So when we take two vectors V and W and we take the addition of these two so V plus W and we take the norm of this. This result has to be smaller or equal to the norm of V plus the norm of W. So this is called the triangle inequality. The second property has now something to do with the scalar multiplication that we have been looking at for the vectors. In this case a is a scalar so an element of R in our case and V is a vector so an element of R to the power of N for example. When we now take the scalar multiplication a times V and then the norm of this element it should be the same then a times the norm of V for every scalar a and R. And the third property is now that the norm of a vector equals to zero if and only if this vector is equal to zero so it is the zero vector. So this is also property for a norm so we are not allowed to assign the value of zero to a vector which is non zero. Yeah and I just included some of the definition information we here is a vector space over field F in our case we just define V as R to the power of N and then it's a vector field about the real order over the field of the real numbers. And as a remark which is important to say on one space like in our case R to the power of N you can define several measurements several norms. So in our case what we want to have a look at on the next two slides is the L1 norm and the L2 norm. These are two norms both defined on Rn but assigning different numbers to each vector but both of them are norm just a different norm. And as I said in the definition of a norm like every function every non negative function that we can define with these three properties define norms of course there are several possibilities. So let us have a look at the first one of the L1 norm. Okay the L1 norm is the first example that we want to have a look at. This one is a norm that one norm is a norm on the space Rn and it's defined by a function which we denote here by this sign which is used for a norm like these vertical lines and then we use the subscript 1 like an index 1 and this function is then going from Rn to R. And it's a non negative function and it is defined as follows for each vector V which is denoted here is the two part of elements we want to Vn and we assign the number which is given here and we just take the absolute value of all the components of the vector and then we sum over these values. And this gives us the L1 norm of a vector V. So this is the definition of the L1 norm and let's have a look at one simple example just to make sure that we understand the concept. So we take now an arbitrary or not arbitrary in this case we take a vector V which is defined as an element from R3 and it has the components 1 minus 3 and 2. And remember we want to calculate the L1 norm which just need to use the definition that we gave above. So it's just the summation over 3 elements in this case 1, 3 and 2. So we just take the absolute values of all the components so in this case minus 3 gets 3 because we take the absolute value and 1 and 2 stay the same and then we just sum over them which gives us the result of 6. So the L1 norm of this vector is equal to 6. So let us have a look at another norm, the second possibility that we can also define on this space of Rn. So in this case we denoted with the subscript of 2 index of 2 and it's again a function from Rn to R like we gave in the definition. And now the definition of this norm of this L2 norm is that it's behaving for a vector V again denoted by this tuple of elements of V1 to Vn and by taking every component to the power of 2 then take the summation over all of them and finally take the square root of this number. So this is giving us the L2 norm of this vector V and again we're going to have a look at the example. We take the same example then this like before. So again we have this vector which is defined by a vector 1 minus 3 and 2 as an element of R3 and then we calculate the L2 norm which is now given by the summation of 3 elements where we take all the components to the power of 2. So 1 to the power of 2 minus 3 to the power of 2 and 2 to the power of 2 and finally we take the square root. So this delivers us finally the number square root 14 which is quite different to the number that we found before to the L1. So you see that both norms are behaving differently for the vectors of our space Rn. Last but not least I want to talk about loss functions because they will play a very important role in the I to the lecture. So a loss function essentially is a function that takes us input 2 vectors and the output is then a measurement of the distance between these 2 vectors. So we can have a look here at the example for the L1 loss and the L2 loss because these are 2 losses that are referring to the L1 norm and respectively L2 norm. So the L1 loss is a loss between 2 vectors so we denote the vectors V and W as elements of Rn and then we can define the L1 loss of these 2 vectors V and W as the L1 norm of the vector V minus W. And as you remember we saw before V minus W is just describing a vector which is connecting to the 2 vectors V and W. So it's kind of describing the difference and then we take the L1 norm which is giving us the measurement of the distance between these 2 vectors but referring here to the L1 norm. The same idea we can also use it for the L2 norm for the Euclidean norm. So here again we have these 2 vectors V and W and then we define the L2 loss as L2 of V and W where we now take the L2 norm of the vector V minus W. So again the same concept but now we take another measurement near our L2 norm instead of our L1 norm. And these are 2 very famous loss functions that you will always see coming around because they are quite popular. You will also see other loss functions but I hope that you better understand the relationship between the norm and the last function which is really easy for this L1 and L2 loss. Before we now start with the second part of our tutorial session I first of all want to give a short outlook of where we need calculus and also where we need our linear algebra that we saw so far on the previous slides. So you know that we want to talk about neural networks. I2DL is about deep learning and neural network structures. And one of the famous tasks that we are looking at in deep learning has been image classification. So image classification the idea of the task is that we have an image and we want to construct a neural network program which is able to predict or to say which object is present in this frame. So in this example we see a cat. For example here we have different classes. Let's say we have 3, we have a dog, we have cat and we have deer. And we want this neural network that we see here in our circle to predict whether we see a deer, a dog or a cat in this image. Of course the answer is cat, we know that. So first of all because before we can just try to start to process our image we have to transform it in a way that that is ready to process. And this is why I introduced the definition of a tensor because I said already images can be transformed or are essentially tensors. So like the information of an image is stored in such a tensor. So an image is our tensor and this is the one or this is the structure that we are feeding into our new network. And this is shown like this. So our new network is taking a tensor which is representing our image and the outcome of our new network is now a vector or a prediction what we are looking at. Like a prediction of our new network which says to us we see this object. So as I said we are looking at 3 classes here. Cat, dog and deer. And you see here it gives us kind of a score like how lightly it is that a cat dog or deer is visible on our image. And the prediction that this new network here is now giving to us is really bad because it shows us a 70% probability that on this image we see a deer which is not true. So in this case what we now have to do is we have to train our new network. We have to show him a lot of images and for each images it gives us a prediction and we have to say him you're good or you're not good and if you're not good we have to change something in our new network. So this is what we do using loss functions. Loss functions are like we said describing the measurements between two vectors here. The one vector is the vector is the outcome of our new network so the prediction and the other one is the truth that we know we just call it ground truth labels. So for this image we know that it should be 100% cat and 0% dog and 0% deer that would be perfect. So then we just calculate the loss between the actual prediction that our new network did and the ground truth. And then we can have a look like how wrong has our new network been and the total goal of learning of the learning process is that we want to minimize the loss. We want to work or we want to train our new network so that it's working pretty good and gives us pretty good predictions. And the new network essentially what it is doing it's like it's a big structure but it's containing of different matrices and these matrices and tensors are processing our image. They are processing they are just doing matrix matrix multiplication and then we have some nonlinear functions and essentially this is just really easy mathematics that is that is happening here and the outcome is then our prediction of our classes. We call the elements of these matrices I call them here W of course we have several of them but like easily speaking we look at the matrix and the weights like the elements of this matrix are the weights of our new network. And what we now want to do in the training process we want to adjust the weights we want to make them better so that the prediction gets better and better by every training sample that the network is seeing. And now is the big question how can we get an accurate matrix W to minimize the loss and minimize is essentially the bird that we need to understand why we need calculus. The loss function has to be minimized that has something to do with the derivative of our last function of course because when we have the derivative we can define like the step the gradient we can define like in which direction do we have to change in order to minimize our loss. And this is exactly what I talked about this method which is using derivatives and the chain rule essentially because we have like large new network structures is a method to approximate the best value for the weights of our new network. And this method is called gradient descent and essentially mathematically seen it's pretty easy because it's just a derivative and the chain rule so nothing is happening more. OK, so after this motivation let's start with the second part of our tutorial session which is calculus. I said that we want to mainly talk about derivatives so scale at the relative gradients to covenants and we also want to have a deeper look at the chain rule. So derivatives in general a derivative is essentially a function which measures the sensibility of changing the function value so the output value and with respect to the input value. And it is well known for all of us and this scale at the derivative which is just a derivative of a real valued function f which is going from r to r. So this is the easy setting. If we go in a higher dimension we need to extend the calculus and also to a higher dimension setting and we call this the matrix calculus. And here we're going to look at function which are similar to the ones that we say or that are denoted here in the second point which are for example functions from r and to r. And functions from r to rn also functions from rn to rm which are even more complex and the maximum here is a function which is starting with a matrix and has as an output a value air. So function which is going from r power n times m to r. So these are more complex functions and we also want to have a look at how the derivative of these functions look like. So this is matrix calculus. And all the concepts that we saw so far in linear algebra and also that we're going to see here in the calculus part are not too difficult. But what makes it difficult later when we want to apply them to the setting of new networks and deep learning and makes it like all that we have a lot of matrices that are concatenated. So we need to use the chain rule and we also have kind of changing dimensions. So we really need to understand the basic setting of a derivative of a derivative in higher dimensions and also of all these matrix multiplication stuff. Because when you're like good to go for these things it will be easy for you to practices and to apply it in a more complex setting where we have a lot of values, a lot of variables, etc. So try to understand these concepts pretty well and then we can just apply them later in the lecture. So again in overview I said like we look at different settings the easiest setting is the real valued settings. So we have a function from r to r. We call it the derivative is just called the scalar derivative. I mean call it by the sign f prime of x. This is the thing that we did in high school already. And the setting the higher dimension is setting one example here is the fine which is going from r and to r and the derivative of such a function is called gradient and we denoted by this nub last symbol which is this triangle but just reversed of f of x. And similar when we look at a function which is taking us input a matrix and going into r so from r m to the power of n times m to r and we also call the derivative of such a function gradient and we use the same symbol this nub last symbol. And the last setting we want to have look at is a function from r and to r m so both sides we have a higher dimension of setting and here we call the derivative a Jacobian and the notation here will be j and then a subscript we have this f. So these are the settings that we're going to have a look at and we want to stop at the most easiest one scalar derivative. So this is easy and we have this real value function the notation as I said before as f prime of x or sometimes we also call it the f of the x. And the geometrical interpretation of the derivative of such a real valued function is something that we looked at already a lot and it's representing essentially at a chosen input value the slope of the tangent line to the graph of the function at that point. So we just like this red line here is describing the slope at certain input value and this is just a derivative of this function at this input value. The slope of this tangent line so this is the geometrical interpretation. On the following two slides I summarized the most important rule when it comes to derivation. So here on the first slide I gave or included the most common functions and the respective derivatives on the second slide here. I included the rules when it comes to a combination of two function. So when you have a combination like the multiplication between two functions so f of x times g for example you need to follow the product rule in order to determine the derivative. So make sure that you remind the most or the derivatives of the most common functions and also all these the relation rules when it comes to a combination of two functions because they will help you to be able to determine every possible or arbitrary and the derivative when it comes to the lecture content in the future. So let's now switch to the higher dimension setting. So first of all we want to have a look at the multivariate function f in this case we start with the setting where our function is going from our end to our on the left side on this slide you can see a visualization of such a function and in this case we start from the space R2 and for each position in R2 we assign with the function f with our multivariate function f. With our multivariate function a function value which is a real value to each of the values in R2 and the graph that you can see here is one example of a graph for such a multi-variate function. So you see that the setting is much more complicated than in the real valued world of functions. So the derivative of such a function is now given by a gradient as I mentioned before. And the notation of such a gradient is given by this NABLAS symbol which is just a reversed delta symbol and our gradient function is now going from Rn to Rn. So it's important to mention here that our output values not a real value anymore but now we have as an output a vector with N components. And on the second line you can see how the gradient is defined and it's defined like the function is defined you take X as an input which is a vector in Rn and the output is now a vector where you can see that when you look at the first component the first component is given by the partial derivative of f of x with respect to the first variable. And correspondingly you determine the second up to the end component of our output vector. So it's just a part partial derivative of our function f and then with respect to the variables x1, x2, up to xn. So that's the definition of our gradient here. Let's have a look at a more complex situation. So when we now want to have a look at a function f which is going from the space R to the power of n times m so our input is essentially a matrix and to the space R. Then our visualization would look much more complicated and it's much more difficult to visualize such a function. But we also call the corresponding derivative gradient and the notation is the same we also use the snap-less symbol and our gradient function is now going from the space R to the power of n times m to R to the power of n times m. So the output again is now not a vector but a matrix and it's pretty similar to the idea of the gradient of the function from R into R. Because here you can see that the output is a matrix and if you look at one special value of our output matrix for example again the first position 11 and you see that the value over there is the partial derivative of our function with respect to the first or to the variable x11. And then you just complete the whole matrix with the partial derivatives of all variables of our input. So x which is in this case a matrix and then you can just build up the function similar to what we saw on the slide before. So this is just the same idea just for another setting. On the following two slides we included two examples for you to better understand the concept of a gradient. So we included two functions f and g which are both functions from R to R so pretty simple functions and we determine the gradient for both of the functions. So try to understand the calculation here or maybe better even try to do the calculations on your own and then compare your work to the solutions here on the slides. Okay, so now I want to move on to the last setting that we want to talk about in this part of calculus which is the vector valued functions. So what is a vector value function? So a vector value function is defined as a function from R and to Rm in comparison to the other settings before where the input space always have been Rm or even higher and the output space has always been R. In this case now we have both in the input space and in the output space a higher dimensional space. So Rm and Rm that makes this function the most complicated one that we saw before or that we saw in general here in this tutorial session. And if you have a look at the second line you can see the definition of the function so it takes as input a vector which is here denoted by x and which is just a tuple with n elements because it's just an element of Rm. So the elements are demoted from x1 to up to xm and the output of our function f is now a vector again but this vector is an element of Rm. So it has components which are elements real values and we have M of them in a tuple. So let's have a look at the first of them. So f, the first component of our output vector is given by f1 of x. And here f1 is determining or defining another function which is going from Rm is it takes x as an input so it's going from Rm and its output value is an element of R so just a real value. And this is the setting that we saw before it's a function from Rm to R and we have these functions we have M function of them all concatenated or put in this vector and this is the output of our function f. When we now look at the derivative of our vector value function and the derivative is called the Jacobian matrix and it's denoted by j with the n-dxf and it's going from the space Rm to the space R to the power of m times n. And you can see here in the definition in the second line that it takes as an input a value or a vector x which is an element of Rm so it again consists of n elements x1 up to xn like on the left side and the output is now a matrix. And this matrix is just a generalization of the gradient that we have been looking before so look at the first row of this matrix. The first row is now given by the partial derivatives of this function f1 which has been a function from Rm to R. And the row is given by all the partial derivatives with respect to the first variable x1 up to the variable xn and this is the first row. The second row is now doing the same with the function f2 and then we go down and repeat this procedure up to the mth row where we have the partial derivatives of the f of the m's function fm. So you can see that it's just kind of combining the gradients of the functions f1 up to fm so it's kind of a generalization of the concept of the gradient. Again we included an example for you to practice your understanding and here we define the function f which is going from R2 to R2 and we define it on these slides. So just try to understand or maybe try to derive on your own the derivatives of this function and then compare your solutions to the solutions here on this slide. So before we finish the second part of our tutorial session today I want to go a little bit more into detail of the chain rule. We saw the chain rule already before when I introduced the main or the most important derivation rules but I want to go a little bit more into detail because the chain rule might be the most difficult of these rules and also it will play the most important rule in the item. The setting here. So let's have a look at it. The setting is now that we are back at a real valued function function so we have a look at a function which is going from R to R and the setting is that we have a composition of two functions. So we are given a function h of x which is defined by the composition of f and g so it is defined by f of g of x. And now we want our task is to determine the derivative of this composition of functions and this is the setting where we use the chain rule. And we have now kind of four steps that you need to follow in order to use the chain rule and to determine the correct derivative of this function. So let's look at these steps. The first step is the introduction of an intermediate variable so the intermediate variable here will be our variable u which is defined to be g of x. This is our intermediate one because it's the one inside the function of f so we just call it u then we can write our function h of x as f of u. And the second step is now that we have to derive two derivatives namely the first one is the derivative of our function f with respect to our intermediate variable u. And the second one is the derivative of our function g with respect to our variable x. And you can also call it the derivative of our function of variable u with respect to our variable x. When you did that then you can just compose these two derivatives and then we'll then give you the derivative of h with respect to the variable x. So you just compose them you just multiply them. And what you do then in the last step is you substitute the intermediate variable back to the original function so you go back from u to g of x. And we will have a look at an example because that my this might look a little bit abstract but it's not too difficult. So let's have a function here which is given by h of x is the sign of x to the power of two. So this is essentially to the composition of two functions namely the first function being the sign function and the second one is the x to the power of two. And our task here is now that we want to determine the derivative of it. So as I mentioned before we have here like this setting of the composition of two functions f and g. F is here defined as sign of x and g is defined as x to the power of two. So x squared. And again we follow our first step so the first step is to introduce our intermediate variable as we saw before. So let us define u the variable u to be the variable x squared. And the second step we now want to calculate the two derivatives first of f with respect to u and second one of g with respect to x. And you see here the derivative of f with respect to u will now be cosine of u and the derivative of g with respect to x is now two times x. And what we then have to do is we have to multiply these both derivatives. So the derivative of the function h with respect to x is now the derivative of f with respect to u times the derivative of u with respect to x. So we obtain cosine of u times two times x. And the first step now was that we want to substitute the intermediate variable back. So now we can just instead of u we substitute our x squared. So then our derivative of h with respect to x is giving us cosine of x squared times two x. And this is an example. So for how you can apply the chain rule to a setting like our setting that we have here. So I highly recommend you to understand these steps if you don't understand them yet. Just have a look at them. They are not too difficult. But this is a nice idea of how you can solve the chain. So the last slide I want to introduce the total derivative chain rule. So this is referring to a little bit more complicated setting than we saw before. I mean now we not only have a function which is a composition of two functions, but we now have a function which takes more than one input value namely x, but also u one of x until u and of x. And then we want to determine the derivative with respect to x. And this is just using or applying the chain rule to all of the different settings here. So the first part of our summation will be the derivative of f with respect to x. And then you have the summation of all different chain rules applied to all of the compositions that we have here like the composition of f with u one up to the composition of f with u n. And then you have to apply out to some all of the applications of this chain rule and then you get kind of the total derivative chain rule. So in case that we will be needing this in the application for our neural networks later, it might be interesting to have this in mind because this is really really useful when you have more than one input variables. Okay, so this has been everything for our calculus part. So let's move on to our probability part. Okay, probability theory is now forming the last and third part of our tutorial session. And I want to talk about the main concepts of probability theory beginning with the definition of probability spaces random variables and also the probability distribution functions. And finally, we want to introduce or review the concept of mean and variances. And we also want to have a look at the most important probability distribution like a normal distribution or uniform distribution. Okay, so let us start with the definition of a probability space. A probability space essentially is describing a random experiment. Always when we talk in probability theory, we are looking at the random experiment and we work on this setting. So this is kind of the basic setting that we are looking at. And for everything that we are doing, we can define this basic setting. So let us have a look how it is defined. So it's consisting essentially of three elements. The first element is our sample space and it's denoted with the omega, the big omega. And the omega is describing a set, which is representing the set of all possible outcomes of our random experiment. The second element is our event space F. Our event space F is now a set and the elements of our set are again sets. And they are subset of omega describing events. So we are looking at possible events for our random experiment. And these events are all represented in our event space F. So these are the underlying definitions. And I think this one is known to everybody. It's our probability measure, which is denoted by this P. And P is just a definition or the definition of P is it is a function, which is defined on the set F and going into the real values. So into R and it satisfies for the following three properties that we want to have a look at. So as it is giving us a probability, it is a non negative function. So on all elements of F, which are sets, it has to be bigger or equal to zero. The second property is that our probability function, when we use it on the whole set of all possible outcomes. So that's our omega, which is always an element of our set F, our event space F. Then the probability always has to be one. So this is the biggest probability that our probability measure can take and it's always defined to be one for the biggest possible set. So in different events, we call them OBD node and by A1 up to A n, which are all elements of our set F. And when we take the union of these sets and the probability of this union must be equal to the sum of the probability of each of these events. Three properties that our function has to satisfy in order to be a probability measure function. So whenever we talk from a random experiment, we're going to have this underlying set up, which is consisting of our set omega, our set F and our probability measure P. So as I said in the beginning, the probability space provides a form and model of a random experiment. So whenever you struggle to understand what is going on, try to go back and try to define the probability space and what are you looking at? Okay, so we want to have a look at one example. It's a really famous and popular example in probability theory. It's namely tossing a six sided die. And I want to look with you together. What is our probability space here? So the sample space was defined as the set of all possible outcomes. Of course, for a die, our outcomes are the numbers one, two, three, four, five, four, six. So this is our sample space. Our event space is now depending on how we want to define it. I defined here three possible event spaces and you can choose on your own depending on how you set up your random experiment, you have to define your own event space. The first one that we are looking at is our F one here. It's the smallest possible. So it's containing the empty set and it's containing whole omega. This is kind of the most simple one. Another one, which is also pretty simple is our F two, which is denoted as P of omega. And this is just a set of all possible subsets of omega. So this is the biggest one that we can have. And our F three is now an example for a third possibility, where we don't have the smallest one and not the biggest one, but we now have kind of one in between. And here you can see that the elements are defined to be the empty set, the whole set omega. And we also have two other sets, a one and a two. And one is the set of odd numbers, so including one, three and five and a two is the set of even numbers two, four and six. So as I said, the probability measure is now a function, which is going from F to R. And we have to make sure that the probability measure of our empty set equals to zero and the probability measure of our omega equals to one. And now we can have a look at the third event space, our F three. And in that case, we have more than only these two sets. We have more than the empty set and the whole set omega, but we also have to set a one and a two. What we can see is that our a one and our a two, if we take the union of these two events, we get the whole set omega. So what we know them with the third probability of our probability measure, you can just go back to the slide before if you don't remind it, you can just derive that you have to know or you have to make sure that the probability of a one plus the probability of a two must be equal to one. So this is what we know that we have to satisfy as a probability measure. And now we want to take as an example the event space F three. So the possible probability measure here, like we can look at one possibility where we define the probability of a one equal to the probability of a two, namely both being one half. So if we take the sum of both of them, we obtain one, which is then fine with the observation that we derived before another possibility would be that we define our probability measure on a one, be equal to one fourth and our probability measure on a two, be equal to three, four. So this is also adding up to one, but these are two different measure probability measures that you can define on the event space F three. So it depends on how you define your probability measure of how your random experiment looks like. And this is kind of the distribution that random variable can take on. So I said random variable. So what is a random variable now? A random variable is now a function which is defined on the probability space. So this two, two, three elements and it's a mapping from the sample space to the real numbers. So the definition now is giving our x, which is the notation for our random variable is going from our space omega into the space r. So for each element in our sample space, we assign an element in R. And for random variables in general, we distinguish between two settings namely the first one being the discrete setting and the second one being the continuous setting. So we're going to have a look at examples for discrete and continuous settings. So again, I want to talk about tossing a fair six sided die. So this is again the same example, then we saw before for our probability setting and the underlying experiment as we derived before is now that we have our omega or event space, which again is the set from the numbers of one up to six. Let me have our event space, which I now define as being the set of all possible subsets of omega. So our P of omega and we define our probability measure as P of x equals to one sticks for all elements in our sample space. So for every number that I can let our die can result in I have the probability one thing. So it's kind of a uniform distribution. This is also we choose this one because I call this example tossing a fair six sided side. So of course, we need to have come like our uniform distribution here. Otherwise, it wouldn't be a fair die. Okay, so what is now our random variable because this is defined on top of our probability space. And here we can choose for different random variables. And the first one that I want to choose here is the random variable, which represents the number that appears on the die. So our x is defined on the omega, so on our sample space and it's going into R. But in this case, it's going in the set one, two, three, four, five, six. So this is the subset of R. Of course, so that's validified. And as the subset where we're resulting in is discrete because we only have six elements and it's not continuous. We call this random variable, it is discrete random variable. So this is important for you to remember. And so one example here, it's a really simple random random variable here because it's essentially just giving the outcome of our die of our dice. So the example now is for one element in omega, for example, we take our omega equals to four, like the small like the sign which is here being equal to w is called the small omega. And if we choose the small omega one element in our big omega set, we choose it here, for example, to be equal to four. Then x of omega would be four because we just say, okay, it's kind of the identical function. We look at what our die is showing us, it's showing us the number four. So the outcome of our random variable is four. And now we defined in the first line our probability measure. So now we can see p x equals to four. This is something which we always write like p. And then we use the random variable equals to something. So here we can see p of the probability that x is equals to four is the same as p like the probability. And then we have to look at the set that I defined in there. So all the omigas in our big omega set such that x of our omega equals to four. This is essentially what x equals to four means. It's just looking at the set and we're looking at all the elements in our sample space omega such that if we put this element in our random variable x, we obtain the number four. And we know by definition of our random variable that this set only contains the number four. Because when I take the number four and I put it into my random variable, I get the number four. If I put anything else in it, I don't get the number four. So this is only a containing the number four. So it's the probability of the set which is only containing number four. And then we know that the probability now is one six. So I hope it's not too confusing with this notation, but this essentially is what we mean when we write p of the probability x equals to four. So let us have a look at another discrete example. It's also a simple example. It's flipping a fair coin two times. So now again, we have to define our probability space as I said in the beginning. So our omigas now is the set of all possible outcomes of our experiment of our random experiment. And if I flip a coin, I can get either hat or tail. And if I do it twice, I can have like the outcomes that I can have is either I get two times a hat where I get in the first flip a hat and then a tail. And the second or I get in my second flip ahead and my first one, a tail. So head tail and tail head are also two elements or I get two times tail. So this is the possible outcomes of our random variable here. Again, we choose for our event space, and p of omega, which is just all possible subsets of our omega, which is always the most simple, which is just the most simple choice for event space f. And again, as I said, flipping a fair coin two times, we need to define, otherwise we wouldn't be a fair coin to be our probability measure to be equal to p of omega equals to one fourth for all possible elements in our sample space omega. So for all possible outcomes, I have the same probability, 91 fourth because our sample space omega consists of four elements. Okay, so this is the setup for the random experiment, which I call flipping a fair coin two times. Now we can define a more complex random variable here our x is now the number of heads that appeared in the two flips. So our x is again function defined on our sample space omega into R, but now like the outcome space is only containing three elements, having zero, one and two because the outcome can be either that I don't have any head, I get one head or in my flips, I get two times ahead. So the outcome like the possible outcomes here are zero, one and two. And again, the outcome space or the target space of my random variable is again a discrete set. So again, here we are looking at a discrete random variable. So this is the second example of one, but here the random variable is a little bit more complex than in the one before. Okay, so now let's have again, let's have a look again at one specific example or outcome in our sample space. And here we choose for an outcome small omega, the element where the first flip gives us a tail and the second one gives us a head. And then we know if we put this one into our random variable, which is representing the number of heads of our outcome, we know that the outcome of our random variable would be one. So this is more complex and here you can also have a look at our probability measure that we defined up there. And again, I want to talk about what does it mean to write p of x equals to one. And here again, x equals to one is just a description for a set. And the set here is the set of all omigas in our sample space omega such that when I put this omega in my random variable, I get the number one. So I have to look in all possible outcomes in my sample space and look which one result in my random variable to the number one. And this is definitely just two of the outcomes namely the one where the first flip gives us a head and the second one a tail or the other way around the first flip gives us a tail and the second flip ahead. Because then always I just have one head and the other one is a tail. So I have here in this set, which I described in the second term, I can just define it as the set, including the elements h and t and the two and h. So this is two of the four to the probability here as we have a uniform probability is now one half. Okay, so this is another example. I really quick want to look at also a continuous example. So let us have a look at radioactive decay. So when we want to have a look at a particle, which is radioactive and we want to look at the decay of it. And the underlying experiment that we are looking at is a set omega, which is now defined to be the real valued or the set of all real values, greater or equal to zero. So the positive part of the real values. And again, we choose our event space f to be the p of omega. So all possible subsets of our omega. And we choose in general a probability measure p. This is much more complicated to define the probability measure here. So I don't do it by hand, but there are possible choices for probability measures. Okay, so now we want to have a look at the continuous random variable. So let now let's now have a look at the random variable here X, which is indicating the amount of time that it takes for radioactive particle to decay. And so the definition of our random variable is now a function X, which is going from R greater or equal to zero into R greater or equal to zero. Because the outcome only can be positive because we're talking about time. And the input is our omega. And this is also our definition of R greater or equal to zero. And this one is now as you see that the outcome space of our X is a continuous set because it's like the real values all greater or equal to zero. We have a continuous random variable. Okay, so this is one example here. The probability measure is now defined on the set of events f. And it's now used for random variable as follows. So again, we have this usage of our probability measure in relation with our X. So we can say what is the probability of our random variable being between a and b. And this again is just looking at our sample space omega big omega and it's taking all the small omigas in the sample space for which X of omega lies between a and b. And this is just giving us the probability of this set. And then it's depending on how we defined our probability measure depends on what this gives us. Okay, so this is an example of a continuous setting. And I hope that you better understood like when we talk about a random variable that we always have this underlying probability space on top or on the basis. The probability or the random variable is just defined on top of such an random experiment. Okay, after having defined the most important basic concepts of probability theory, which are including the probability space and the random variable, we now want to have a look at probability measure of the random variable. And we can specify the probability measure of a random variable with alternative functions. And we call them here cumulative density function CDF, probability mass function PMF and the probability density function. And essentially, we have to make a difference between discrete and continuous random variables. And I included a short overview over these function, which are describing our probability measure. And the first column or second column is giving us the cumulative density function. This is something which is the same for both discrete and continuous random variable. And this is a function which is denoted by a big F with a subscript x for our random variable x. And it's giving us the probability that our random variable is smaller equal to the value x. And this can be defined as I said before for discrete and continuous probability or random variables. The third column is now what makes the difference between a discrete and a continuous case. In the discrete case, you can define the probability mass function. This is now giving the probability that our random variable is taking the value x. So it's just looking at one specific value. This is not really possible for continuous case. So here we define in the continuous case the probability density function. And the probability density function is essentially just the derivative of our cumulative density function in the continuous case. In the continuous case, most of the time the CDF is differentiable, which makes it possible to define our PDF as the derivative of our CDF. Okay, so let's just shortly review the definition of these functions. I said the cumulative distribution function is for both settings the same. And it's essentially a function F of x which is defined from R into the interval between 0 and 1. And it's defined to be the probability that our random variable is smaller or equal to the value of x small x. And it also has four properties. I won't read through all of them, but you can just have a look. But essentially it just means that it always have to end in one. This is the biggest possible value that it can take and it starts at 0. Okay. So there you also have an image of a possible sample cumulative function. In the discrete case, I said that we can have a look at the probability mass function. And here the probability mass function is denoted by a small p with a subscript x for the random variable x. And it is defined to go from the sample space into R. And I said before it's just giving us the probability that our random variable in the discrete case is equal to the value of x. And again, we have some properties which I just derived here. It has to be between 0 and 1 which is natural because it just representing a probability and it also has to sum up up to 1. Okay. Now we have one example of a PMF and a CDF. So this is the discrete case if we want to have a look at the sum of two dices. So you see that the cumulative density function is summing up up to 1 and the probability mass function is essentially giving you the probability for all of the possible outcomes which are here ranging between 2 and 12. Okay. And in the continuous case, I said that we're going to have a look or we have the probability density function which is now defined as the derivative of our cumulative distribution function f of x. So it's essentially just the definition here. It's given by a small f with a subscript x for the random variable x. And it's going from omega into R. And it's just defined as the derivative of our cumulative distribution function. Again, we have like very nice properties and I also included a figure so that you can better have a look at this one. So this is describing the continuous case. And it's important to mention here that the value of a PDF so probability density function at any given point x is not the probability of that event. Because here we are in the continuous case and that makes it more more difficult. So we can't really describe it as the probability of the input value, but it's more kind of the space under the graph of giving you the probability. So this is the relationship to the cumulative distribution function. Okay, so now finally, let's have a look at the concept of expectation and variance of a random variable. So let us start with the expectation. So the idea of the expectation is that it's given or it's defining a weighted average of the values that the random variable can take on. And the weights that we are using here are just given by the probability measure function, which is corresponding to our random variable x. So I said before that we always have to make a difference between the discrete and continuous setting. This is the same here for our expectation. So first of all, we want to look at the definition in the discrete case. So let us assume that x is a discrete random variable and we have a probability mass function p of x. And now the expectation of x is defined as e from x, e of x, which is equal to the sum over all elements in our sample space omega. And over the function x times the probability mass function of x of p x of x. So here you can easily see this is the weighted average of all our elements in our sample space and the weights are given by our probability mass function. So similar in the continuous setting, the definition is quite the same. So here we assume that x is now continuous random variable and then instead of a probability mass function, we have our probability distribution function f of x. And now our expectation is defined by an integral instead of a sum. So e of x equals the integral from minus infinity to infinity over the function x times f of x where f of x is our probability distribution function. So this is like very much the same as just in the discrete setting and in the continuous setting. So now I include it and XR before expectation. And again, I included first of all the definition in our discrete case. And now we want to look at our random experiment, experiment of tossing a six sided die. So again, as we knew from the previous slides, our omigas now the set of one, two, three, four, five, and six. Because this is all the possible outcomes that our experiment has. And our x is the random variable, which represents the outcome of the toss. So just kind of the number which is shown on the die. And we know that our probability mass function here, like the probability that our random variable equals to a specific element in our omigas equals to one six because we have a fair die. So this is for all elements in our omigas sample space. And now we can just take the sum, which is defining our expectation of our random variable, which is just summing or calculating the weighted average of all possible outcomes. So the outcomes are one, two, three, four, five, and six. And the probability for all of them, so the probability mass function as we have a fair die is now one six for all possible outcomes. So if you take this sum, then you get a 3.5 as result, which definitely makes sense because we have a fair die. So it has to be the center of all possible outcomes. So this is one example for our expectation. Okay, the expectation of a random variable has two important properties that we want to have a look on this slide. And the first property is related to a constant value. So if you have a constant a, which is just an element of r, then the expectation of this constant is just the constant itself. And that makes total distance because in a constant you don't have anything random. The second property now is linearity. So the expectation, the operation of expectation is linear. So if we have two random variables here denoted by x and y, and we also have two scalars a and b, and we take a new random variable, which is a times x plus b times y, then we can take the expectation of that. And this one has to be equal to a times the expectation of x plus b times the expectation of y. And this shows you the linearity of the expectation operation. And this is very important. So as a hint, you have to use that in the exercise sheet and it will be also very important in the future in the lecture. Okay, so now we want to have a look at the variance of a random variable and the idea of the variances, roughly speaking, is just a measure how concentrated the distribution of a random variable x is around its expectation or mean. The expectation and mean is just the same. So you want to know how much how dense it is around its mean and how much it's spreading also. And the definition of a variance of the variance of a random variable is now given by this term. You just need to compute the expectation of x squared, then you need to know the expectation of x and you need to take this to the power of two. And then you just combine it and subtract it and then you get the variance of x. On the figure that I'm including here, you can see different probability measures of a normal distribution. And here the red and the blue one are both normal distributions with a mean around zero. And you can see that the red one has a variance which is much bigger than the one and the blue one. So it's much less concentrated around its mean and it's spreading more. The blue one is more dense around the mean. So here you can see kind of the difference what it is when you have a bigger variance or smaller variance. And now let's have a look also at an example of the computation of the variance of a random variable. And again, we want to have a look at the same example that we have been looking for the expectation. So tossing a fair, six sided die. So the setting is exactly the same. We have our sample space, omega, we have our random variable, which is a discrete random variable. And we already know the probability mass function, which was a uniform distribution. And we also know the expectation, which we have been computing a few slides ago. So what we now have to do is we have to compute the expectation of x squared. And then we have to combine it in order to obtain our variance of our random variable x. So I just let it to you to understand these steps, but please try to understand them and. Okay, so last but not least also the variance has some important properties that I want to name here pretty quickly. So the first one again, we're looking at a constant, which is nothing random about it. So the variance of a constant is just zero. Which makes also interpretation wise a lot of sense. And now if you look at random variable x and you take a new random variable, which is defined as a times x plus b, you obtain for the variance of this new variable that it's equal to a to the power of two times the variance of x. So the constant part in our term is just vanishing and the scalar, which is multiplied or the scalar, which is used to be multiplied to our random variable is getting outside, but you have to take it to the power of two. Okay, so again, I also included the image of our normal distributions here, which is showing you different normal distributions with different means and variances. On this final slide, we summarized the most important probability distribution and we included to discrete ones, the Bernoulli and the Vynomial distribution and to continuous one in the uniform and the normal distribution. In our lecture, the normal distribution will by far be the most important one. So make sure that you have the information in this table in mind. And with this slide, we reach the end of our second tutorial session. Thank you so much for listening and see you next week.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 5.0, "text": " Hello everybody and welcome back to the second tutorial session of I2DM.", "tokens": [2425, 2201, 293, 2928, 646, 281, 264, 1150, 7073, 5481, 295, 286, 17, 35, 44, 13], "temperature": 0.0, "avg_logprob": -0.20326267276798282, "compression_ratio": 1.6289752650176679, "no_speech_prob": 0.12249163538217545}, {"id": 1, "seek": 0, "start": 5.0, "end": 9.0, "text": " My name is Francesca and hopefully the last week's tutorial session", "tokens": [1222, 1315, 307, 31441, 496, 293, 4696, 264, 1036, 1243, 311, 7073, 5481], "temperature": 0.0, "avg_logprob": -0.20326267276798282, "compression_ratio": 1.6289752650176679, "no_speech_prob": 0.12249163538217545}, {"id": 2, "seek": 0, "start": 9.0, "end": 12.0, "text": " gave you a good overview of an electress structure", "tokens": [2729, 291, 257, 665, 12492, 295, 364, 2185, 735, 3877], "temperature": 0.0, "avg_logprob": -0.20326267276798282, "compression_ratio": 1.6289752650176679, "no_speech_prob": 0.12249163538217545}, {"id": 3, "seek": 0, "start": 12.0, "end": 15.0, "text": " and got your familiar with our communication platform, Piazza,", "tokens": [293, 658, 428, 4963, 365, 527, 6101, 3663, 11, 430, 654, 26786, 11], "temperature": 0.0, "avg_logprob": -0.20326267276798282, "compression_ratio": 1.6289752650176679, "no_speech_prob": 0.12249163538217545}, {"id": 4, "seek": 0, "start": 15.0, "end": 18.0, "text": " and our submission system on our website.", "tokens": [293, 527, 23689, 1185, 322, 527, 3144, 13], "temperature": 0.0, "avg_logprob": -0.20326267276798282, "compression_ratio": 1.6289752650176679, "no_speech_prob": 0.12249163538217545}, {"id": 5, "seek": 0, "start": 18.0, "end": 22.0, "text": " Since our first submission exercise will start next week,", "tokens": [4162, 527, 700, 23689, 5380, 486, 722, 958, 1243, 11], "temperature": 0.0, "avg_logprob": -0.20326267276798282, "compression_ratio": 1.6289752650176679, "no_speech_prob": 0.12249163538217545}, {"id": 6, "seek": 0, "start": 22.0, "end": 24.0, "text": " in case you have any problems left,", "tokens": [294, 1389, 291, 362, 604, 2740, 1411, 11], "temperature": 0.0, "avg_logprob": -0.20326267276798282, "compression_ratio": 1.6289752650176679, "no_speech_prob": 0.12249163538217545}, {"id": 7, "seek": 0, "start": 24.0, "end": 27.0, "text": " please reach out on Piazza to our teaching assistants to get some help", "tokens": [1767, 2524, 484, 322, 430, 654, 26786, 281, 527, 4571, 34949, 281, 483, 512, 854], "temperature": 0.0, "avg_logprob": -0.20326267276798282, "compression_ratio": 1.6289752650176679, "no_speech_prob": 0.12249163538217545}, {"id": 8, "seek": 2700, "start": 27.0, "end": 31.0, "text": " so that you're good to go next week for the first exercise submission.", "tokens": [370, 300, 291, 434, 665, 281, 352, 958, 1243, 337, 264, 700, 5380, 23689, 13], "temperature": 0.0, "avg_logprob": -0.07021459937095642, "compression_ratio": 1.9059233449477353, "no_speech_prob": 6.000134089845233e-05}, {"id": 9, "seek": 2700, "start": 31.0, "end": 34.0, "text": " Our program today will be a math recap", "tokens": [2621, 1461, 965, 486, 312, 257, 5221, 20928], "temperature": 0.0, "avg_logprob": -0.07021459937095642, "compression_ratio": 1.9059233449477353, "no_speech_prob": 6.000134089845233e-05}, {"id": 10, "seek": 2700, "start": 34.0, "end": 37.0, "text": " and I will give you an overview of the most important mathematical concepts", "tokens": [293, 286, 486, 976, 291, 364, 12492, 295, 264, 881, 1021, 18894, 10392], "temperature": 0.0, "avg_logprob": -0.07021459937095642, "compression_ratio": 1.9059233449477353, "no_speech_prob": 6.000134089845233e-05}, {"id": 11, "seek": 2700, "start": 37.0, "end": 39.0, "text": " that will be needed in our lecture.", "tokens": [300, 486, 312, 2978, 294, 527, 7991, 13], "temperature": 0.0, "avg_logprob": -0.07021459937095642, "compression_ratio": 1.9059233449477353, "no_speech_prob": 6.000134089845233e-05}, {"id": 12, "seek": 2700, "start": 39.0, "end": 42.0, "text": " For those students of you with a strong mathematical background,", "tokens": [1171, 729, 1731, 295, 291, 365, 257, 2068, 18894, 3678, 11], "temperature": 0.0, "avg_logprob": -0.07021459937095642, "compression_ratio": 1.9059233449477353, "no_speech_prob": 6.000134089845233e-05}, {"id": 13, "seek": 2700, "start": 42.0, "end": 44.0, "text": " this tutorial session won't be too exciting,", "tokens": [341, 7073, 5481, 1582, 380, 312, 886, 4670, 11], "temperature": 0.0, "avg_logprob": -0.07021459937095642, "compression_ratio": 1.9059233449477353, "no_speech_prob": 6.000134089845233e-05}, {"id": 14, "seek": 2700, "start": 44.0, "end": 47.0, "text": " but for us it's important to get all of you to the same level", "tokens": [457, 337, 505, 309, 311, 1021, 281, 483, 439, 295, 291, 281, 264, 912, 1496], "temperature": 0.0, "avg_logprob": -0.07021459937095642, "compression_ratio": 1.9059233449477353, "no_speech_prob": 6.000134089845233e-05}, {"id": 15, "seek": 2700, "start": 47.0, "end": 51.0, "text": " and also to give you an overview of the mathematical concepts that will be needed.", "tokens": [293, 611, 281, 976, 291, 364, 12492, 295, 264, 18894, 10392, 300, 486, 312, 2978, 13], "temperature": 0.0, "avg_logprob": -0.07021459937095642, "compression_ratio": 1.9059233449477353, "no_speech_prob": 6.000134089845233e-05}, {"id": 16, "seek": 2700, "start": 51.0, "end": 56.0, "text": " We also uploaded an exercise sheet with five exercises on our website.", "tokens": [492, 611, 17135, 364, 5380, 8193, 365, 1732, 11900, 322, 527, 3144, 13], "temperature": 0.0, "avg_logprob": -0.07021459937095642, "compression_ratio": 1.9059233449477353, "no_speech_prob": 6.000134089845233e-05}, {"id": 17, "seek": 5600, "start": 56.0, "end": 60.0, "text": " These exercises will give you the opportunity to practice your understanding", "tokens": [1981, 11900, 486, 976, 291, 264, 2650, 281, 3124, 428, 3701], "temperature": 0.0, "avg_logprob": -0.051384156210380685, "compression_ratio": 1.9565217391304348, "no_speech_prob": 5.6354441767325625e-05}, {"id": 18, "seek": 5600, "start": 60.0, "end": 64.0, "text": " but we highly recommend you to work on the exercises this week", "tokens": [457, 321, 5405, 2748, 291, 281, 589, 322, 264, 11900, 341, 1243], "temperature": 0.0, "avg_logprob": -0.051384156210380685, "compression_ratio": 1.9565217391304348, "no_speech_prob": 5.6354441767325625e-05}, {"id": 19, "seek": 5600, "start": 64.0, "end": 67.0, "text": " as we choose them closely to the content on the lecture.", "tokens": [382, 321, 2826, 552, 8185, 281, 264, 2701, 322, 264, 7991, 13], "temperature": 0.0, "avg_logprob": -0.051384156210380685, "compression_ratio": 1.9565217391304348, "no_speech_prob": 5.6354441767325625e-05}, {"id": 20, "seek": 5600, "start": 67.0, "end": 71.0, "text": " So all of the exercises at some point in the future will come back to you.", "tokens": [407, 439, 295, 264, 11900, 412, 512, 935, 294, 264, 2027, 486, 808, 646, 281, 291, 13], "temperature": 0.0, "avg_logprob": -0.051384156210380685, "compression_ratio": 1.9565217391304348, "no_speech_prob": 5.6354441767325625e-05}, {"id": 21, "seek": 5600, "start": 71.0, "end": 75.0, "text": " So a good understanding of them will definitely help you in our lecture.", "tokens": [407, 257, 665, 3701, 295, 552, 486, 2138, 854, 291, 294, 527, 7991, 13], "temperature": 0.0, "avg_logprob": -0.051384156210380685, "compression_ratio": 1.9565217391304348, "no_speech_prob": 5.6354441767325625e-05}, {"id": 22, "seek": 5600, "start": 75.0, "end": 80.0, "text": " We didn't include so far any solutions but we will do so next week.", "tokens": [492, 994, 380, 4090, 370, 1400, 604, 6547, 457, 321, 486, 360, 370, 958, 1243, 13], "temperature": 0.0, "avg_logprob": -0.051384156210380685, "compression_ratio": 1.9565217391304348, "no_speech_prob": 5.6354441767325625e-05}, {"id": 23, "seek": 5600, "start": 80.0, "end": 84.0, "text": " This also gives you the opportunity to work on the exercises on your own this week", "tokens": [639, 611, 2709, 291, 264, 2650, 281, 589, 322, 264, 11900, 322, 428, 1065, 341, 1243], "temperature": 0.0, "avg_logprob": -0.051384156210380685, "compression_ratio": 1.9565217391304348, "no_speech_prob": 5.6354441767325625e-05}, {"id": 24, "seek": 8400, "start": 84.0, "end": 88.0, "text": " and compare your own work to the solution next week.", "tokens": [293, 6794, 428, 1065, 589, 281, 264, 3827, 958, 1243, 13], "temperature": 0.0, "avg_logprob": -0.11514524655921438, "compression_ratio": 1.624031007751938, "no_speech_prob": 5.513153155334294e-05}, {"id": 25, "seek": 8400, "start": 88.0, "end": 92.0, "text": " Okay, so this was everything from the organization aside", "tokens": [1033, 11, 370, 341, 390, 1203, 490, 264, 4475, 7359], "temperature": 0.0, "avg_logprob": -0.11514524655921438, "compression_ratio": 1.624031007751938, "no_speech_prob": 5.513153155334294e-05}, {"id": 26, "seek": 8400, "start": 92.0, "end": 95.0, "text": " so let us start with the math recap.", "tokens": [370, 718, 505, 722, 365, 264, 5221, 20928, 13], "temperature": 0.0, "avg_logprob": -0.11514524655921438, "compression_ratio": 1.624031007751938, "no_speech_prob": 5.513153155334294e-05}, {"id": 27, "seek": 8400, "start": 95.0, "end": 98.0, "text": " Okay, before we now dive into the mathematics,", "tokens": [1033, 11, 949, 321, 586, 9192, 666, 264, 18666, 11], "temperature": 0.0, "avg_logprob": -0.11514524655921438, "compression_ratio": 1.624031007751938, "no_speech_prob": 5.513153155334294e-05}, {"id": 28, "seek": 8400, "start": 98.0, "end": 101.0, "text": " I first of all want to give you a short overview", "tokens": [286, 700, 295, 439, 528, 281, 976, 291, 257, 2099, 12492], "temperature": 0.0, "avg_logprob": -0.11514524655921438, "compression_ratio": 1.624031007751938, "no_speech_prob": 5.513153155334294e-05}, {"id": 29, "seek": 8400, "start": 101.0, "end": 104.0, "text": " of how this tutorial session is structured.", "tokens": [295, 577, 341, 7073, 5481, 307, 18519, 13], "temperature": 0.0, "avg_logprob": -0.11514524655921438, "compression_ratio": 1.624031007751938, "no_speech_prob": 5.513153155334294e-05}, {"id": 30, "seek": 8400, "start": 104.0, "end": 106.0, "text": " So we subdivided it into three parts.", "tokens": [407, 321, 31662, 1843, 292, 309, 666, 1045, 3166, 13], "temperature": 0.0, "avg_logprob": -0.11514524655921438, "compression_ratio": 1.624031007751938, "no_speech_prob": 5.513153155334294e-05}, {"id": 31, "seek": 8400, "start": 106.0, "end": 109.0, "text": " The first one being linear algebra, the second calculus,", "tokens": [440, 700, 472, 885, 8213, 21989, 11, 264, 1150, 33400, 11], "temperature": 0.0, "avg_logprob": -0.11514524655921438, "compression_ratio": 1.624031007751938, "no_speech_prob": 5.513153155334294e-05}, {"id": 32, "seek": 8400, "start": 109.0, "end": 111.0, "text": " and the third one probability theory.", "tokens": [293, 264, 2636, 472, 8482, 5261, 13], "temperature": 0.0, "avg_logprob": -0.11514524655921438, "compression_ratio": 1.624031007751938, "no_speech_prob": 5.513153155334294e-05}, {"id": 33, "seek": 11100, "start": 111.0, "end": 115.0, "text": " So I summarized the contents that we will have a look at", "tokens": [407, 286, 14611, 1602, 264, 15768, 300, 321, 486, 362, 257, 574, 412], "temperature": 0.0, "avg_logprob": -0.08337142944335937, "compression_ratio": 1.7389830508474575, "no_speech_prob": 6.138206663308665e-05}, {"id": 34, "seek": 11100, "start": 115.0, "end": 117.0, "text": " and let me just go through it with you together.", "tokens": [293, 718, 385, 445, 352, 807, 309, 365, 291, 1214, 13], "temperature": 0.0, "avg_logprob": -0.08337142944335937, "compression_ratio": 1.7389830508474575, "no_speech_prob": 6.138206663308665e-05}, {"id": 35, "seek": 11100, "start": 117.0, "end": 121.0, "text": " So linear algebra, we will have a look at vector and matrices", "tokens": [407, 8213, 21989, 11, 321, 486, 362, 257, 574, 412, 8062, 293, 32284], "temperature": 0.0, "avg_logprob": -0.08337142944335937, "compression_ratio": 1.7389830508474575, "no_speech_prob": 6.138206663308665e-05}, {"id": 36, "seek": 11100, "start": 121.0, "end": 124.0, "text": " and at the most important basic operations on them.", "tokens": [293, 412, 264, 881, 1021, 3875, 7705, 322, 552, 13], "temperature": 0.0, "avg_logprob": -0.08337142944335937, "compression_ratio": 1.7389830508474575, "no_speech_prob": 6.138206663308665e-05}, {"id": 37, "seek": 11100, "start": 124.0, "end": 125.0, "text": " So that won't be too difficult.", "tokens": [407, 300, 1582, 380, 312, 886, 2252, 13], "temperature": 0.0, "avg_logprob": -0.08337142944335937, "compression_ratio": 1.7389830508474575, "no_speech_prob": 6.138206663308665e-05}, {"id": 38, "seek": 11100, "start": 125.0, "end": 130.0, "text": " Also, we will introduce tensils which are essentially just a generalization of matrices", "tokens": [2743, 11, 321, 486, 5366, 10688, 4174, 597, 366, 4476, 445, 257, 2674, 2144, 295, 32284], "temperature": 0.0, "avg_logprob": -0.08337142944335937, "compression_ratio": 1.7389830508474575, "no_speech_prob": 6.138206663308665e-05}, {"id": 39, "seek": 11100, "start": 130.0, "end": 134.0, "text": " and which are very useful in the setting of deep learning and computer vision.", "tokens": [293, 597, 366, 588, 4420, 294, 264, 3287, 295, 2452, 2539, 293, 3820, 5201, 13], "temperature": 0.0, "avg_logprob": -0.08337142944335937, "compression_ratio": 1.7389830508474575, "no_speech_prob": 6.138206663308665e-05}, {"id": 40, "seek": 11100, "start": 134.0, "end": 137.0, "text": " Finally, I will talk about norms and loss functions", "tokens": [6288, 11, 286, 486, 751, 466, 24357, 293, 4470, 6828], "temperature": 0.0, "avg_logprob": -0.08337142944335937, "compression_ratio": 1.7389830508474575, "no_speech_prob": 6.138206663308665e-05}, {"id": 41, "seek": 11100, "start": 137.0, "end": 140.0, "text": " which will be used heavily in our lecture.", "tokens": [597, 486, 312, 1143, 10950, 294, 527, 7991, 13], "temperature": 0.0, "avg_logprob": -0.08337142944335937, "compression_ratio": 1.7389830508474575, "no_speech_prob": 6.138206663308665e-05}, {"id": 42, "seek": 14000, "start": 140.0, "end": 142.0, "text": " So that's also good to see them again.", "tokens": [407, 300, 311, 611, 665, 281, 536, 552, 797, 13], "temperature": 0.0, "avg_logprob": -0.10259352611894366, "compression_ratio": 1.7196969696969697, "no_speech_prob": 2.1486148398253135e-05}, {"id": 43, "seek": 14000, "start": 142.0, "end": 144.0, "text": " The second part is calculus.", "tokens": [440, 1150, 644, 307, 33400, 13], "temperature": 0.0, "avg_logprob": -0.10259352611894366, "compression_ratio": 1.7196969696969697, "no_speech_prob": 2.1486148398253135e-05}, {"id": 44, "seek": 14000, "start": 144.0, "end": 147.0, "text": " So there we will mainly talk about derivatives.", "tokens": [407, 456, 321, 486, 8704, 751, 466, 33733, 13], "temperature": 0.0, "avg_logprob": -0.10259352611894366, "compression_ratio": 1.7196969696969697, "no_speech_prob": 2.1486148398253135e-05}, {"id": 45, "seek": 14000, "start": 147.0, "end": 149.0, "text": " We start with a scale at the derivative", "tokens": [492, 722, 365, 257, 4373, 412, 264, 13760], "temperature": 0.0, "avg_logprob": -0.10259352611894366, "compression_ratio": 1.7196969696969697, "no_speech_prob": 2.1486148398253135e-05}, {"id": 46, "seek": 14000, "start": 149.0, "end": 152.0, "text": " and then we will go up to the higher dimensions.", "tokens": [293, 550, 321, 486, 352, 493, 281, 264, 2946, 12819, 13], "temperature": 0.0, "avg_logprob": -0.10259352611894366, "compression_ratio": 1.7196969696969697, "no_speech_prob": 2.1486148398253135e-05}, {"id": 47, "seek": 14000, "start": 152.0, "end": 156.0, "text": " So we will look at gradients and at the Jacobian matrix.", "tokens": [407, 321, 486, 574, 412, 2771, 2448, 293, 412, 264, 14117, 952, 8141, 13], "temperature": 0.0, "avg_logprob": -0.10259352611894366, "compression_ratio": 1.7196969696969697, "no_speech_prob": 2.1486148398253135e-05}, {"id": 48, "seek": 14000, "start": 156.0, "end": 160.0, "text": " Finally, in calculus, we also want to have a deeper look at the chain world", "tokens": [6288, 11, 294, 33400, 11, 321, 611, 528, 281, 362, 257, 7731, 574, 412, 264, 5021, 1002], "temperature": 0.0, "avg_logprob": -0.10259352611894366, "compression_ratio": 1.7196969696969697, "no_speech_prob": 2.1486148398253135e-05}, {"id": 49, "seek": 14000, "start": 160.0, "end": 164.0, "text": " which will play a very important role in the setting of deep learning and new networks.", "tokens": [597, 486, 862, 257, 588, 1021, 3090, 294, 264, 3287, 295, 2452, 2539, 293, 777, 9590, 13], "temperature": 0.0, "avg_logprob": -0.10259352611894366, "compression_ratio": 1.7196969696969697, "no_speech_prob": 2.1486148398253135e-05}, {"id": 50, "seek": 14000, "start": 164.0, "end": 167.0, "text": " So that's important as well.", "tokens": [407, 300, 311, 1021, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.10259352611894366, "compression_ratio": 1.7196969696969697, "no_speech_prob": 2.1486148398253135e-05}, {"id": 51, "seek": 16700, "start": 167.0, "end": 170.0, "text": " The third part is probability theory.", "tokens": [440, 2636, 644, 307, 8482, 5261, 13], "temperature": 0.0, "avg_logprob": -0.0896907655319365, "compression_ratio": 1.8770491803278688, "no_speech_prob": 2.3901704480522312e-05}, {"id": 52, "seek": 16700, "start": 170.0, "end": 172.0, "text": " So there we will start with the main concepts", "tokens": [407, 456, 321, 486, 722, 365, 264, 2135, 10392], "temperature": 0.0, "avg_logprob": -0.0896907655319365, "compression_ratio": 1.8770491803278688, "no_speech_prob": 2.3901704480522312e-05}, {"id": 53, "seek": 16700, "start": 172.0, "end": 174.0, "text": " and we will have a look at their definition.", "tokens": [293, 321, 486, 362, 257, 574, 412, 641, 7123, 13], "temperature": 0.0, "avg_logprob": -0.0896907655319365, "compression_ratio": 1.8770491803278688, "no_speech_prob": 2.3901704480522312e-05}, {"id": 54, "seek": 16700, "start": 174.0, "end": 176.0, "text": " So probability spaces, random variables,", "tokens": [407, 8482, 7673, 11, 4974, 9102, 11], "temperature": 0.0, "avg_logprob": -0.0896907655319365, "compression_ratio": 1.8770491803278688, "no_speech_prob": 2.3901704480522312e-05}, {"id": 55, "seek": 16700, "start": 176.0, "end": 179.0, "text": " and also the corresponding distribution functions.", "tokens": [293, 611, 264, 11760, 7316, 6828, 13], "temperature": 0.0, "avg_logprob": -0.0896907655319365, "compression_ratio": 1.8770491803278688, "no_speech_prob": 2.3901704480522312e-05}, {"id": 56, "seek": 16700, "start": 179.0, "end": 184.0, "text": " Furthermore, we also review the concept of mean invariances", "tokens": [23999, 11, 321, 611, 3131, 264, 3410, 295, 914, 33270, 2676], "temperature": 0.0, "avg_logprob": -0.0896907655319365, "compression_ratio": 1.8770491803278688, "no_speech_prob": 2.3901704480522312e-05}, {"id": 57, "seek": 16700, "start": 184.0, "end": 188.0, "text": " and we will give you a short overview of the most important probability distributions", "tokens": [293, 321, 486, 976, 291, 257, 2099, 12492, 295, 264, 881, 1021, 8482, 37870], "temperature": 0.0, "avg_logprob": -0.0896907655319365, "compression_ratio": 1.8770491803278688, "no_speech_prob": 2.3901704480522312e-05}, {"id": 58, "seek": 16700, "start": 188.0, "end": 190.0, "text": " of the random variable.", "tokens": [295, 264, 4974, 7006, 13], "temperature": 0.0, "avg_logprob": -0.0896907655319365, "compression_ratio": 1.8770491803278688, "no_speech_prob": 2.3901704480522312e-05}, {"id": 59, "seek": 16700, "start": 190.0, "end": 194.0, "text": " For example, the Gaussian distribution or the uniform distribution.", "tokens": [1171, 1365, 11, 264, 39148, 7316, 420, 264, 9452, 7316, 13], "temperature": 0.0, "avg_logprob": -0.0896907655319365, "compression_ratio": 1.8770491803278688, "no_speech_prob": 2.3901704480522312e-05}, {"id": 60, "seek": 19400, "start": 194.0, "end": 200.0, "text": " So let us have a look at the first part, linear algebra.", "tokens": [407, 718, 505, 362, 257, 574, 412, 264, 700, 644, 11, 8213, 21989, 13], "temperature": 0.0, "avg_logprob": -0.16097957252437234, "compression_ratio": 1.7465437788018434, "no_speech_prob": 3.674884283100255e-05}, {"id": 61, "seek": 19400, "start": 200.0, "end": 203.0, "text": " So before we give the definition,", "tokens": [407, 949, 321, 976, 264, 7123, 11], "temperature": 0.0, "avg_logprob": -0.16097957252437234, "compression_ratio": 1.7465437788018434, "no_speech_prob": 3.674884283100255e-05}, {"id": 62, "seek": 19400, "start": 203.0, "end": 208.0, "text": " I want to have a look at the basic notations here before we go further.", "tokens": [286, 528, 281, 362, 257, 574, 412, 264, 3875, 406, 763, 510, 949, 321, 352, 3052, 13], "temperature": 0.0, "avg_logprob": -0.16097957252437234, "compression_ratio": 1.7465437788018434, "no_speech_prob": 3.674884283100255e-05}, {"id": 63, "seek": 19400, "start": 208.0, "end": 210.0, "text": " So when we talk about a vector,", "tokens": [407, 562, 321, 751, 466, 257, 8062, 11], "temperature": 0.0, "avg_logprob": -0.16097957252437234, "compression_ratio": 1.7465437788018434, "no_speech_prob": 3.674884283100255e-05}, {"id": 64, "seek": 19400, "start": 210.0, "end": 213.0, "text": " we talk about an element of Rn mostly.", "tokens": [321, 751, 466, 364, 4478, 295, 497, 77, 5240, 13], "temperature": 0.0, "avg_logprob": -0.16097957252437234, "compression_ratio": 1.7465437788018434, "no_speech_prob": 3.674884283100255e-05}, {"id": 65, "seek": 19400, "start": 213.0, "end": 216.0, "text": " So it's a vector. It's just a two-wheel-width n elements.", "tokens": [407, 309, 311, 257, 8062, 13, 467, 311, 445, 257, 732, 12, 22830, 12, 21271, 297, 4959, 13], "temperature": 0.0, "avg_logprob": -0.16097957252437234, "compression_ratio": 1.7465437788018434, "no_speech_prob": 3.674884283100255e-05}, {"id": 66, "seek": 19400, "start": 216.0, "end": 218.0, "text": " In our case, we're looking at Rn.", "tokens": [682, 527, 1389, 11, 321, 434, 1237, 412, 497, 77, 13], "temperature": 0.0, "avg_logprob": -0.16097957252437234, "compression_ratio": 1.7465437788018434, "no_speech_prob": 3.674884283100255e-05}, {"id": 67, "seek": 19400, "start": 218.0, "end": 221.0, "text": " Of course, the vector is not always an element of Rn.", "tokens": [2720, 1164, 11, 264, 8062, 307, 406, 1009, 364, 4478, 295, 497, 77, 13], "temperature": 0.0, "avg_logprob": -0.16097957252437234, "compression_ratio": 1.7465437788018434, "no_speech_prob": 3.674884283100255e-05}, {"id": 68, "seek": 22100, "start": 221.0, "end": 226.0, "text": " But in our case, we are just restricting ourselves to Rn here.", "tokens": [583, 294, 527, 1389, 11, 321, 366, 445, 1472, 37714, 4175, 281, 497, 77, 510, 13], "temperature": 0.0, "avg_logprob": -0.13964883416099885, "compression_ratio": 1.7277227722772277, "no_speech_prob": 3.0287170375231653e-05}, {"id": 69, "seek": 22100, "start": 226.0, "end": 229.0, "text": " So when we choose a vector v in Rn,", "tokens": [407, 562, 321, 2826, 257, 8062, 371, 294, 497, 77, 11], "temperature": 0.0, "avg_logprob": -0.13964883416099885, "compression_ratio": 1.7277227722772277, "no_speech_prob": 3.0287170375231653e-05}, {"id": 70, "seek": 22100, "start": 229.0, "end": 233.0, "text": " we can refer to the i-th element by using the index.", "tokens": [321, 393, 2864, 281, 264, 741, 12, 392, 4478, 538, 1228, 264, 8186, 13], "temperature": 0.0, "avg_logprob": -0.13964883416099885, "compression_ratio": 1.7277227722772277, "no_speech_prob": 3.0287170375231653e-05}, {"id": 71, "seek": 22100, "start": 233.0, "end": 240.0, "text": " So the e-th element of our vector v in Rn is then given by v in the index i,", "tokens": [407, 264, 308, 12, 392, 4478, 295, 527, 8062, 371, 294, 497, 77, 307, 550, 2212, 538, 371, 294, 264, 8186, 741, 11], "temperature": 0.0, "avg_logprob": -0.13964883416099885, "compression_ratio": 1.7277227722772277, "no_speech_prob": 3.0287170375231653e-05}, {"id": 72, "seek": 22100, "start": 240.0, "end": 243.0, "text": " which is then an element in R, of course.", "tokens": [597, 307, 550, 364, 4478, 294, 497, 11, 295, 1164, 13], "temperature": 0.0, "avg_logprob": -0.13964883416099885, "compression_ratio": 1.7277227722772277, "no_speech_prob": 3.0287170375231653e-05}, {"id": 73, "seek": 22100, "start": 243.0, "end": 245.0, "text": " Equivalent D.", "tokens": [15624, 3576, 317, 413, 13], "temperature": 0.0, "avg_logprob": -0.13964883416099885, "compression_ratio": 1.7277227722772277, "no_speech_prob": 3.0287170375231653e-05}, {"id": 74, "seek": 22100, "start": 245.0, "end": 250.0, "text": " Matrix is an element now of the set R to the power of n times n.", "tokens": [36274, 307, 364, 4478, 586, 295, 264, 992, 497, 281, 264, 1347, 295, 297, 1413, 297, 13], "temperature": 0.0, "avg_logprob": -0.13964883416099885, "compression_ratio": 1.7277227722772277, "no_speech_prob": 3.0287170375231653e-05}, {"id": 75, "seek": 25000, "start": 250.0, "end": 254.0, "text": " So it's a matrix with n rows and m columns.", "tokens": [407, 309, 311, 257, 8141, 365, 297, 13241, 293, 275, 13766, 13], "temperature": 0.0, "avg_logprob": -0.12407471049915661, "compression_ratio": 1.6847290640394088, "no_speech_prob": 3.49334986822214e-05}, {"id": 76, "seek": 25000, "start": 254.0, "end": 260.0, "text": " And here, if we take arbitrary matrix A and our set R to the power of n times m,", "tokens": [400, 510, 11, 498, 321, 747, 23211, 8141, 316, 293, 527, 992, 497, 281, 264, 1347, 295, 297, 1413, 275, 11], "temperature": 0.0, "avg_logprob": -0.12407471049915661, "compression_ratio": 1.6847290640394088, "no_speech_prob": 3.49334986822214e-05}, {"id": 77, "seek": 25000, "start": 260.0, "end": 263.0, "text": " we denote an element in this matrix.", "tokens": [321, 45708, 364, 4478, 294, 341, 8141, 13], "temperature": 0.0, "avg_logprob": -0.12407471049915661, "compression_ratio": 1.6847290640394088, "no_speech_prob": 3.49334986822214e-05}, {"id": 78, "seek": 25000, "start": 263.0, "end": 270.0, "text": " For example, in the i-th row and the j-th column by the element a, a, a, i, j,", "tokens": [1171, 1365, 11, 294, 264, 741, 12, 392, 5386, 293, 264, 361, 12, 392, 7738, 538, 264, 4478, 257, 11, 257, 11, 257, 11, 741, 11, 361, 11], "temperature": 0.0, "avg_logprob": -0.12407471049915661, "compression_ratio": 1.6847290640394088, "no_speech_prob": 3.49334986822214e-05}, {"id": 79, "seek": 25000, "start": 270.0, "end": 273.0, "text": " where i and j is in the index.", "tokens": [689, 741, 293, 361, 307, 294, 264, 8186, 13], "temperature": 0.0, "avg_logprob": -0.12407471049915661, "compression_ratio": 1.6847290640394088, "no_speech_prob": 3.49334986822214e-05}, {"id": 80, "seek": 25000, "start": 273.0, "end": 278.0, "text": " And this element, as well as before in the vector, is an element in R.", "tokens": [400, 341, 4478, 11, 382, 731, 382, 949, 294, 264, 8062, 11, 307, 364, 4478, 294, 497, 13], "temperature": 0.0, "avg_logprob": -0.12407471049915661, "compression_ratio": 1.6847290640394088, "no_speech_prob": 3.49334986822214e-05}, {"id": 81, "seek": 27800, "start": 278.0, "end": 281.0, "text": " OK, so this is the notation that we will use.", "tokens": [2264, 11, 370, 341, 307, 264, 24657, 300, 321, 486, 764, 13], "temperature": 0.0, "avg_logprob": -0.10775395443564967, "compression_ratio": 1.7666666666666666, "no_speech_prob": 4.712482768809423e-05}, {"id": 82, "seek": 27800, "start": 281.0, "end": 287.0, "text": " Another notation that I want to shortly review is the transpose of a matrix.", "tokens": [3996, 24657, 300, 286, 528, 281, 13392, 3131, 307, 264, 25167, 295, 257, 8141, 13], "temperature": 0.0, "avg_logprob": -0.10775395443564967, "compression_ratio": 1.7666666666666666, "no_speech_prob": 4.712482768809423e-05}, {"id": 83, "seek": 27800, "start": 287.0, "end": 293.0, "text": " And the transpose of a matrix essentially is just flipping columns and rows.", "tokens": [400, 264, 25167, 295, 257, 8141, 4476, 307, 445, 26886, 13766, 293, 13241, 13], "temperature": 0.0, "avg_logprob": -0.10775395443564967, "compression_ratio": 1.7666666666666666, "no_speech_prob": 4.712482768809423e-05}, {"id": 84, "seek": 27800, "start": 293.0, "end": 296.0, "text": " So you just flip them one around.", "tokens": [407, 291, 445, 7929, 552, 472, 926, 13], "temperature": 0.0, "avg_logprob": -0.10775395443564967, "compression_ratio": 1.7666666666666666, "no_speech_prob": 4.712482768809423e-05}, {"id": 85, "seek": 27800, "start": 296.0, "end": 301.0, "text": " And then we denote a transpose matrix, which has been the matrix A.", "tokens": [400, 550, 321, 45708, 257, 25167, 8141, 11, 597, 575, 668, 264, 8141, 316, 13], "temperature": 0.0, "avg_logprob": -0.10775395443564967, "compression_ratio": 1.7666666666666666, "no_speech_prob": 4.712482768809423e-05}, {"id": 86, "seek": 27800, "start": 301.0, "end": 306.0, "text": " We denote it by A to the power of t, which is referring to transpose.", "tokens": [492, 45708, 309, 538, 316, 281, 264, 1347, 295, 256, 11, 597, 307, 13761, 281, 25167, 13], "temperature": 0.0, "avg_logprob": -0.10775395443564967, "compression_ratio": 1.7666666666666666, "no_speech_prob": 4.712482768809423e-05}, {"id": 87, "seek": 30600, "start": 306.0, "end": 311.0, "text": " And when the matrix A is an element of R to the power n times m,", "tokens": [400, 562, 264, 8141, 316, 307, 364, 4478, 295, 497, 281, 264, 1347, 297, 1413, 275, 11], "temperature": 0.0, "avg_logprob": -0.06062418222427368, "compression_ratio": 1.8855721393034826, "no_speech_prob": 1.3613946066470817e-05}, {"id": 88, "seek": 30600, "start": 311.0, "end": 315.0, "text": " then the transpose matrix, as we flip rows and columns,", "tokens": [550, 264, 25167, 8141, 11, 382, 321, 7929, 13241, 293, 13766, 11], "temperature": 0.0, "avg_logprob": -0.06062418222427368, "compression_ratio": 1.8855721393034826, "no_speech_prob": 1.3613946066470817e-05}, {"id": 89, "seek": 30600, "start": 315.0, "end": 319.0, "text": " is then an element of the set R to the power m times n.", "tokens": [307, 550, 364, 4478, 295, 264, 992, 497, 281, 264, 1347, 275, 1413, 297, 13], "temperature": 0.0, "avg_logprob": -0.06062418222427368, "compression_ratio": 1.8855721393034826, "no_speech_prob": 1.3613946066470817e-05}, {"id": 90, "seek": 30600, "start": 319.0, "end": 322.0, "text": " So we flip also the dimensions.", "tokens": [407, 321, 7929, 611, 264, 12819, 13], "temperature": 0.0, "avg_logprob": -0.06062418222427368, "compression_ratio": 1.8855721393034826, "no_speech_prob": 1.3613946066470817e-05}, {"id": 91, "seek": 30600, "start": 322.0, "end": 327.0, "text": " And this concept of transpose is used similarly for vectors.", "tokens": [400, 341, 3410, 295, 25167, 307, 1143, 14138, 337, 18875, 13], "temperature": 0.0, "avg_logprob": -0.06062418222427368, "compression_ratio": 1.8855721393034826, "no_speech_prob": 1.3613946066470817e-05}, {"id": 92, "seek": 30600, "start": 327.0, "end": 330.0, "text": " So you can also flip rows and columns in a vector.", "tokens": [407, 291, 393, 611, 7929, 13241, 293, 13766, 294, 257, 8062, 13], "temperature": 0.0, "avg_logprob": -0.06062418222427368, "compression_ratio": 1.8855721393034826, "no_speech_prob": 1.3613946066470817e-05}, {"id": 93, "seek": 30600, "start": 330.0, "end": 335.0, "text": " So a vector like this will just be kind of a lying vector.", "tokens": [407, 257, 8062, 411, 341, 486, 445, 312, 733, 295, 257, 8493, 8062, 13], "temperature": 0.0, "avg_logprob": -0.06062418222427368, "compression_ratio": 1.8855721393034826, "no_speech_prob": 1.3613946066470817e-05}, {"id": 94, "seek": 33500, "start": 335.0, "end": 340.0, "text": " And you don't maybe have a vector in n or R to the power n times n.", "tokens": [400, 291, 500, 380, 1310, 362, 257, 8062, 294, 297, 420, 497, 281, 264, 1347, 297, 1413, 297, 13], "temperature": 0.0, "avg_logprob": -0.13524092697515722, "compression_ratio": 1.7456896551724137, "no_speech_prob": 1.1609109606069978e-05}, {"id": 95, "seek": 33500, "start": 340.0, "end": 346.0, "text": " But rather in the element when the set R to the power one time times n.", "tokens": [583, 2831, 294, 264, 4478, 562, 264, 992, 497, 281, 264, 1347, 472, 565, 1413, 297, 13], "temperature": 0.0, "avg_logprob": -0.13524092697515722, "compression_ratio": 1.7456896551724137, "no_speech_prob": 1.1609109606069978e-05}, {"id": 96, "seek": 33500, "start": 346.0, "end": 351.0, "text": " So yeah, you can also transpose vectors is what I want to say here.", "tokens": [407, 1338, 11, 291, 393, 611, 25167, 18875, 307, 437, 286, 528, 281, 584, 510, 13], "temperature": 0.0, "avg_logprob": -0.13524092697515722, "compression_ratio": 1.7456896551724137, "no_speech_prob": 1.1609109606069978e-05}, {"id": 97, "seek": 33500, "start": 351.0, "end": 354.0, "text": " OK, so vectors, let's have a look at that.", "tokens": [2264, 11, 370, 18875, 11, 718, 311, 362, 257, 574, 412, 300, 13], "temperature": 0.0, "avg_logprob": -0.13524092697515722, "compression_ratio": 1.7456896551724137, "no_speech_prob": 1.1609109606069978e-05}, {"id": 98, "seek": 33500, "start": 354.0, "end": 358.0, "text": " As I said, a vector is an element in an n dimensional space.", "tokens": [1018, 286, 848, 11, 257, 8062, 307, 364, 4478, 294, 364, 297, 18795, 1901, 13], "temperature": 0.0, "avg_logprob": -0.13524092697515722, "compression_ratio": 1.7456896551724137, "no_speech_prob": 1.1609109606069978e-05}, {"id": 99, "seek": 33500, "start": 358.0, "end": 361.0, "text": " And now, OK, we look at the space Rn.", "tokens": [400, 586, 11, 2264, 11, 321, 574, 412, 264, 1901, 497, 77, 13], "temperature": 0.0, "avg_logprob": -0.13524092697515722, "compression_ratio": 1.7456896551724137, "no_speech_prob": 1.1609109606069978e-05}, {"id": 100, "seek": 33500, "start": 361.0, "end": 364.0, "text": " And you can just denote it by a tupper with n elements.", "tokens": [400, 291, 393, 445, 45708, 309, 538, 257, 2604, 3717, 365, 297, 4959, 13], "temperature": 0.0, "avg_logprob": -0.13524092697515722, "compression_ratio": 1.7456896551724137, "no_speech_prob": 1.1609109606069978e-05}, {"id": 101, "seek": 36400, "start": 364.0, "end": 369.0, "text": " And the elements are now given by v1, v2, up to vn.", "tokens": [400, 264, 4959, 366, 586, 2212, 538, 371, 16, 11, 371, 17, 11, 493, 281, 371, 77, 13], "temperature": 0.0, "avg_logprob": -0.09277199576882755, "compression_ratio": 1.553763440860215, "no_speech_prob": 3.191994983353652e-05}, {"id": 102, "seek": 36400, "start": 369.0, "end": 374.0, "text": " And the figure here is showing a three-dimensional space.", "tokens": [400, 264, 2573, 510, 307, 4099, 257, 1045, 12, 18759, 1901, 13], "temperature": 0.0, "avg_logprob": -0.09277199576882755, "compression_ratio": 1.553763440860215, "no_speech_prob": 3.191994983353652e-05}, {"id": 103, "seek": 36400, "start": 374.0, "end": 380.0, "text": " And now, OK, for example, R3, where each of the little pictures that you see", "tokens": [400, 586, 11, 2264, 11, 337, 1365, 11, 497, 18, 11, 689, 1184, 295, 264, 707, 5242, 300, 291, 536], "temperature": 0.0, "avg_logprob": -0.09277199576882755, "compression_ratio": 1.553763440860215, "no_speech_prob": 3.191994983353652e-05}, {"id": 104, "seek": 36400, "start": 380.0, "end": 384.0, "text": " is one vector in our three-dimensional space R3.", "tokens": [307, 472, 8062, 294, 527, 1045, 12, 18759, 1901, 497, 18, 13], "temperature": 0.0, "avg_logprob": -0.09277199576882755, "compression_ratio": 1.553763440860215, "no_speech_prob": 3.191994983353652e-05}, {"id": 105, "seek": 36400, "start": 384.0, "end": 390.0, "text": " And for vectors, we can define four basic operations.", "tokens": [400, 337, 18875, 11, 321, 393, 6964, 1451, 3875, 7705, 13], "temperature": 0.0, "avg_logprob": -0.09277199576882755, "compression_ratio": 1.553763440860215, "no_speech_prob": 3.191994983353652e-05}, {"id": 106, "seek": 39000, "start": 390.0, "end": 396.0, "text": " And these are given by addition, subtraction, scalar multiplication, and the dot product.", "tokens": [400, 613, 366, 2212, 538, 4500, 11, 16390, 313, 11, 39684, 27290, 11, 293, 264, 5893, 1674, 13], "temperature": 0.0, "avg_logprob": -0.09410252571105956, "compression_ratio": 1.8357487922705313, "no_speech_prob": 1.932199847942684e-05}, {"id": 107, "seek": 39000, "start": 396.0, "end": 400.0, "text": " And we will have a short view over these operations.", "tokens": [400, 321, 486, 362, 257, 2099, 1910, 670, 613, 7705, 13], "temperature": 0.0, "avg_logprob": -0.09410252571105956, "compression_ratio": 1.8357487922705313, "no_speech_prob": 1.932199847942684e-05}, {"id": 108, "seek": 39000, "start": 400.0, "end": 403.0, "text": " So addition of two vectors.", "tokens": [407, 4500, 295, 732, 18875, 13], "temperature": 0.0, "avg_logprob": -0.09410252571105956, "compression_ratio": 1.8357487922705313, "no_speech_prob": 1.932199847942684e-05}, {"id": 109, "seek": 39000, "start": 403.0, "end": 406.0, "text": " So we start by taking two vectors arbitrary.", "tokens": [407, 321, 722, 538, 1940, 732, 18875, 23211, 13], "temperature": 0.0, "avg_logprob": -0.09410252571105956, "compression_ratio": 1.8357487922705313, "no_speech_prob": 1.932199847942684e-05}, {"id": 110, "seek": 39000, "start": 406.0, "end": 409.0, "text": " So a and b are two elements of Rn.", "tokens": [407, 257, 293, 272, 366, 732, 4959, 295, 497, 77, 13], "temperature": 0.0, "avg_logprob": -0.09410252571105956, "compression_ratio": 1.8357487922705313, "no_speech_prob": 1.932199847942684e-05}, {"id": 111, "seek": 39000, "start": 409.0, "end": 414.0, "text": " And then you find the addition of these two vectors by the tupper,", "tokens": [400, 550, 291, 915, 264, 4500, 295, 613, 732, 18875, 538, 264, 2604, 3717, 11], "temperature": 0.0, "avg_logprob": -0.09410252571105956, "compression_ratio": 1.8357487922705313, "no_speech_prob": 1.932199847942684e-05}, {"id": 112, "seek": 39000, "start": 414.0, "end": 418.0, "text": " where you component by add the elements of the vector a and b.", "tokens": [689, 291, 6542, 538, 909, 264, 4959, 295, 264, 8062, 257, 293, 272, 13], "temperature": 0.0, "avg_logprob": -0.09410252571105956, "compression_ratio": 1.8357487922705313, "no_speech_prob": 1.932199847942684e-05}, {"id": 113, "seek": 41800, "start": 418.0, "end": 421.0, "text": " So the first element is given by a1 plus b1.", "tokens": [407, 264, 700, 4478, 307, 2212, 538, 257, 16, 1804, 272, 16, 13], "temperature": 0.0, "avg_logprob": -0.07392339886359449, "compression_ratio": 1.7641509433962264, "no_speech_prob": 9.302169019065332e-06}, {"id": 114, "seek": 41800, "start": 421.0, "end": 424.0, "text": " The last element is given by a and plus bn.", "tokens": [440, 1036, 4478, 307, 2212, 538, 257, 293, 1804, 272, 77, 13], "temperature": 0.0, "avg_logprob": -0.07392339886359449, "compression_ratio": 1.7641509433962264, "no_speech_prob": 9.302169019065332e-06}, {"id": 115, "seek": 41800, "start": 424.0, "end": 432.0, "text": " And of course, the result of the addition of two vectors in Rn is again an element in Rn.", "tokens": [400, 295, 1164, 11, 264, 1874, 295, 264, 4500, 295, 732, 18875, 294, 497, 77, 307, 797, 364, 4478, 294, 497, 77, 13], "temperature": 0.0, "avg_logprob": -0.07392339886359449, "compression_ratio": 1.7641509433962264, "no_speech_prob": 9.302169019065332e-06}, {"id": 116, "seek": 41800, "start": 432.0, "end": 438.0, "text": " And the figure and the lower right of this slide shows you the geometric interpretation", "tokens": [400, 264, 2573, 293, 264, 3126, 558, 295, 341, 4137, 3110, 291, 264, 33246, 14174], "temperature": 0.0, "avg_logprob": -0.07392339886359449, "compression_ratio": 1.7641509433962264, "no_speech_prob": 9.302169019065332e-06}, {"id": 117, "seek": 41800, "start": 438.0, "end": 441.0, "text": " of what does it mean to add two vectors in the space Rn.", "tokens": [295, 437, 775, 309, 914, 281, 909, 732, 18875, 294, 264, 1901, 497, 77, 13], "temperature": 0.0, "avg_logprob": -0.07392339886359449, "compression_ratio": 1.7641509433962264, "no_speech_prob": 9.302169019065332e-06}, {"id": 118, "seek": 41800, "start": 441.0, "end": 445.0, "text": " So here, for example, we have the vectors a and b.", "tokens": [407, 510, 11, 337, 1365, 11, 321, 362, 264, 18875, 257, 293, 272, 13], "temperature": 0.0, "avg_logprob": -0.07392339886359449, "compression_ratio": 1.7641509433962264, "no_speech_prob": 9.302169019065332e-06}, {"id": 119, "seek": 44500, "start": 445.0, "end": 451.0, "text": " And addition of these vectors geometrically means that we just concatenate.", "tokens": [400, 4500, 295, 613, 18875, 12956, 81, 984, 1355, 300, 321, 445, 1588, 7186, 473, 13], "temperature": 0.0, "avg_logprob": -0.08365309472177543, "compression_ratio": 1.829268292682927, "no_speech_prob": 5.328276893123984e-05}, {"id": 120, "seek": 44500, "start": 451.0, "end": 453.0, "text": " So first of all, we take the vector of a.", "tokens": [407, 700, 295, 439, 11, 321, 747, 264, 8062, 295, 257, 13], "temperature": 0.0, "avg_logprob": -0.08365309472177543, "compression_ratio": 1.829268292682927, "no_speech_prob": 5.328276893123984e-05}, {"id": 121, "seek": 44500, "start": 453.0, "end": 461.0, "text": " And then after that, we concatenate the vector of b, which then results in the red vector here given by a plus b.", "tokens": [400, 550, 934, 300, 11, 321, 1588, 7186, 473, 264, 8062, 295, 272, 11, 597, 550, 3542, 294, 264, 2182, 8062, 510, 2212, 538, 257, 1804, 272, 13], "temperature": 0.0, "avg_logprob": -0.08365309472177543, "compression_ratio": 1.829268292682927, "no_speech_prob": 5.328276893123984e-05}, {"id": 122, "seek": 44500, "start": 461.0, "end": 465.0, "text": " And you can also do it the other way around, start by taking the vector b.", "tokens": [400, 291, 393, 611, 360, 309, 264, 661, 636, 926, 11, 722, 538, 1940, 264, 8062, 272, 13], "temperature": 0.0, "avg_logprob": -0.08365309472177543, "compression_ratio": 1.829268292682927, "no_speech_prob": 5.328276893123984e-05}, {"id": 123, "seek": 44500, "start": 465.0, "end": 469.0, "text": " And then concatenate the vector a, which gives you exactly the same.", "tokens": [400, 550, 1588, 7186, 473, 264, 8062, 257, 11, 597, 2709, 291, 2293, 264, 912, 13], "temperature": 0.0, "avg_logprob": -0.08365309472177543, "compression_ratio": 1.829268292682927, "no_speech_prob": 5.328276893123984e-05}, {"id": 124, "seek": 46900, "start": 469.0, "end": 479.0, "text": " And because these are exactly the same, we can define or we can just see that the addition is a co-communative operation.", "tokens": [400, 570, 613, 366, 2293, 264, 912, 11, 321, 393, 6964, 420, 321, 393, 445, 536, 300, 264, 4500, 307, 257, 598, 12, 25451, 1166, 6916, 13], "temperature": 0.0, "avg_logprob": -0.11706936068651153, "compression_ratio": 1.6288659793814433, "no_speech_prob": 1.3617305739899166e-05}, {"id": 125, "seek": 46900, "start": 479.0, "end": 483.0, "text": " Okay, so let us have a look at subtraction.", "tokens": [1033, 11, 370, 718, 505, 362, 257, 574, 412, 16390, 313, 13], "temperature": 0.0, "avg_logprob": -0.11706936068651153, "compression_ratio": 1.6288659793814433, "no_speech_prob": 1.3617305739899166e-05}, {"id": 126, "seek": 46900, "start": 483.0, "end": 490.0, "text": " The subtraction operation is exactly the same as the addition, but we're just going to replace the plus sign by a minus sign.", "tokens": [440, 16390, 313, 6916, 307, 2293, 264, 912, 382, 264, 4500, 11, 457, 321, 434, 445, 516, 281, 7406, 264, 1804, 1465, 538, 257, 3175, 1465, 13], "temperature": 0.0, "avg_logprob": -0.11706936068651153, "compression_ratio": 1.6288659793814433, "no_speech_prob": 1.3617305739899166e-05}, {"id": 127, "seek": 46900, "start": 490.0, "end": 492.0, "text": " So it's nothing special.", "tokens": [407, 309, 311, 1825, 2121, 13], "temperature": 0.0, "avg_logprob": -0.11706936068651153, "compression_ratio": 1.6288659793814433, "no_speech_prob": 1.3617305739899166e-05}, {"id": 128, "seek": 49200, "start": 492.0, "end": 502.0, "text": " So again, we start by a and b being two vectors in Rn, and then the subtraction of these two vectors a minus b is defined by the vector when you subtract component vice.", "tokens": [407, 797, 11, 321, 722, 538, 257, 293, 272, 885, 732, 18875, 294, 497, 77, 11, 293, 550, 264, 16390, 313, 295, 613, 732, 18875, 257, 3175, 272, 307, 7642, 538, 264, 8062, 562, 291, 16390, 6542, 11964, 13], "temperature": 0.0, "avg_logprob": -0.11354281950970085, "compression_ratio": 2.0054054054054054, "no_speech_prob": 3.676858977996744e-05}, {"id": 129, "seek": 49200, "start": 502.0, "end": 508.0, "text": " So the first element then or the first element in the vector a minus b is given by a one minus b one.", "tokens": [407, 264, 700, 4478, 550, 420, 264, 700, 4478, 294, 264, 8062, 257, 3175, 272, 307, 2212, 538, 257, 472, 3175, 272, 472, 13], "temperature": 0.0, "avg_logprob": -0.11354281950970085, "compression_ratio": 2.0054054054054054, "no_speech_prob": 3.676858977996744e-05}, {"id": 130, "seek": 49200, "start": 508.0, "end": 512.0, "text": " And the last element is given by a and minus bn.", "tokens": [400, 264, 1036, 4478, 307, 2212, 538, 257, 293, 3175, 272, 77, 13], "temperature": 0.0, "avg_logprob": -0.11354281950970085, "compression_ratio": 2.0054054054054054, "no_speech_prob": 3.676858977996744e-05}, {"id": 131, "seek": 49200, "start": 512.0, "end": 517.0, "text": " So the resulting vector is an element in Rn again.", "tokens": [407, 264, 16505, 8062, 307, 364, 4478, 294, 497, 77, 797, 13], "temperature": 0.0, "avg_logprob": -0.11354281950970085, "compression_ratio": 2.0054054054054054, "no_speech_prob": 3.676858977996744e-05}, {"id": 132, "seek": 51700, "start": 517.0, "end": 522.0, "text": " If you look at the figure on the lower right, you can see again the geometric interpretation.", "tokens": [759, 291, 574, 412, 264, 2573, 322, 264, 3126, 558, 11, 291, 393, 536, 797, 264, 33246, 14174, 13], "temperature": 0.0, "avg_logprob": -0.13637586740347055, "compression_ratio": 1.608695652173913, "no_speech_prob": 5.098043766338378e-05}, {"id": 133, "seek": 51700, "start": 522.0, "end": 526.0, "text": " And here you have like a blue the vectors a and b.", "tokens": [400, 510, 291, 362, 411, 257, 3344, 264, 18875, 257, 293, 272, 13], "temperature": 0.0, "avg_logprob": -0.13637586740347055, "compression_ratio": 1.608695652173913, "no_speech_prob": 5.098043766338378e-05}, {"id": 134, "seek": 51700, "start": 526.0, "end": 537.0, "text": " And then red you then have denoted or represented the vector a minus b, which is connecting the ends of the vector a and b.", "tokens": [400, 550, 2182, 291, 550, 362, 1441, 23325, 420, 10379, 264, 8062, 257, 3175, 272, 11, 597, 307, 11015, 264, 5314, 295, 264, 8062, 257, 293, 272, 13], "temperature": 0.0, "avg_logprob": -0.13637586740347055, "compression_ratio": 1.608695652173913, "no_speech_prob": 5.098043766338378e-05}, {"id": 135, "seek": 51700, "start": 537.0, "end": 539.0, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.13637586740347055, "compression_ratio": 1.608695652173913, "no_speech_prob": 5.098043766338378e-05}, {"id": 136, "seek": 51700, "start": 539.0, "end": 540.0, "text": " Scala multiplication.", "tokens": [2747, 5159, 27290, 13], "temperature": 0.0, "avg_logprob": -0.13637586740347055, "compression_ratio": 1.608695652173913, "no_speech_prob": 5.098043766338378e-05}, {"id": 137, "seek": 54000, "start": 540.0, "end": 552.0, "text": " This is maybe more interesting than addition and subtraction, but I think it's also pretty simple. So here we don't start having two vectors in the beginning a and b, but we only take one vector a and Rn.", "tokens": [639, 307, 1310, 544, 1880, 813, 4500, 293, 16390, 313, 11, 457, 286, 519, 309, 311, 611, 1238, 2199, 13, 407, 510, 321, 500, 380, 722, 1419, 732, 18875, 294, 264, 2863, 257, 293, 272, 11, 457, 321, 787, 747, 472, 8062, 257, 293, 497, 77, 13], "temperature": 0.0, "avg_logprob": -0.1407171342431045, "compression_ratio": 1.5686274509803921, "no_speech_prob": 4.5430009777192026e-05}, {"id": 138, "seek": 54000, "start": 552.0, "end": 560.0, "text": " And then on top of that, we take a scalar c and R, which will then be used to multiply the vector by this scalar c.", "tokens": [400, 550, 322, 1192, 295, 300, 11, 321, 747, 257, 39684, 269, 293, 497, 11, 597, 486, 550, 312, 1143, 281, 12972, 264, 8062, 538, 341, 39684, 269, 13], "temperature": 0.0, "avg_logprob": -0.1407171342431045, "compression_ratio": 1.5686274509803921, "no_speech_prob": 4.5430009777192026e-05}, {"id": 139, "seek": 56000, "start": 560.0, "end": 571.0, "text": " So the scalar multiplication is defined by c times a and then it's given by the vector by each component or each element in the vector a is multiplied by this scalar c.", "tokens": [407, 264, 39684, 27290, 307, 7642, 538, 269, 1413, 257, 293, 550, 309, 311, 2212, 538, 264, 8062, 538, 1184, 6542, 420, 1184, 4478, 294, 264, 8062, 257, 307, 17207, 538, 341, 39684, 269, 13], "temperature": 0.0, "avg_logprob": -0.1180296980816385, "compression_ratio": 2.1864406779661016, "no_speech_prob": 9.998800669563934e-05}, {"id": 140, "seek": 56000, "start": 571.0, "end": 587.0, "text": " So the first element in our vector c times a is given by c times a one and the last element is given by c times a and and the result of scalar multiplication of a scalar with the vector in Rn is again an element in Rn.", "tokens": [407, 264, 700, 4478, 294, 527, 8062, 269, 1413, 257, 307, 2212, 538, 269, 1413, 257, 472, 293, 264, 1036, 4478, 307, 2212, 538, 269, 1413, 257, 293, 293, 264, 1874, 295, 39684, 27290, 295, 257, 39684, 365, 264, 8062, 294, 497, 77, 307, 797, 364, 4478, 294, 497, 77, 13], "temperature": 0.0, "avg_logprob": -0.1180296980816385, "compression_ratio": 2.1864406779661016, "no_speech_prob": 9.998800669563934e-05}, {"id": 141, "seek": 58700, "start": 587.0, "end": 600.0, "text": " And the figure here on the lower right of the slide again gives you a geometric interpretation of what the scalar multiplication is doing to a vector and essentially that just means that we can stretch our vector a.", "tokens": [400, 264, 2573, 510, 322, 264, 3126, 558, 295, 264, 4137, 797, 2709, 291, 257, 33246, 14174, 295, 437, 264, 39684, 27290, 307, 884, 281, 257, 8062, 293, 4476, 300, 445, 1355, 300, 321, 393, 5985, 527, 8062, 257, 13], "temperature": 0.0, "avg_logprob": -0.11588751418249947, "compression_ratio": 1.5911949685534592, "no_speech_prob": 6.196692265802994e-05}, {"id": 142, "seek": 58700, "start": 600.0, "end": 603.0, "text": " So we can stretch it by the scalar c.", "tokens": [407, 321, 393, 5985, 309, 538, 264, 39684, 269, 13], "temperature": 0.0, "avg_logprob": -0.11588751418249947, "compression_ratio": 1.5911949685534592, "no_speech_prob": 6.196692265802994e-05}, {"id": 143, "seek": 60300, "start": 603.0, "end": 618.0, "text": " If you have a scalar which is positive and bigger than one, then we stretch it and make the vector longer. If you have a scalar which is between 0 and 1, you can just make the vector smaller or shorter.", "tokens": [759, 291, 362, 257, 39684, 597, 307, 3353, 293, 3801, 813, 472, 11, 550, 321, 5985, 309, 293, 652, 264, 8062, 2854, 13, 759, 291, 362, 257, 39684, 597, 307, 1296, 1958, 293, 502, 11, 291, 393, 445, 652, 264, 8062, 4356, 420, 11639, 13], "temperature": 0.0, "avg_logprob": -0.08009160132635207, "compression_ratio": 1.7463414634146341, "no_speech_prob": 1.1593081580940634e-05}, {"id": 144, "seek": 60300, "start": 618.0, "end": 624.0, "text": " And if you now choose a negative scalar, you can also reverse the direction of the vector.", "tokens": [400, 498, 291, 586, 2826, 257, 3671, 39684, 11, 291, 393, 611, 9943, 264, 3513, 295, 264, 8062, 13], "temperature": 0.0, "avg_logprob": -0.08009160132635207, "compression_ratio": 1.7463414634146341, "no_speech_prob": 1.1593081580940634e-05}, {"id": 145, "seek": 60300, "start": 624.0, "end": 628.0, "text": " So this is geometrically speaking what scalar multiplication is.", "tokens": [407, 341, 307, 12956, 81, 984, 4124, 437, 39684, 27290, 307, 13], "temperature": 0.0, "avg_logprob": -0.08009160132635207, "compression_ratio": 1.7463414634146341, "no_speech_prob": 1.1593081580940634e-05}, {"id": 146, "seek": 62800, "start": 628.0, "end": 636.0, "text": " Okay, so finally let's have a look at the dot product which is maybe the most important operation of all of these.", "tokens": [1033, 11, 370, 2721, 718, 311, 362, 257, 574, 412, 264, 5893, 1674, 597, 307, 1310, 264, 881, 1021, 6916, 295, 439, 295, 613, 13], "temperature": 0.0, "avg_logprob": -0.1465605014079326, "compression_ratio": 1.605263157894737, "no_speech_prob": 6.915751873748377e-05}, {"id": 147, "seek": 62800, "start": 636.0, "end": 648.0, "text": " Okay, so for the dot product again we start with two vectors. So taking a and b as elements from Rn and then the dot product is defined as you just use the multiplication side for a times b.", "tokens": [1033, 11, 370, 337, 264, 5893, 1674, 797, 321, 722, 365, 732, 18875, 13, 407, 1940, 257, 293, 272, 382, 4959, 490, 497, 77, 293, 550, 264, 5893, 1674, 307, 7642, 382, 291, 445, 764, 264, 27290, 1252, 337, 257, 1413, 272, 13], "temperature": 0.0, "avg_logprob": -0.1465605014079326, "compression_ratio": 1.605263157894737, "no_speech_prob": 6.915751873748377e-05}, {"id": 148, "seek": 64800, "start": 648.0, "end": 663.0, "text": " But essentially it is like the multiplication between the vector, the transposed vector of a and the vector b. And in this case that just means that I take component bias to product the elements of the two vectors.", "tokens": [583, 4476, 309, 307, 411, 264, 27290, 1296, 264, 8062, 11, 264, 7132, 1744, 8062, 295, 257, 293, 264, 8062, 272, 13, 400, 294, 341, 1389, 300, 445, 1355, 300, 286, 747, 6542, 12577, 281, 1674, 264, 4959, 295, 264, 732, 18875, 13], "temperature": 0.0, "avg_logprob": -0.11355235841539171, "compression_ratio": 1.7880434782608696, "no_speech_prob": 8.396743396588136e-06}, {"id": 149, "seek": 64800, "start": 663.0, "end": 671.0, "text": " And then I take the sum over all of the entries of this resulting component bias multiplication of vector a and b.", "tokens": [400, 550, 286, 747, 264, 2408, 670, 439, 295, 264, 23041, 295, 341, 16505, 6542, 12577, 27290, 295, 8062, 257, 293, 272, 13], "temperature": 0.0, "avg_logprob": -0.11355235841539171, "compression_ratio": 1.7880434782608696, "no_speech_prob": 8.396743396588136e-06}, {"id": 150, "seek": 67100, "start": 671.0, "end": 682.0, "text": " So the result of the dot product is given here in the second and third line. So you can just see it's the sum over a i times b i.", "tokens": [407, 264, 1874, 295, 264, 5893, 1674, 307, 2212, 510, 294, 264, 1150, 293, 2636, 1622, 13, 407, 291, 393, 445, 536, 309, 311, 264, 2408, 670, 257, 741, 1413, 272, 741, 13], "temperature": 0.0, "avg_logprob": -0.10950676961378618, "compression_ratio": 1.796875, "no_speech_prob": 3.7572979636024684e-05}, {"id": 151, "seek": 67100, "start": 682.0, "end": 687.0, "text": " So the sum over component bias multiplication of the elements of the vector a and b.", "tokens": [407, 264, 2408, 670, 6542, 12577, 27290, 295, 264, 4959, 295, 264, 8062, 257, 293, 272, 13], "temperature": 0.0, "avg_logprob": -0.10950676961378618, "compression_ratio": 1.796875, "no_speech_prob": 3.7572979636024684e-05}, {"id": 152, "seek": 67100, "start": 687.0, "end": 697.0, "text": " Which is important to mention is now that the result of the dot product of two vectors in Rn is not any more element in Rn itself.", "tokens": [3013, 307, 1021, 281, 2152, 307, 586, 300, 264, 1874, 295, 264, 5893, 1674, 295, 732, 18875, 294, 497, 77, 307, 406, 604, 544, 4478, 294, 497, 77, 2564, 13], "temperature": 0.0, "avg_logprob": -0.10950676961378618, "compression_ratio": 1.796875, "no_speech_prob": 3.7572979636024684e-05}, {"id": 153, "seek": 69700, "start": 697.0, "end": 704.0, "text": " But now we obtain an element in R so scalar. So it's important to remember for the dot product.", "tokens": [583, 586, 321, 12701, 364, 4478, 294, 497, 370, 39684, 13, 407, 309, 311, 1021, 281, 1604, 337, 264, 5893, 1674, 13], "temperature": 0.0, "avg_logprob": -0.09598729965534616, "compression_ratio": 1.7766990291262137, "no_speech_prob": 3.272589310654439e-05}, {"id": 154, "seek": 69700, "start": 704.0, "end": 711.0, "text": " The dot product now has some interesting properties. So the first one is that the dot product is a commutative operation.", "tokens": [440, 5893, 1674, 586, 575, 512, 1880, 7221, 13, 407, 264, 700, 472, 307, 300, 264, 5893, 1674, 307, 257, 800, 325, 1166, 6916, 13], "temperature": 0.0, "avg_logprob": -0.09598729965534616, "compression_ratio": 1.7766990291262137, "no_speech_prob": 3.272589310654439e-05}, {"id": 155, "seek": 69700, "start": 711.0, "end": 721.0, "text": " That means that if I take two vectors a and b, then the dot product of a and b so a times b equals to b times a. So the dot product between b and a.", "tokens": [663, 1355, 300, 498, 286, 747, 732, 18875, 257, 293, 272, 11, 550, 264, 5893, 1674, 295, 257, 293, 272, 370, 257, 1413, 272, 6915, 281, 272, 1413, 257, 13, 407, 264, 5893, 1674, 1296, 272, 293, 257, 13], "temperature": 0.0, "avg_logprob": -0.09598729965534616, "compression_ratio": 1.7766990291262137, "no_speech_prob": 3.272589310654439e-05}, {"id": 156, "seek": 72100, "start": 721.0, "end": 729.0, "text": " This is interesting. We also have that for addition and the scalar multiplication as well. So all of them are commutative.", "tokens": [639, 307, 1880, 13, 492, 611, 362, 300, 337, 4500, 293, 264, 39684, 27290, 382, 731, 13, 407, 439, 295, 552, 366, 800, 325, 1166, 13], "temperature": 0.0, "avg_logprob": -0.13058300540871817, "compression_ratio": 1.5743589743589743, "no_speech_prob": 5.4103795264381915e-05}, {"id": 157, "seek": 72100, "start": 729.0, "end": 736.0, "text": " Another one is the geometric interpretation, which is in this case it is a bit more complicated than the cases before.", "tokens": [3996, 472, 307, 264, 33246, 14174, 11, 597, 307, 294, 341, 1389, 309, 307, 257, 857, 544, 6179, 813, 264, 3331, 949, 13], "temperature": 0.0, "avg_logprob": -0.13058300540871817, "compression_ratio": 1.5743589743589743, "no_speech_prob": 5.4103795264381915e-05}, {"id": 158, "seek": 72100, "start": 736.0, "end": 739.0, "text": " So let's have a look at the figure that I included on this slide.", "tokens": [407, 718, 311, 362, 257, 574, 412, 264, 2573, 300, 286, 5556, 322, 341, 4137, 13], "temperature": 0.0, "avg_logprob": -0.13058300540871817, "compression_ratio": 1.5743589743589743, "no_speech_prob": 5.4103795264381915e-05}, {"id": 159, "seek": 73900, "start": 739.0, "end": 754.0, "text": " There you see two vectors x and y and we usually denote that the vectors by a and b so that's kind of the same. And you see that between these vectors x and y you can define the angle theta between these two vectors.", "tokens": [821, 291, 536, 732, 18875, 2031, 293, 288, 293, 321, 2673, 45708, 300, 264, 18875, 538, 257, 293, 272, 370, 300, 311, 733, 295, 264, 912, 13, 400, 291, 536, 300, 1296, 613, 18875, 2031, 293, 288, 291, 393, 6964, 264, 5802, 9725, 1296, 613, 732, 18875, 13], "temperature": 0.0, "avg_logprob": -0.1175948056307706, "compression_ratio": 1.7746478873239437, "no_speech_prob": 2.212082654295955e-05}, {"id": 160, "seek": 73900, "start": 754.0, "end": 767.0, "text": " So the dot product is now in relation to this angle and it's given by the second line geometric interpretation that I gave a formula which is given by a times b.", "tokens": [407, 264, 5893, 1674, 307, 586, 294, 9721, 281, 341, 5802, 293, 309, 311, 2212, 538, 264, 1150, 1622, 33246, 14174, 300, 286, 2729, 257, 8513, 597, 307, 2212, 538, 257, 1413, 272, 13], "temperature": 0.0, "avg_logprob": -0.1175948056307706, "compression_ratio": 1.7746478873239437, "no_speech_prob": 2.212082654295955e-05}, {"id": 161, "seek": 76700, "start": 767.0, "end": 778.0, "text": " So the dot product of a and b is equal to the length of a times the length of b times cosine of theta. So cosine of the angle between our two vectors a and b.", "tokens": [407, 264, 5893, 1674, 295, 257, 293, 272, 307, 2681, 281, 264, 4641, 295, 257, 1413, 264, 4641, 295, 272, 1413, 23565, 295, 9725, 13, 407, 23565, 295, 264, 5802, 1296, 527, 732, 18875, 257, 293, 272, 13], "temperature": 0.0, "avg_logprob": -0.07341257301536766, "compression_ratio": 1.8901734104046244, "no_speech_prob": 1.9646831788122654e-05}, {"id": 162, "seek": 76700, "start": 778.0, "end": 790.0, "text": " So this is a property and interpretation of the dot product which is really interesting because it refers to this more complex concept of the angle between two vectors.", "tokens": [407, 341, 307, 257, 4707, 293, 14174, 295, 264, 5893, 1674, 597, 307, 534, 1880, 570, 309, 14942, 281, 341, 544, 3997, 3410, 295, 264, 5802, 1296, 732, 18875, 13], "temperature": 0.0, "avg_logprob": -0.07341257301536766, "compression_ratio": 1.8901734104046244, "no_speech_prob": 1.9646831788122654e-05}, {"id": 163, "seek": 79000, "start": 790.0, "end": 804.0, "text": " You can never look at orthogonal vectors. What does it mean to be orthogonal for two vectors? Two vectors are orthogonal if the angle between them is exactly 90 degrees. So we have a right angle like this.", "tokens": [509, 393, 1128, 574, 412, 41488, 18875, 13, 708, 775, 309, 914, 281, 312, 41488, 337, 732, 18875, 30, 4453, 18875, 366, 41488, 498, 264, 5802, 1296, 552, 307, 2293, 4289, 5310, 13, 407, 321, 362, 257, 558, 5802, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.1302069936479841, "compression_ratio": 1.6544502617801047, "no_speech_prob": 9.651213986217044e-06}, {"id": 164, "seek": 79000, "start": 804.0, "end": 812.0, "text": " And one property in relation to our dot product is now that two non zero vectors are orthogonal to each other.", "tokens": [400, 472, 4707, 294, 9721, 281, 527, 5893, 1674, 307, 586, 300, 732, 2107, 4018, 18875, 366, 41488, 281, 1184, 661, 13], "temperature": 0.0, "avg_logprob": -0.1302069936479841, "compression_ratio": 1.6544502617801047, "no_speech_prob": 9.651213986217044e-06}, {"id": 165, "seek": 81200, "start": 812.0, "end": 823.0, "text": " So if I know the dot product is zero, I can conclude that they are orthogonal to each other and the other way around.", "tokens": [407, 498, 286, 458, 264, 5893, 1674, 307, 4018, 11, 286, 393, 16886, 300, 436, 366, 41488, 281, 1184, 661, 293, 264, 661, 636, 926, 13], "temperature": 0.0, "avg_logprob": -0.15310623334801715, "compression_ratio": 1.426356589147287, "no_speech_prob": 2.0955818399670534e-05}, {"id": 166, "seek": 81200, "start": 823.0, "end": 829.0, "text": " So this is a nice interpretation geometrically of the dot product.", "tokens": [407, 341, 307, 257, 1481, 14174, 12956, 81, 984, 295, 264, 5893, 1674, 13], "temperature": 0.0, "avg_logprob": -0.15310623334801715, "compression_ratio": 1.426356589147287, "no_speech_prob": 2.0955818399670534e-05}, {"id": 167, "seek": 82900, "start": 829.0, "end": 849.0, "text": " Next slide, I included a figure where you can nicely see as well this interpretation with the angle between two vectors. I just start the animation and you can see that if the angle between two vectors is bigger than 90 degrees, you obtain a negative dot product.", "tokens": [3087, 4137, 11, 286, 5556, 257, 2573, 689, 291, 393, 9594, 536, 382, 731, 341, 14174, 365, 264, 5802, 1296, 732, 18875, 13, 286, 445, 722, 264, 9603, 293, 291, 393, 536, 300, 498, 264, 5802, 1296, 732, 18875, 307, 3801, 813, 4289, 5310, 11, 291, 12701, 257, 3671, 5893, 1674, 13], "temperature": 0.0, "avg_logprob": -0.1064463768686567, "compression_ratio": 1.593939393939394, "no_speech_prob": 3.343226126162335e-05}, {"id": 168, "seek": 84900, "start": 849.0, "end": 863.0, "text": " So the angle is exactly 90 degrees than our dot product is zero. And if the angle is smaller than 90 degrees, then we have a positive dot product. So this is something which you should know about the dot product which is quite interesting.", "tokens": [407, 264, 5802, 307, 2293, 4289, 5310, 813, 527, 5893, 1674, 307, 4018, 13, 400, 498, 264, 5802, 307, 4356, 813, 4289, 5310, 11, 550, 321, 362, 257, 3353, 5893, 1674, 13, 407, 341, 307, 746, 597, 291, 820, 458, 466, 264, 5893, 1674, 597, 307, 1596, 1880, 13], "temperature": 0.0, "avg_logprob": -0.15041703616871555, "compression_ratio": 1.6636363636363636, "no_speech_prob": 2.6037612769869156e-05}, {"id": 169, "seek": 84900, "start": 863.0, "end": 872.0, "text": " So let's continue with matrices. As I mentioned before, matrix is an element of the set or the space R to the power n times m.", "tokens": [407, 718, 311, 2354, 365, 32284, 13, 1018, 286, 2835, 949, 11, 8141, 307, 364, 4478, 295, 264, 992, 420, 264, 1901, 497, 281, 264, 1347, 297, 1413, 275, 13], "temperature": 0.0, "avg_logprob": -0.15041703616871555, "compression_ratio": 1.6636363636363636, "no_speech_prob": 2.6037612769869156e-05}, {"id": 170, "seek": 87200, "start": 872.0, "end": 882.0, "text": " So we have noted with the matrix notation, which is essentially matrix, including all the elements starting with the element a one one up to the element a and m.", "tokens": [407, 321, 362, 12964, 365, 264, 8141, 24657, 11, 597, 307, 4476, 8141, 11, 3009, 439, 264, 4959, 2891, 365, 264, 4478, 257, 472, 472, 493, 281, 264, 4478, 257, 293, 275, 13], "temperature": 0.0, "avg_logprob": -0.28010107066533335, "compression_ratio": 1.7591623036649215, "no_speech_prob": 0.0001282204029848799}, {"id": 171, "seek": 87200, "start": 882.0, "end": 893.0, "text": " And on the matrix, we can define also basic operations. In our case, we will have a look at matrix vector multiplication matrix matrix multiplication and the hardmer product.", "tokens": [400, 322, 264, 8141, 11, 321, 393, 6964, 611, 3875, 7705, 13, 682, 527, 1389, 11, 321, 486, 362, 257, 574, 412, 8141, 8062, 27290, 8141, 8141, 27290, 293, 264, 1152, 936, 1674, 13], "temperature": 0.0, "avg_logprob": -0.28010107066533335, "compression_ratio": 1.7591623036649215, "no_speech_prob": 0.0001282204029848799}, {"id": 172, "seek": 89300, "start": 893.0, "end": 909.0, "text": " And I think the first two are well known from all of you, the harder mud product is something which will not often define a mathematical lecture, but it's a basic operation, which is really heavily used in the I to the setting, and which is also not very difficult.", "tokens": [400, 286, 519, 264, 700, 732, 366, 731, 2570, 490, 439, 295, 291, 11, 264, 6081, 8933, 1674, 307, 746, 597, 486, 406, 2049, 6964, 257, 18894, 7991, 11, 457, 309, 311, 257, 3875, 6916, 11, 597, 307, 534, 10950, 1143, 294, 264, 286, 281, 264, 3287, 11, 293, 597, 307, 611, 406, 588, 2252, 13], "temperature": 0.0, "avg_logprob": -0.22670899816306242, "compression_ratio": 1.537117903930131, "no_speech_prob": 4.506922050495632e-05}, {"id": 173, "seek": 89300, "start": 909.0, "end": 912.0, "text": " So we also have a look at this one.", "tokens": [407, 321, 611, 362, 257, 574, 412, 341, 472, 13], "temperature": 0.0, "avg_logprob": -0.22670899816306242, "compression_ratio": 1.537117903930131, "no_speech_prob": 4.506922050495632e-05}, {"id": 174, "seek": 89300, "start": 912.0, "end": 916.0, "text": " Let's start with the matrix vector multiplication.", "tokens": [961, 311, 722, 365, 264, 8141, 8062, 27290, 13], "temperature": 0.0, "avg_logprob": -0.22670899816306242, "compression_ratio": 1.537117903930131, "no_speech_prob": 4.506922050495632e-05}, {"id": 175, "seek": 91600, "start": 916.0, "end": 932.0, "text": " The name is already mentioning or saying we have here a multiplication between the matrix and the vector. So we start with a matrix a and we take it from a set R to the power n times m and a vector from the set R to the power m.", "tokens": [440, 1315, 307, 1217, 18315, 420, 1566, 321, 362, 510, 257, 27290, 1296, 264, 8141, 293, 264, 8062, 13, 407, 321, 722, 365, 257, 8141, 257, 293, 321, 747, 309, 490, 257, 992, 497, 281, 264, 1347, 297, 1413, 275, 293, 257, 8062, 490, 264, 992, 497, 281, 264, 1347, 275, 13], "temperature": 0.0, "avg_logprob": -0.16669631004333496, "compression_ratio": 1.5944055944055944, "no_speech_prob": 9.483000030741096e-05}, {"id": 176, "seek": 93200, "start": 932.0, "end": 952.0, "text": " And now the multiplication between this matrix and this vector like a times b is given in this formula that I included in our slide here. So we have our matrix a we have our vector b and essentially what we are doing is we lay our we have our matrix and we lay our vector on top of the matrix.", "tokens": [400, 586, 264, 27290, 1296, 341, 8141, 293, 341, 8062, 411, 257, 1413, 272, 307, 2212, 294, 341, 8513, 300, 286, 5556, 294, 527, 4137, 510, 13, 407, 321, 362, 527, 8141, 257, 321, 362, 527, 8062, 272, 293, 4476, 437, 321, 366, 884, 307, 321, 2360, 527, 321, 362, 527, 8141, 293, 321, 2360, 527, 8062, 322, 1192, 295, 264, 8141, 13], "temperature": 0.0, "avg_logprob": -0.16271475891568768, "compression_ratio": 1.723529411764706, "no_speech_prob": 1.053731466527097e-05}, {"id": 177, "seek": 95200, "start": 952.0, "end": 962.0, "text": " And what we then do is we do kind of a dot product with each row of the matrix. So the first element of the result, the first component, the first row.", "tokens": [400, 437, 321, 550, 360, 307, 321, 360, 733, 295, 257, 5893, 1674, 365, 1184, 5386, 295, 264, 8141, 13, 407, 264, 700, 4478, 295, 264, 1874, 11, 264, 700, 6542, 11, 264, 700, 5386, 13], "temperature": 0.0, "avg_logprob": -0.10706775453355577, "compression_ratio": 2.05, "no_speech_prob": 1.7832664525485598e-05}, {"id": 178, "seek": 95200, "start": 962.0, "end": 976.0, "text": " And it's the dot product of the first row of the matrix a together with the vector b. And then you continue doing that the last rows, then the dot product between the last row of the matrix a together with a vector b.", "tokens": [400, 309, 311, 264, 5893, 1674, 295, 264, 700, 5386, 295, 264, 8141, 257, 1214, 365, 264, 8062, 272, 13, 400, 550, 291, 2354, 884, 300, 264, 1036, 13241, 11, 550, 264, 5893, 1674, 1296, 264, 1036, 5386, 295, 264, 8141, 257, 1214, 365, 257, 8062, 272, 13], "temperature": 0.0, "avg_logprob": -0.10706775453355577, "compression_ratio": 2.05, "no_speech_prob": 1.7832664525485598e-05}, {"id": 179, "seek": 97600, "start": 976.0, "end": 986.0, "text": " And of course, as you see here the outcome of the matrix vector multiplication between a and b is here an element of R and so has dimension n.", "tokens": [400, 295, 1164, 11, 382, 291, 536, 510, 264, 9700, 295, 264, 8141, 8062, 27290, 1296, 257, 293, 272, 307, 510, 364, 4478, 295, 497, 293, 370, 575, 10139, 297, 13], "temperature": 0.0, "avg_logprob": -0.12203964288683905, "compression_ratio": 1.7740112994350283, "no_speech_prob": 3.679965084302239e-05}, {"id": 180, "seek": 97600, "start": 986.0, "end": 996.0, "text": " And this is important if you do matrix vector multiplication and also all the other multiplications on matrices, it's important that you always take care of the dimension.", "tokens": [400, 341, 307, 1021, 498, 291, 360, 8141, 8062, 27290, 293, 611, 439, 264, 661, 17596, 763, 322, 32284, 11, 309, 311, 1021, 300, 291, 1009, 747, 1127, 295, 264, 10139, 13], "temperature": 0.0, "avg_logprob": -0.12203964288683905, "compression_ratio": 1.7740112994350283, "no_speech_prob": 3.679965084302239e-05}, {"id": 181, "seek": 99600, "start": 996.0, "end": 1013.0, "text": " And you have to be fitting to each other because otherwise it's not well defined. So in our case, like you see here on the second point, it's important to make sure that in our case a is of dimension n times m and our vector has to be of dimension m times one.", "tokens": [400, 291, 362, 281, 312, 15669, 281, 1184, 661, 570, 5911, 309, 311, 406, 731, 7642, 13, 407, 294, 527, 1389, 11, 411, 291, 536, 510, 322, 264, 1150, 935, 11, 309, 311, 1021, 281, 652, 988, 300, 294, 527, 1389, 257, 307, 295, 10139, 297, 1413, 275, 293, 527, 8062, 575, 281, 312, 295, 10139, 275, 1413, 472, 13], "temperature": 0.0, "avg_logprob": -0.14975011348724365, "compression_ratio": 1.5950920245398772, "no_speech_prob": 6.23595406068489e-05}, {"id": 182, "seek": 101300, "start": 1013.0, "end": 1027.0, "text": " And just the dimension, the second dimension of our matrix has to be equal to the first dimension of our vector, otherwise it's not defined. And this is easily seen by the definition of the matrix vector of multiplication.", "tokens": [400, 445, 264, 10139, 11, 264, 1150, 10139, 295, 527, 8141, 575, 281, 312, 2681, 281, 264, 700, 10139, 295, 527, 8062, 11, 5911, 309, 311, 406, 7642, 13, 400, 341, 307, 3612, 1612, 538, 264, 7123, 295, 264, 8141, 8062, 295, 27290, 13], "temperature": 0.0, "avg_logprob": -0.12505928675333658, "compression_ratio": 1.8074866310160427, "no_speech_prob": 1.3846718502463773e-05}, {"id": 183, "seek": 101300, "start": 1027.0, "end": 1036.0, "text": " And as I said before, the outcome is now an element, a vector of dimension n times one, so a vector of dimension n.", "tokens": [400, 382, 286, 848, 949, 11, 264, 9700, 307, 586, 364, 4478, 11, 257, 8062, 295, 10139, 297, 1413, 472, 11, 370, 257, 8062, 295, 10139, 297, 13], "temperature": 0.0, "avg_logprob": -0.12505928675333658, "compression_ratio": 1.8074866310160427, "no_speech_prob": 1.3846718502463773e-05}, {"id": 184, "seek": 103600, "start": 1036.0, "end": 1046.0, "text": " And the lower part of the slide, you can see an example which uses a matrix of dimension three times one and a vector of dimension two.", "tokens": [400, 264, 3126, 644, 295, 264, 4137, 11, 291, 393, 536, 364, 1365, 597, 4960, 257, 8141, 295, 10139, 1045, 1413, 472, 293, 257, 8062, 295, 10139, 732, 13], "temperature": 0.0, "avg_logprob": -0.09192839646950746, "compression_ratio": 1.8634146341463416, "no_speech_prob": 3.4604116081027314e-05}, {"id": 185, "seek": 103600, "start": 1046.0, "end": 1062.0, "text": " And then you get the matrix multiplication delivers an element of dimension three. And here you can just test your understanding of this multiplication by just doing this exercise on your own and comparing your solution to the solution down here.", "tokens": [400, 550, 291, 483, 264, 8141, 27290, 24860, 364, 4478, 295, 10139, 1045, 13, 400, 510, 291, 393, 445, 1500, 428, 3701, 295, 341, 27290, 538, 445, 884, 341, 5380, 322, 428, 1065, 293, 15763, 428, 3827, 281, 264, 3827, 760, 510, 13], "temperature": 0.0, "avg_logprob": -0.09192839646950746, "compression_ratio": 1.8634146341463416, "no_speech_prob": 3.4604116081027314e-05}, {"id": 186, "seek": 106200, "start": 1062.0, "end": 1073.0, "text": " And then we just continue with the matrix matrix multiplication. And as the name the same, this is just a generalization of the multiplication that we saw before of the matrix of the vector multiplication.", "tokens": [400, 550, 321, 445, 2354, 365, 264, 8141, 8141, 27290, 13, 400, 382, 264, 1315, 264, 912, 11, 341, 307, 445, 257, 2674, 2144, 295, 264, 27290, 300, 321, 1866, 949, 295, 264, 8141, 295, 264, 8062, 27290, 13], "temperature": 0.0, "avg_logprob": -0.24873034159342447, "compression_ratio": 1.8826815642458101, "no_speech_prob": 0.00012522170436568558}, {"id": 187, "seek": 106200, "start": 1073.0, "end": 1082.0, "text": " In this case, we don't take a vector B, but we take a second matrix and then we multiply multiply the two matrices with these data.", "tokens": [682, 341, 1389, 11, 321, 500, 380, 747, 257, 8062, 363, 11, 457, 321, 747, 257, 1150, 8141, 293, 550, 321, 12972, 12972, 264, 732, 32284, 365, 613, 1412, 13], "temperature": 0.0, "avg_logprob": -0.24873034159342447, "compression_ratio": 1.8826815642458101, "no_speech_prob": 0.00012522170436568558}, {"id": 188, "seek": 108200, "start": 1082.0, "end": 1092.0, "text": " And then we multiply the two matrices with the matrix A, which is from the set R to the power n times m and B, which is an element of the set R to the power m times L.", "tokens": [400, 550, 321, 12972, 264, 732, 32284, 365, 264, 8141, 316, 11, 597, 307, 490, 264, 992, 497, 281, 264, 1347, 297, 1413, 275, 293, 363, 11, 597, 307, 364, 4478, 295, 264, 992, 497, 281, 264, 1347, 275, 1413, 441, 13], "temperature": 0.0, "avg_logprob": -0.19730710481342517, "compression_ratio": 1.897196261682243, "no_speech_prob": 0.00011007584544131532}, {"id": 189, "seek": 108200, "start": 1092.0, "end": 1107.0, "text": " Again, have a look at the dimensions here because that's important. And the multiplication is now given by the multiplication between A and B, between these two matrices, where each element in the outcome, the outcome is again a matrix of", "tokens": [3764, 11, 362, 257, 574, 412, 264, 12819, 510, 570, 300, 311, 1021, 13, 400, 264, 27290, 307, 586, 2212, 538, 264, 27290, 1296, 316, 293, 363, 11, 1296, 613, 732, 32284, 11, 689, 1184, 4478, 294, 264, 9700, 11, 264, 9700, 307, 797, 257, 8141, 295], "temperature": 0.0, "avg_logprob": -0.19730710481342517, "compression_ratio": 1.897196261682243, "no_speech_prob": 0.00011007584544131532}, {"id": 190, "seek": 110700, "start": 1107.0, "end": 1122.0, "text": " dimension n times L. And each element in the outcome of this multiplication is essentially just the dot product between one row in the matrix A and the column in the matrix B.", "tokens": [10139, 297, 1413, 441, 13, 400, 1184, 4478, 294, 264, 9700, 295, 341, 27290, 307, 4476, 445, 264, 5893, 1674, 1296, 472, 5386, 294, 264, 8141, 316, 293, 264, 7738, 294, 264, 8141, 363, 13], "temperature": 0.0, "avg_logprob": -0.09242554811330941, "compression_ratio": 1.4112903225806452, "no_speech_prob": 6.592192221432924e-05}, {"id": 191, "seek": 112200, "start": 1122.0, "end": 1137.0, "text": " So if you have a look at the second line of equations here on the slide, C ij, which is just one element in the resulting matrix is now the dot product between the Ith row of the matrix A and the J's column of the matrix B.", "tokens": [407, 498, 291, 362, 257, 574, 412, 264, 1150, 1622, 295, 11787, 510, 322, 264, 4137, 11, 383, 741, 73, 11, 597, 307, 445, 472, 4478, 294, 264, 16505, 8141, 307, 586, 264, 5893, 1674, 1296, 264, 286, 392, 5386, 295, 264, 8141, 316, 293, 264, 508, 311, 7738, 295, 264, 8141, 363, 13], "temperature": 0.0, "avg_logprob": -0.1585515340169271, "compression_ratio": 1.7662337662337662, "no_speech_prob": 9.865386346064042e-06}, {"id": 192, "seek": 112200, "start": 1137.0, "end": 1150.0, "text": " So that's the definition. And then you just build up this whole resulting matrix with n elements or n times L elements. And that gives you then the matrix matrix multiplication result.", "tokens": [407, 300, 311, 264, 7123, 13, 400, 550, 291, 445, 1322, 493, 341, 1379, 16505, 8141, 365, 297, 4959, 420, 297, 1413, 441, 4959, 13, 400, 300, 2709, 291, 550, 264, 8141, 8141, 27290, 1874, 13], "temperature": 0.0, "avg_logprob": -0.1585515340169271, "compression_ratio": 1.7662337662337662, "no_speech_prob": 9.865386346064042e-06}, {"id": 193, "seek": 115000, "start": 1150.0, "end": 1171.0, "text": " And again, I also want to point at the dimensions again, we started here with an element A from dimension n times m and B has dimension m times L. And that's important for this multiplication that the second dimension of the matrix A equals the first dimension of the matrix B.", "tokens": [400, 797, 11, 286, 611, 528, 281, 935, 412, 264, 12819, 797, 11, 321, 1409, 510, 365, 364, 4478, 316, 490, 10139, 297, 1413, 275, 293, 363, 575, 10139, 275, 1413, 441, 13, 400, 300, 311, 1021, 337, 341, 27290, 300, 264, 1150, 10139, 295, 264, 8141, 316, 6915, 264, 700, 10139, 295, 264, 8141, 363, 13], "temperature": 0.0, "avg_logprob": -0.12163524158665391, "compression_ratio": 1.6993865030674846, "no_speech_prob": 8.525388693669811e-05}, {"id": 194, "seek": 117100, "start": 1171.0, "end": 1186.0, "text": " Because otherwise you can just see it by the definition that multiplication wouldn't be identified. So always make sure that you check the dimensions and we also have an exercise which is pointing to this dimension problem of the multiplication.", "tokens": [1436, 5911, 291, 393, 445, 536, 309, 538, 264, 7123, 300, 27290, 2759, 380, 312, 9234, 13, 407, 1009, 652, 988, 300, 291, 1520, 264, 12819, 293, 321, 611, 362, 364, 5380, 597, 307, 12166, 281, 341, 10139, 1154, 295, 264, 27290, 13], "temperature": 0.0, "avg_logprob": -0.14043059247605344, "compression_ratio": 1.5123456790123457, "no_speech_prob": 4.071439252584241e-05}, {"id": 195, "seek": 118600, "start": 1186.0, "end": 1201.0, "text": " And one last thing before I mentioned that the operations that we have been looking at for the vectors are commutative in this case matrix matrix multiplication is not a commutative operation.", "tokens": [400, 472, 1036, 551, 949, 286, 2835, 300, 264, 7705, 300, 321, 362, 668, 1237, 412, 337, 264, 18875, 366, 800, 325, 1166, 294, 341, 1389, 8141, 8141, 27290, 307, 406, 257, 800, 325, 1166, 6916, 13], "temperature": 0.0, "avg_logprob": -0.10847027270824879, "compression_ratio": 1.725, "no_speech_prob": 1.8674174498301e-05}, {"id": 196, "seek": 118600, "start": 1201.0, "end": 1211.0, "text": " So you're not allowed to just switch the order of the multiplication and say that A times B is the same than B times A because that's here not the case.", "tokens": [407, 291, 434, 406, 4350, 281, 445, 3679, 264, 1668, 295, 264, 27290, 293, 584, 300, 316, 1413, 363, 307, 264, 912, 813, 363, 1413, 316, 570, 300, 311, 510, 406, 264, 1389, 13], "temperature": 0.0, "avg_logprob": -0.10847027270824879, "compression_ratio": 1.725, "no_speech_prob": 1.8674174498301e-05}, {"id": 197, "seek": 121100, "start": 1211.0, "end": 1225.0, "text": " So that's important to say it's always pretty easy if you have something which is commutative in this case matrix matrix multiplication and also matrix vector multiplication is not commutative.", "tokens": [407, 300, 311, 1021, 281, 584, 309, 311, 1009, 1238, 1858, 498, 291, 362, 746, 597, 307, 800, 325, 1166, 294, 341, 1389, 8141, 8141, 27290, 293, 611, 8141, 8062, 27290, 307, 406, 800, 325, 1166, 13], "temperature": 0.0, "avg_logprob": -0.11441969290012266, "compression_ratio": 1.5317460317460319, "no_speech_prob": 9.592425340088084e-05}, {"id": 198, "seek": 122500, "start": 1225.0, "end": 1243.0, "text": " So last but not least, the other part of the product I've been talking about that in the introduction of the basic operation on matrices is a really easy operation because it's not using the product in the multiplication, but it's just the component wise multiplication of two matrices.", "tokens": [407, 1036, 457, 406, 1935, 11, 264, 661, 644, 295, 264, 1674, 286, 600, 668, 1417, 466, 300, 294, 264, 9339, 295, 264, 3875, 6916, 322, 32284, 307, 257, 534, 1858, 6916, 570, 309, 311, 406, 1228, 264, 1674, 294, 264, 27290, 11, 457, 309, 311, 445, 264, 6542, 10829, 27290, 295, 732, 32284, 13], "temperature": 0.0, "avg_logprob": -0.2911443871966863, "compression_ratio": 1.7333333333333334, "no_speech_prob": 1.1132153304060921e-05}, {"id": 199, "seek": 124300, "start": 1243.0, "end": 1264.0, "text": " So we start with two matrices A and B in this case now we have the same dimension for both of the matrices. So both of them are an element of the set R to the power n times M and what we then do we define this heart am I product which is denoted by a multiplication sign and the circle around it.", "tokens": [407, 321, 722, 365, 732, 32284, 316, 293, 363, 294, 341, 1389, 586, 321, 362, 264, 912, 10139, 337, 1293, 295, 264, 32284, 13, 407, 1293, 295, 552, 366, 364, 4478, 295, 264, 992, 497, 281, 264, 1347, 297, 1413, 376, 293, 437, 321, 550, 360, 321, 6964, 341, 1917, 669, 286, 1674, 597, 307, 1441, 23325, 538, 257, 27290, 1465, 293, 264, 6329, 926, 309, 13], "temperature": 0.0, "avg_logprob": -0.2775466542848399, "compression_ratio": 1.574468085106383, "no_speech_prob": 4.5060332922730595e-05}, {"id": 200, "seek": 126400, "start": 1264.0, "end": 1284.0, "text": " So we say the result of the heart am I product is a matrix where we just have component wise multiplication. So if you look at the first entry at space 1 1 you can just see that the entry in resulting matrix is A 1 1 times B 1 1 and then you do that component wise for all the entries in the matrices.", "tokens": [407, 321, 584, 264, 1874, 295, 264, 1917, 669, 286, 1674, 307, 257, 8141, 689, 321, 445, 362, 6542, 10829, 27290, 13, 407, 498, 291, 574, 412, 264, 700, 8729, 412, 1901, 502, 502, 291, 393, 445, 536, 300, 264, 8729, 294, 16505, 8141, 307, 316, 502, 502, 1413, 363, 502, 502, 293, 550, 291, 360, 300, 6542, 10829, 337, 439, 264, 23041, 294, 264, 32284, 13], "temperature": 0.0, "avg_logprob": -0.23054872432225187, "compression_ratio": 1.6629834254143647, "no_speech_prob": 0.00011604274914134294}, {"id": 201, "seek": 128400, "start": 1284.0, "end": 1302.0, "text": " So it's giving you a matrix of dimension n times M and this is the heart am I product so just component wise multiplication. And I think that's maybe the most simple operations on matrices, but as I said, it's highly important for our I2DL lecture.", "tokens": [407, 309, 311, 2902, 291, 257, 8141, 295, 10139, 297, 1413, 376, 293, 341, 307, 264, 1917, 669, 286, 1674, 370, 445, 6542, 10829, 27290, 13, 400, 286, 519, 300, 311, 1310, 264, 881, 2199, 7705, 322, 32284, 11, 457, 382, 286, 848, 11, 309, 311, 5405, 1021, 337, 527, 286, 17, 35, 43, 7991, 13], "temperature": 0.0, "avg_logprob": -0.19328114191691081, "compression_ratio": 1.3626373626373627, "no_speech_prob": 3.582156205084175e-05}, {"id": 202, "seek": 130200, "start": 1302.0, "end": 1317.0, "text": " So make sure that you know the difference between other multiplication on matrices and always make sure that the dimensions are fitting because that's kind of the most important thing when you work with matrices that you always take care of the dimensions and that they fit together.", "tokens": [407, 652, 988, 300, 291, 458, 264, 2649, 1296, 661, 27290, 322, 32284, 293, 1009, 652, 988, 300, 264, 12819, 366, 15669, 570, 300, 311, 733, 295, 264, 881, 1021, 551, 562, 291, 589, 365, 32284, 300, 291, 1009, 747, 1127, 295, 264, 12819, 293, 300, 436, 3318, 1214, 13], "temperature": 0.0, "avg_logprob": -0.13567900657653809, "compression_ratio": 1.725609756097561, "no_speech_prob": 6.41610604361631e-05}, {"id": 203, "seek": 131700, "start": 1317.0, "end": 1335.0, "text": " As I promised before we also want to look at tensors. Then those are essentially a multi-dimensional array and they are a generalization of the concept of vectors and matrices. So on the figure that I included on this slide you can just see the relationship to scalars, vectors and matrices.", "tokens": [1018, 286, 10768, 949, 321, 611, 528, 281, 574, 412, 10688, 830, 13, 1396, 729, 366, 4476, 257, 4825, 12, 18759, 10225, 293, 436, 366, 257, 2674, 2144, 295, 264, 3410, 295, 18875, 293, 32284, 13, 407, 322, 264, 2573, 300, 286, 5556, 322, 341, 4137, 291, 393, 445, 536, 264, 2480, 281, 15664, 685, 11, 18875, 293, 32284, 13], "temperature": 0.0, "avg_logprob": -0.13245125114917755, "compression_ratio": 1.572972972972973, "no_speech_prob": 6.690991085633868e-06}, {"id": 204, "seek": 133500, "start": 1335.0, "end": 1347.0, "text": " So let's have a look at scalars first. The scalar is just an element from a set. In our case we look at the set R so the set of really numbers so scalar is just one of the real numbers that we choose arbitrarily.", "tokens": [407, 718, 311, 362, 257, 574, 412, 15664, 685, 700, 13, 440, 39684, 307, 445, 364, 4478, 490, 257, 992, 13, 682, 527, 1389, 321, 574, 412, 264, 992, 497, 370, 264, 992, 295, 534, 3547, 370, 39684, 307, 445, 472, 295, 264, 957, 3547, 300, 321, 2826, 19071, 3289, 13], "temperature": 0.0, "avg_logprob": -0.1263206173675229, "compression_ratio": 1.7777777777777777, "no_speech_prob": 1.6282898286590353e-05}, {"id": 205, "seek": 133500, "start": 1347.0, "end": 1359.0, "text": " The vector now can be seen either as a real vector or column vector. And this is essentially just a generalization of the concept of a scalar because we just concatenate several scalars in one structure.", "tokens": [440, 8062, 586, 393, 312, 1612, 2139, 382, 257, 957, 8062, 420, 7738, 8062, 13, 400, 341, 307, 4476, 445, 257, 2674, 2144, 295, 264, 3410, 295, 257, 39684, 570, 321, 445, 1588, 7186, 473, 2940, 15664, 685, 294, 472, 3877, 13], "temperature": 0.0, "avg_logprob": -0.1263206173675229, "compression_ratio": 1.7777777777777777, "no_speech_prob": 1.6282898286590353e-05}, {"id": 206, "seek": 135900, "start": 1359.0, "end": 1374.0, "text": " Here's the structure of vectors. The same is happening for matrix. A matrix is a generalization of the concept of vectors because it concatenates or includes combines several vectors in one structure.", "tokens": [1692, 311, 264, 3877, 295, 18875, 13, 440, 912, 307, 2737, 337, 8141, 13, 316, 8141, 307, 257, 2674, 2144, 295, 264, 3410, 295, 18875, 570, 309, 1588, 7186, 1024, 420, 5974, 29520, 2940, 18875, 294, 472, 3877, 13], "temperature": 0.0, "avg_logprob": -0.09701307160513742, "compression_ratio": 1.6428571428571428, "no_speech_prob": 2.3454063921235502e-05}, {"id": 207, "seek": 135900, "start": 1374.0, "end": 1385.0, "text": " So now here we not only have one array but we have like in a matrix rows and columns which are combining several vectors.", "tokens": [407, 586, 510, 321, 406, 787, 362, 472, 10225, 457, 321, 362, 411, 294, 257, 8141, 13241, 293, 13766, 597, 366, 21928, 2940, 18875, 13], "temperature": 0.0, "avg_logprob": -0.09701307160513742, "compression_ratio": 1.6428571428571428, "no_speech_prob": 2.3454063921235502e-05}, {"id": 208, "seek": 138500, "start": 1385.0, "end": 1397.0, "text": " And this is then the same step that we are going from matrix to tensor. A tensor is a generalization of a matrix. So it's just combining concatenating several matrices in one structure.", "tokens": [400, 341, 307, 550, 264, 912, 1823, 300, 321, 366, 516, 490, 8141, 281, 40863, 13, 316, 40863, 307, 257, 2674, 2144, 295, 257, 8141, 13, 407, 309, 311, 445, 21928, 1588, 7186, 990, 2940, 32284, 294, 472, 3877, 13], "temperature": 0.0, "avg_logprob": -0.0628413302557809, "compression_ratio": 1.672811059907834, "no_speech_prob": 4.687979890150018e-05}, {"id": 209, "seek": 138500, "start": 1397.0, "end": 1407.0, "text": " So what we see here on the image you see that we are not only having rows and columns but we also start having channels. Channels are describing the depth of our structure here.", "tokens": [407, 437, 321, 536, 510, 322, 264, 3256, 291, 536, 300, 321, 366, 406, 787, 1419, 13241, 293, 13766, 457, 321, 611, 722, 1419, 9235, 13, 761, 969, 1625, 366, 16141, 264, 7161, 295, 527, 3877, 510, 13], "temperature": 0.0, "avg_logprob": -0.0628413302557809, "compression_ratio": 1.672811059907834, "no_speech_prob": 4.687979890150018e-05}, {"id": 210, "seek": 140700, "start": 1407.0, "end": 1419.0, "text": " So in this case we have three rows, three columns and three channels describing the depth. So let's have a look where we can use these and where we're going to use it in it to the end.", "tokens": [407, 294, 341, 1389, 321, 362, 1045, 13241, 11, 1045, 13766, 293, 1045, 9235, 16141, 264, 7161, 13, 407, 718, 311, 362, 257, 574, 689, 321, 393, 764, 613, 293, 689, 321, 434, 516, 281, 764, 309, 294, 309, 281, 264, 917, 13], "temperature": 0.0, "avg_logprob": -0.11431589471288474, "compression_ratio": 1.7033492822966507, "no_speech_prob": 1.2886911463283468e-05}, {"id": 211, "seek": 140700, "start": 1419.0, "end": 1429.0, "text": " So tensors are really common to use in computer vision and it's really easy to just say one application because each image that we want to process is essentially a tensor.", "tokens": [407, 10688, 830, 366, 534, 2689, 281, 764, 294, 3820, 5201, 293, 309, 311, 534, 1858, 281, 445, 584, 472, 3861, 570, 1184, 3256, 300, 321, 528, 281, 1399, 307, 4476, 257, 40863, 13], "temperature": 0.0, "avg_logprob": -0.11431589471288474, "compression_ratio": 1.7033492822966507, "no_speech_prob": 1.2886911463283468e-05}, {"id": 212, "seek": 142900, "start": 1429.0, "end": 1445.0, "text": " Images are often represented by RGB images. RGB stands for red, green and blue. So we are kind of including the information of the red colors, the green colors and the blue colors and three different challenges over the image over the pixels.", "tokens": [4331, 1660, 366, 2049, 10379, 538, 31231, 5267, 13, 31231, 7382, 337, 2182, 11, 3092, 293, 3344, 13, 407, 321, 366, 733, 295, 3009, 264, 1589, 295, 264, 2182, 4577, 11, 264, 3092, 4577, 293, 264, 3344, 4577, 293, 1045, 819, 4759, 670, 264, 3256, 670, 264, 18668, 13], "temperature": 0.0, "avg_logprob": -0.15953143807344658, "compression_ratio": 1.7256637168141593, "no_speech_prob": 6.347702583298087e-05}, {"id": 213, "seek": 142900, "start": 1445.0, "end": 1456.0, "text": " So one image can be described as a dimension of age times W times RGB which is standing for three in this case because we have three channels, RGB.", "tokens": [407, 472, 3256, 393, 312, 7619, 382, 257, 10139, 295, 3205, 1413, 343, 1413, 31231, 597, 307, 4877, 337, 1045, 294, 341, 1389, 570, 321, 362, 1045, 9235, 11, 31231, 13], "temperature": 0.0, "avg_logprob": -0.15953143807344658, "compression_ratio": 1.7256637168141593, "no_speech_prob": 6.347702583298087e-05}, {"id": 214, "seek": 145600, "start": 1456.0, "end": 1472.0, "text": " And on the image here or on the figure that is included on this slide you can just see it again. We have this funny picture of a cat which is then described as a tensor where we have like the size of the image is made and with.", "tokens": [400, 322, 264, 3256, 510, 420, 322, 264, 2573, 300, 307, 5556, 322, 341, 4137, 291, 393, 445, 536, 309, 797, 13, 492, 362, 341, 4074, 3036, 295, 257, 3857, 597, 307, 550, 7619, 382, 257, 40863, 689, 321, 362, 411, 264, 2744, 295, 264, 3256, 307, 1027, 293, 365, 13], "temperature": 0.0, "avg_logprob": -0.11512070959740943, "compression_ratio": 1.792626728110599, "no_speech_prob": 5.0289749196963385e-05}, {"id": 215, "seek": 145600, "start": 1472.0, "end": 1481.0, "text": " And then we have the channels red, green and blue. And in these channels we can then include all the information on the red colors, green colors and blue colors.", "tokens": [400, 550, 321, 362, 264, 9235, 2182, 11, 3092, 293, 3344, 13, 400, 294, 613, 9235, 321, 393, 550, 4090, 439, 264, 1589, 322, 264, 2182, 4577, 11, 3092, 4577, 293, 3344, 4577, 13], "temperature": 0.0, "avg_logprob": -0.11512070959740943, "compression_ratio": 1.792626728110599, "no_speech_prob": 5.0289749196963385e-05}, {"id": 216, "seek": 148100, "start": 1481.0, "end": 1492.0, "text": " So this is where tensors are an application and because images are represented there tensors they are really, really important for the whole setting of I2D.", "tokens": [407, 341, 307, 689, 10688, 830, 366, 364, 3861, 293, 570, 5267, 366, 10379, 456, 10688, 830, 436, 366, 534, 11, 534, 1021, 337, 264, 1379, 3287, 295, 286, 17, 35, 13], "temperature": 0.0, "avg_logprob": -0.18655895364695582, "compression_ratio": 1.7235023041474655, "no_speech_prob": 3.9983940951060504e-05}, {"id": 217, "seek": 148100, "start": 1492.0, "end": 1498.0, "text": " Because when we process an image in a neural network of course we have to work with tensors as well.", "tokens": [1436, 562, 321, 1399, 364, 3256, 294, 257, 18161, 3209, 295, 1164, 321, 362, 281, 589, 365, 10688, 830, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.18655895364695582, "compression_ratio": 1.7235023041474655, "no_speech_prob": 3.9983940951060504e-05}, {"id": 218, "seek": 148100, "start": 1498.0, "end": 1505.0, "text": " So just have in mind that each image is represented as a tensor and the tensors just a generalization of the matrix.", "tokens": [407, 445, 362, 294, 1575, 300, 1184, 3256, 307, 10379, 382, 257, 40863, 293, 264, 10688, 830, 445, 257, 2674, 2144, 295, 264, 8141, 13], "temperature": 0.0, "avg_logprob": -0.18655895364695582, "compression_ratio": 1.7235023041474655, "no_speech_prob": 3.9983940951060504e-05}, {"id": 219, "seek": 150500, "start": 1505.0, "end": 1511.0, "text": " Okay, so last but not least I want to talk about norms and laws functions.", "tokens": [1033, 11, 370, 1036, 457, 406, 1935, 286, 528, 281, 751, 466, 24357, 293, 6064, 6828, 13], "temperature": 0.0, "avg_logprob": -0.23114359113905164, "compression_ratio": 1.6340425531914893, "no_speech_prob": 7.060360803734511e-05}, {"id": 220, "seek": 150500, "start": 1511.0, "end": 1519.0, "text": " So a norm mathematically seen you can describe it as the measure of the length of a vector.", "tokens": [407, 257, 2026, 44003, 1612, 291, 393, 6786, 309, 382, 264, 3481, 295, 264, 4641, 295, 257, 8062, 13], "temperature": 0.0, "avg_logprob": -0.23114359113905164, "compression_ratio": 1.6340425531914893, "no_speech_prob": 7.060360803734511e-05}, {"id": 221, "seek": 150500, "start": 1519.0, "end": 1534.0, "text": " And I included the definition here so norm is a non negative function which is always denoted by these vertical lines which is time going from a vector space which we do in T by V into the space R of the real numbers.", "tokens": [400, 286, 5556, 264, 7123, 510, 370, 2026, 307, 257, 2107, 3671, 2445, 597, 307, 1009, 1441, 23325, 538, 613, 9429, 3876, 597, 307, 565, 516, 490, 257, 8062, 1901, 597, 321, 360, 294, 314, 538, 691, 666, 264, 1901, 497, 295, 264, 957, 3547, 13], "temperature": 0.0, "avg_logprob": -0.23114359113905164, "compression_ratio": 1.6340425531914893, "no_speech_prob": 7.060360803734511e-05}, {"id": 222, "seek": 153400, "start": 1534.0, "end": 1540.0, "text": " And it's non negative as I said. So we always have a positive outcome or a zero outcome.", "tokens": [400, 309, 311, 2107, 3671, 382, 286, 848, 13, 407, 321, 1009, 362, 257, 3353, 9700, 420, 257, 4018, 9700, 13], "temperature": 0.0, "avg_logprob": -0.11940326188739978, "compression_ratio": 1.64321608040201, "no_speech_prob": 2.7289283025311306e-05}, {"id": 223, "seek": 153400, "start": 1540.0, "end": 1547.0, "text": " And this function has to has three properties. If it satisfies these properties then we can call it an R.", "tokens": [400, 341, 2445, 575, 281, 575, 1045, 7221, 13, 759, 309, 44271, 613, 7221, 550, 321, 393, 818, 309, 364, 497, 13], "temperature": 0.0, "avg_logprob": -0.11940326188739978, "compression_ratio": 1.64321608040201, "no_speech_prob": 2.7289283025311306e-05}, {"id": 224, "seek": 153400, "start": 1547.0, "end": 1554.0, "text": " And these properties probably you all of you have already seen them is first of all a very important one is the triangle inequality.", "tokens": [400, 613, 7221, 1391, 291, 439, 295, 291, 362, 1217, 1612, 552, 307, 700, 295, 439, 257, 588, 1021, 472, 307, 264, 13369, 16970, 13], "temperature": 0.0, "avg_logprob": -0.11940326188739978, "compression_ratio": 1.64321608040201, "no_speech_prob": 2.7289283025311306e-05}, {"id": 225, "seek": 155400, "start": 1554.0, "end": 1564.0, "text": " So when we take two vectors V and W and we take the addition of these two so V plus W and we take the norm of this.", "tokens": [407, 562, 321, 747, 732, 18875, 691, 293, 343, 293, 321, 747, 264, 4500, 295, 613, 732, 370, 691, 1804, 343, 293, 321, 747, 264, 2026, 295, 341, 13], "temperature": 0.0, "avg_logprob": -0.08897099711678245, "compression_ratio": 1.748768472906404, "no_speech_prob": 1.7103895515901968e-05}, {"id": 226, "seek": 155400, "start": 1564.0, "end": 1570.0, "text": " This result has to be smaller or equal to the norm of V plus the norm of W.", "tokens": [639, 1874, 575, 281, 312, 4356, 420, 2681, 281, 264, 2026, 295, 691, 1804, 264, 2026, 295, 343, 13], "temperature": 0.0, "avg_logprob": -0.08897099711678245, "compression_ratio": 1.748768472906404, "no_speech_prob": 1.7103895515901968e-05}, {"id": 227, "seek": 155400, "start": 1570.0, "end": 1573.0, "text": " So this is called the triangle inequality.", "tokens": [407, 341, 307, 1219, 264, 13369, 16970, 13], "temperature": 0.0, "avg_logprob": -0.08897099711678245, "compression_ratio": 1.748768472906404, "no_speech_prob": 1.7103895515901968e-05}, {"id": 228, "seek": 155400, "start": 1573.0, "end": 1579.0, "text": " The second property has now something to do with the scalar multiplication that we have been looking at for the vectors.", "tokens": [440, 1150, 4707, 575, 586, 746, 281, 360, 365, 264, 39684, 27290, 300, 321, 362, 668, 1237, 412, 337, 264, 18875, 13], "temperature": 0.0, "avg_logprob": -0.08897099711678245, "compression_ratio": 1.748768472906404, "no_speech_prob": 1.7103895515901968e-05}, {"id": 229, "seek": 157900, "start": 1579.0, "end": 1588.0, "text": " In this case a is a scalar so an element of R in our case and V is a vector so an element of R to the power of N for example.", "tokens": [682, 341, 1389, 257, 307, 257, 39684, 370, 364, 4478, 295, 497, 294, 527, 1389, 293, 691, 307, 257, 8062, 370, 364, 4478, 295, 497, 281, 264, 1347, 295, 426, 337, 1365, 13], "temperature": 0.0, "avg_logprob": -0.11807823181152344, "compression_ratio": 1.75, "no_speech_prob": 9.982626943383366e-05}, {"id": 230, "seek": 157900, "start": 1588.0, "end": 1600.0, "text": " When we now take the scalar multiplication a times V and then the norm of this element it should be the same then a times the norm of V for every scalar a and R.", "tokens": [1133, 321, 586, 747, 264, 39684, 27290, 257, 1413, 691, 293, 550, 264, 2026, 295, 341, 4478, 309, 820, 312, 264, 912, 550, 257, 1413, 264, 2026, 295, 691, 337, 633, 39684, 257, 293, 497, 13], "temperature": 0.0, "avg_logprob": -0.11807823181152344, "compression_ratio": 1.75, "no_speech_prob": 9.982626943383366e-05}, {"id": 231, "seek": 160000, "start": 1600.0, "end": 1610.0, "text": " And the third property is now that the norm of a vector equals to zero if and only if this vector is equal to zero so it is the zero vector.", "tokens": [400, 264, 2636, 4707, 307, 586, 300, 264, 2026, 295, 257, 8062, 6915, 281, 4018, 498, 293, 787, 498, 341, 8062, 307, 2681, 281, 4018, 370, 309, 307, 264, 4018, 8062, 13], "temperature": 0.0, "avg_logprob": -0.09235059298001803, "compression_ratio": 1.6907894736842106, "no_speech_prob": 2.047173802566249e-05}, {"id": 232, "seek": 160000, "start": 1610.0, "end": 1618.0, "text": " So this is also property for a norm so we are not allowed to assign the value of zero to a vector which is non zero.", "tokens": [407, 341, 307, 611, 4707, 337, 257, 2026, 370, 321, 366, 406, 4350, 281, 6269, 264, 2158, 295, 4018, 281, 257, 8062, 597, 307, 2107, 4018, 13], "temperature": 0.0, "avg_logprob": -0.09235059298001803, "compression_ratio": 1.6907894736842106, "no_speech_prob": 2.047173802566249e-05}, {"id": 233, "seek": 161800, "start": 1618.0, "end": 1636.0, "text": " Yeah and I just included some of the definition information we here is a vector space over field F in our case we just define V as R to the power of N and then it's a vector field about the real order over the field of the real numbers.", "tokens": [865, 293, 286, 445, 5556, 512, 295, 264, 7123, 1589, 321, 510, 307, 257, 8062, 1901, 670, 2519, 479, 294, 527, 1389, 321, 445, 6964, 691, 382, 497, 281, 264, 1347, 295, 426, 293, 550, 309, 311, 257, 8062, 2519, 466, 264, 957, 1668, 670, 264, 2519, 295, 264, 957, 3547, 13], "temperature": 0.0, "avg_logprob": -0.18121659755706787, "compression_ratio": 1.5225806451612902, "no_speech_prob": 2.104040868289303e-05}, {"id": 234, "seek": 163600, "start": 1636.0, "end": 1647.0, "text": " And as a remark which is important to say on one space like in our case R to the power of N you can define several measurements several norms.", "tokens": [400, 382, 257, 7942, 597, 307, 1021, 281, 584, 322, 472, 1901, 411, 294, 527, 1389, 497, 281, 264, 1347, 295, 426, 291, 393, 6964, 2940, 15383, 2940, 24357, 13], "temperature": 0.0, "avg_logprob": -0.12369529149865591, "compression_ratio": 1.6434782608695653, "no_speech_prob": 1.4164440472086426e-05}, {"id": 235, "seek": 163600, "start": 1647.0, "end": 1652.0, "text": " So in our case what we want to have a look at on the next two slides is the L1 norm and the L2 norm.", "tokens": [407, 294, 527, 1389, 437, 321, 528, 281, 362, 257, 574, 412, 322, 264, 958, 732, 9788, 307, 264, 441, 16, 2026, 293, 264, 441, 17, 2026, 13], "temperature": 0.0, "avg_logprob": -0.12369529149865591, "compression_ratio": 1.6434782608695653, "no_speech_prob": 1.4164440472086426e-05}, {"id": 236, "seek": 163600, "start": 1652.0, "end": 1662.0, "text": " These are two norms both defined on Rn but assigning different numbers to each vector but both of them are norm just a different norm.", "tokens": [1981, 366, 732, 24357, 1293, 7642, 322, 497, 77, 457, 49602, 819, 3547, 281, 1184, 8062, 457, 1293, 295, 552, 366, 2026, 445, 257, 819, 2026, 13], "temperature": 0.0, "avg_logprob": -0.12369529149865591, "compression_ratio": 1.6434782608695653, "no_speech_prob": 1.4164440472086426e-05}, {"id": 237, "seek": 166200, "start": 1662.0, "end": 1675.0, "text": " And as I said in the definition of a norm like every function every non negative function that we can define with these three properties define norms of course there are several possibilities.", "tokens": [400, 382, 286, 848, 294, 264, 7123, 295, 257, 2026, 411, 633, 2445, 633, 2107, 3671, 2445, 300, 321, 393, 6964, 365, 613, 1045, 7221, 6964, 24357, 295, 1164, 456, 366, 2940, 12178, 13], "temperature": 0.0, "avg_logprob": -0.10458057805111534, "compression_ratio": 1.6861702127659575, "no_speech_prob": 4.9347530875820667e-05}, {"id": 238, "seek": 166200, "start": 1675.0, "end": 1678.0, "text": " So let us have a look at the first one of the L1 norm.", "tokens": [407, 718, 505, 362, 257, 574, 412, 264, 700, 472, 295, 264, 441, 16, 2026, 13], "temperature": 0.0, "avg_logprob": -0.10458057805111534, "compression_ratio": 1.6861702127659575, "no_speech_prob": 4.9347530875820667e-05}, {"id": 239, "seek": 166200, "start": 1678.0, "end": 1683.0, "text": " Okay the L1 norm is the first example that we want to have a look at.", "tokens": [1033, 264, 441, 16, 2026, 307, 264, 700, 1365, 300, 321, 528, 281, 362, 257, 574, 412, 13], "temperature": 0.0, "avg_logprob": -0.10458057805111534, "compression_ratio": 1.6861702127659575, "no_speech_prob": 4.9347530875820667e-05}, {"id": 240, "seek": 168300, "start": 1683.0, "end": 1701.0, "text": " This one is a norm that one norm is a norm on the space Rn and it's defined by a function which we denote here by this sign which is used for a norm like these vertical lines and then we use the subscript 1 like an index 1 and this function is then going from Rn to R.", "tokens": [639, 472, 307, 257, 2026, 300, 472, 2026, 307, 257, 2026, 322, 264, 1901, 497, 77, 293, 309, 311, 7642, 538, 257, 2445, 597, 321, 45708, 510, 538, 341, 1465, 597, 307, 1143, 337, 257, 2026, 411, 613, 9429, 3876, 293, 550, 321, 764, 264, 2325, 662, 502, 411, 364, 8186, 502, 293, 341, 2445, 307, 550, 516, 490, 497, 77, 281, 497, 13], "temperature": 0.0, "avg_logprob": -0.15668552062090704, "compression_ratio": 1.6341463414634145, "no_speech_prob": 2.97492206300376e-05}, {"id": 241, "seek": 170100, "start": 1701.0, "end": 1722.0, "text": " And it's a non negative function and it is defined as follows for each vector V which is denoted here is the two part of elements we want to Vn and we assign the number which is given here and we just take the absolute value of all the components of the vector and then we sum over these values.", "tokens": [400, 309, 311, 257, 2107, 3671, 2445, 293, 309, 307, 7642, 382, 10002, 337, 1184, 8062, 691, 597, 307, 1441, 23325, 510, 307, 264, 732, 644, 295, 4959, 321, 528, 281, 691, 77, 293, 321, 6269, 264, 1230, 597, 307, 2212, 510, 293, 321, 445, 747, 264, 8236, 2158, 295, 439, 264, 6677, 295, 264, 8062, 293, 550, 321, 2408, 670, 613, 4190, 13], "temperature": 0.0, "avg_logprob": -0.14594444106606877, "compression_ratio": 1.6573033707865168, "no_speech_prob": 1.8673463273444213e-05}, {"id": 242, "seek": 172200, "start": 1722.0, "end": 1733.0, "text": " And this gives us the L1 norm of a vector V. So this is the definition of the L1 norm and let's have a look at one simple example just to make sure that we understand the concept.", "tokens": [400, 341, 2709, 505, 264, 441, 16, 2026, 295, 257, 8062, 691, 13, 407, 341, 307, 264, 7123, 295, 264, 441, 16, 2026, 293, 718, 311, 362, 257, 574, 412, 472, 2199, 1365, 445, 281, 652, 988, 300, 321, 1223, 264, 3410, 13], "temperature": 0.0, "avg_logprob": -0.07317262472108353, "compression_ratio": 1.6328502415458936, "no_speech_prob": 2.6564222935121506e-05}, {"id": 243, "seek": 172200, "start": 1733.0, "end": 1746.0, "text": " So we take now an arbitrary or not arbitrary in this case we take a vector V which is defined as an element from R3 and it has the components 1 minus 3 and 2.", "tokens": [407, 321, 747, 586, 364, 23211, 420, 406, 23211, 294, 341, 1389, 321, 747, 257, 8062, 691, 597, 307, 7642, 382, 364, 4478, 490, 497, 18, 293, 309, 575, 264, 6677, 502, 3175, 805, 293, 568, 13], "temperature": 0.0, "avg_logprob": -0.07317262472108353, "compression_ratio": 1.6328502415458936, "no_speech_prob": 2.6564222935121506e-05}, {"id": 244, "seek": 174600, "start": 1746.0, "end": 1758.0, "text": " And remember we want to calculate the L1 norm which just need to use the definition that we gave above. So it's just the summation over 3 elements in this case 1, 3 and 2.", "tokens": [400, 1604, 321, 528, 281, 8873, 264, 441, 16, 2026, 597, 445, 643, 281, 764, 264, 7123, 300, 321, 2729, 3673, 13, 407, 309, 311, 445, 264, 28811, 670, 805, 4959, 294, 341, 1389, 502, 11, 805, 293, 568, 13], "temperature": 0.0, "avg_logprob": -0.13134513362761466, "compression_ratio": 1.7297297297297298, "no_speech_prob": 8.50995274959132e-06}, {"id": 245, "seek": 174600, "start": 1758.0, "end": 1772.0, "text": " So we just take the absolute values of all the components so in this case minus 3 gets 3 because we take the absolute value and 1 and 2 stay the same and then we just sum over them which gives us the result of 6.", "tokens": [407, 321, 445, 747, 264, 8236, 4190, 295, 439, 264, 6677, 370, 294, 341, 1389, 3175, 805, 2170, 805, 570, 321, 747, 264, 8236, 2158, 293, 502, 293, 568, 1754, 264, 912, 293, 550, 321, 445, 2408, 670, 552, 597, 2709, 505, 264, 1874, 295, 1386, 13], "temperature": 0.0, "avg_logprob": -0.13134513362761466, "compression_ratio": 1.7297297297297298, "no_speech_prob": 8.50995274959132e-06}, {"id": 246, "seek": 177200, "start": 1772.0, "end": 1783.0, "text": " So the L1 norm of this vector is equal to 6. So let us have a look at another norm, the second possibility that we can also define on this space of Rn.", "tokens": [407, 264, 441, 16, 2026, 295, 341, 8062, 307, 2681, 281, 1386, 13, 407, 718, 505, 362, 257, 574, 412, 1071, 2026, 11, 264, 1150, 7959, 300, 321, 393, 611, 6964, 322, 341, 1901, 295, 497, 77, 13], "temperature": 0.0, "avg_logprob": -0.09639197740799342, "compression_ratio": 1.518716577540107, "no_speech_prob": 8.106785389827564e-05}, {"id": 247, "seek": 177200, "start": 1783.0, "end": 1792.0, "text": " So in this case we denoted with the subscript of 2 index of 2 and it's again a function from Rn to R like we gave in the definition.", "tokens": [407, 294, 341, 1389, 321, 1441, 23325, 365, 264, 2325, 662, 295, 568, 8186, 295, 568, 293, 309, 311, 797, 257, 2445, 490, 497, 77, 281, 497, 411, 321, 2729, 294, 264, 7123, 13], "temperature": 0.0, "avg_logprob": -0.09639197740799342, "compression_ratio": 1.518716577540107, "no_speech_prob": 8.106785389827564e-05}, {"id": 248, "seek": 179200, "start": 1792.0, "end": 1814.0, "text": " And now the definition of this norm of this L2 norm is that it's behaving for a vector V again denoted by this tuple of elements of V1 to Vn and by taking every component to the power of 2 then take the summation over all of them and finally take the square root of this number.", "tokens": [400, 586, 264, 7123, 295, 341, 2026, 295, 341, 441, 17, 2026, 307, 300, 309, 311, 35263, 337, 257, 8062, 691, 797, 1441, 23325, 538, 341, 2604, 781, 295, 4959, 295, 691, 16, 281, 691, 77, 293, 538, 1940, 633, 6542, 281, 264, 1347, 295, 568, 550, 747, 264, 28811, 670, 439, 295, 552, 293, 2721, 747, 264, 3732, 5593, 295, 341, 1230, 13], "temperature": 0.0, "avg_logprob": -0.08664795931647806, "compression_ratio": 1.606936416184971, "no_speech_prob": 1.311376672674669e-05}, {"id": 249, "seek": 181400, "start": 1814.0, "end": 1823.0, "text": " So this is giving us the L2 norm of this vector V and again we're going to have a look at the example. We take the same example then this like before.", "tokens": [407, 341, 307, 2902, 505, 264, 441, 17, 2026, 295, 341, 8062, 691, 293, 797, 321, 434, 516, 281, 362, 257, 574, 412, 264, 1365, 13, 492, 747, 264, 912, 1365, 550, 341, 411, 949, 13], "temperature": 0.0, "avg_logprob": -0.07968390113429019, "compression_ratio": 1.735159817351598, "no_speech_prob": 1.5159515896812081e-05}, {"id": 250, "seek": 181400, "start": 1823.0, "end": 1839.0, "text": " So again we have this vector which is defined by a vector 1 minus 3 and 2 as an element of R3 and then we calculate the L2 norm which is now given by the summation of 3 elements where we take all the components to the power of 2.", "tokens": [407, 797, 321, 362, 341, 8062, 597, 307, 7642, 538, 257, 8062, 502, 3175, 805, 293, 568, 382, 364, 4478, 295, 497, 18, 293, 550, 321, 8873, 264, 441, 17, 2026, 597, 307, 586, 2212, 538, 264, 28811, 295, 805, 4959, 689, 321, 747, 439, 264, 6677, 281, 264, 1347, 295, 568, 13], "temperature": 0.0, "avg_logprob": -0.07968390113429019, "compression_ratio": 1.735159817351598, "no_speech_prob": 1.5159515896812081e-05}, {"id": 251, "seek": 183900, "start": 1839.0, "end": 1846.0, "text": " So 1 to the power of 2 minus 3 to the power of 2 and 2 to the power of 2 and finally we take the square root.", "tokens": [407, 502, 281, 264, 1347, 295, 568, 3175, 805, 281, 264, 1347, 295, 568, 293, 568, 281, 264, 1347, 295, 568, 293, 2721, 321, 747, 264, 3732, 5593, 13], "temperature": 0.0, "avg_logprob": -0.08944403095009887, "compression_ratio": 1.7527472527472527, "no_speech_prob": 1.6874850189196877e-05}, {"id": 252, "seek": 183900, "start": 1846.0, "end": 1854.0, "text": " So this delivers us finally the number square root 14 which is quite different to the number that we found before to the L1.", "tokens": [407, 341, 24860, 505, 2721, 264, 1230, 3732, 5593, 3499, 597, 307, 1596, 819, 281, 264, 1230, 300, 321, 1352, 949, 281, 264, 441, 16, 13], "temperature": 0.0, "avg_logprob": -0.08944403095009887, "compression_ratio": 1.7527472527472527, "no_speech_prob": 1.6874850189196877e-05}, {"id": 253, "seek": 183900, "start": 1854.0, "end": 1860.0, "text": " So you see that both norms are behaving differently for the vectors of our space Rn.", "tokens": [407, 291, 536, 300, 1293, 24357, 366, 35263, 7614, 337, 264, 18875, 295, 527, 1901, 497, 77, 13], "temperature": 0.0, "avg_logprob": -0.08944403095009887, "compression_ratio": 1.7527472527472527, "no_speech_prob": 1.6874850189196877e-05}, {"id": 254, "seek": 186000, "start": 1860.0, "end": 1869.0, "text": " Last but not least I want to talk about loss functions because they will play a very important role in the I to the lecture.", "tokens": [5264, 457, 406, 1935, 286, 528, 281, 751, 466, 4470, 6828, 570, 436, 486, 862, 257, 588, 1021, 3090, 294, 264, 286, 281, 264, 7991, 13], "temperature": 0.0, "avg_logprob": -0.08370166666367475, "compression_ratio": 1.752, "no_speech_prob": 1.3684206351172179e-05}, {"id": 255, "seek": 186000, "start": 1869.0, "end": 1878.0, "text": " So a loss function essentially is a function that takes us input 2 vectors and the output is then a measurement of the distance between these 2 vectors.", "tokens": [407, 257, 4470, 2445, 4476, 307, 257, 2445, 300, 2516, 505, 4846, 568, 18875, 293, 264, 5598, 307, 550, 257, 13160, 295, 264, 4560, 1296, 613, 568, 18875, 13], "temperature": 0.0, "avg_logprob": -0.08370166666367475, "compression_ratio": 1.752, "no_speech_prob": 1.3684206351172179e-05}, {"id": 256, "seek": 186000, "start": 1878.0, "end": 1887.0, "text": " So we can have a look here at the example for the L1 loss and the L2 loss because these are 2 losses that are referring to the L1 norm and respectively L2 norm.", "tokens": [407, 321, 393, 362, 257, 574, 510, 412, 264, 1365, 337, 264, 441, 16, 4470, 293, 264, 441, 17, 4470, 570, 613, 366, 568, 15352, 300, 366, 13761, 281, 264, 441, 16, 2026, 293, 25009, 441, 17, 2026, 13], "temperature": 0.0, "avg_logprob": -0.08370166666367475, "compression_ratio": 1.752, "no_speech_prob": 1.3684206351172179e-05}, {"id": 257, "seek": 188700, "start": 1887.0, "end": 1904.0, "text": " So the L1 loss is a loss between 2 vectors so we denote the vectors V and W as elements of Rn and then we can define the L1 loss of these 2 vectors V and W as the L1 norm of the vector V minus W.", "tokens": [407, 264, 441, 16, 4470, 307, 257, 4470, 1296, 568, 18875, 370, 321, 45708, 264, 18875, 691, 293, 343, 382, 4959, 295, 497, 77, 293, 550, 321, 393, 6964, 264, 441, 16, 4470, 295, 613, 568, 18875, 691, 293, 343, 382, 264, 441, 16, 2026, 295, 264, 8062, 691, 3175, 343, 13], "temperature": 0.0, "avg_logprob": -0.06665323036057609, "compression_ratio": 1.625, "no_speech_prob": 8.878994412953034e-05}, {"id": 258, "seek": 190400, "start": 1904.0, "end": 1925.0, "text": " And as you remember we saw before V minus W is just describing a vector which is connecting to the 2 vectors V and W. So it's kind of describing the difference and then we take the L1 norm which is giving us the measurement of the distance between these 2 vectors but referring here to the L1 norm.", "tokens": [400, 382, 291, 1604, 321, 1866, 949, 691, 3175, 343, 307, 445, 16141, 257, 8062, 597, 307, 11015, 281, 264, 568, 18875, 691, 293, 343, 13, 407, 309, 311, 733, 295, 16141, 264, 2649, 293, 550, 321, 747, 264, 441, 16, 2026, 597, 307, 2902, 505, 264, 13160, 295, 264, 4560, 1296, 613, 568, 18875, 457, 13761, 510, 281, 264, 441, 16, 2026, 13], "temperature": 0.0, "avg_logprob": -0.06225156082826502, "compression_ratio": 1.6108108108108108, "no_speech_prob": 1.7341137208859436e-05}, {"id": 259, "seek": 192500, "start": 1925.0, "end": 1943.0, "text": " The same idea we can also use it for the L2 norm for the Euclidean norm. So here again we have these 2 vectors V and W and then we define the L2 loss as L2 of V and W where we now take the L2 norm of the vector V minus W.", "tokens": [440, 912, 1558, 321, 393, 611, 764, 309, 337, 264, 441, 17, 2026, 337, 264, 462, 1311, 31264, 282, 2026, 13, 407, 510, 797, 321, 362, 613, 568, 18875, 691, 293, 343, 293, 550, 321, 6964, 264, 441, 17, 4470, 382, 441, 17, 295, 691, 293, 343, 689, 321, 586, 747, 264, 441, 17, 2026, 295, 264, 8062, 691, 3175, 343, 13], "temperature": 0.0, "avg_logprob": -0.05619747710950447, "compression_ratio": 1.556338028169014, "no_speech_prob": 6.231766019482166e-05}, {"id": 260, "seek": 194300, "start": 1943.0, "end": 1955.0, "text": " So again the same concept but now we take another measurement near our L2 norm instead of our L1 norm. And these are 2 very famous loss functions that you will always see coming around because they are quite popular.", "tokens": [407, 797, 264, 912, 3410, 457, 586, 321, 747, 1071, 13160, 2651, 527, 441, 17, 2026, 2602, 295, 527, 441, 16, 2026, 13, 400, 613, 366, 568, 588, 4618, 4470, 6828, 300, 291, 486, 1009, 536, 1348, 926, 570, 436, 366, 1596, 3743, 13], "temperature": 0.0, "avg_logprob": -0.09339766169703284, "compression_ratio": 1.6808510638297873, "no_speech_prob": 7.0274777499435e-06}, {"id": 261, "seek": 194300, "start": 1955.0, "end": 1965.0, "text": " You will also see other loss functions but I hope that you better understand the relationship between the norm and the last function which is really easy for this L1 and L2 loss.", "tokens": [509, 486, 611, 536, 661, 4470, 6828, 457, 286, 1454, 300, 291, 1101, 1223, 264, 2480, 1296, 264, 2026, 293, 264, 1036, 2445, 597, 307, 534, 1858, 337, 341, 441, 16, 293, 441, 17, 4470, 13], "temperature": 0.0, "avg_logprob": -0.09339766169703284, "compression_ratio": 1.6808510638297873, "no_speech_prob": 7.0274777499435e-06}, {"id": 262, "seek": 196500, "start": 1965.0, "end": 1978.0, "text": " Before we now start with the second part of our tutorial session I first of all want to give a short outlook of where we need calculus and also where we need our linear algebra that we saw so far on the previous slides.", "tokens": [4546, 321, 586, 722, 365, 264, 1150, 644, 295, 527, 7073, 5481, 286, 700, 295, 439, 528, 281, 976, 257, 2099, 26650, 295, 689, 321, 643, 33400, 293, 611, 689, 321, 643, 527, 8213, 21989, 300, 321, 1866, 370, 1400, 322, 264, 3894, 9788, 13], "temperature": 0.0, "avg_logprob": -0.10347560832374975, "compression_ratio": 1.645320197044335, "no_speech_prob": 9.07721696421504e-05}, {"id": 263, "seek": 196500, "start": 1978.0, "end": 1985.0, "text": " So you know that we want to talk about neural networks. I2DL is about deep learning and neural network structures.", "tokens": [407, 291, 458, 300, 321, 528, 281, 751, 466, 18161, 9590, 13, 286, 17, 35, 43, 307, 466, 2452, 2539, 293, 18161, 3209, 9227, 13], "temperature": 0.0, "avg_logprob": -0.10347560832374975, "compression_ratio": 1.645320197044335, "no_speech_prob": 9.07721696421504e-05}, {"id": 264, "seek": 198500, "start": 1985.0, "end": 2007.0, "text": " And one of the famous tasks that we are looking at in deep learning has been image classification. So image classification the idea of the task is that we have an image and we want to construct a neural network program which is able to predict or to say which object is present in this frame.", "tokens": [400, 472, 295, 264, 4618, 9608, 300, 321, 366, 1237, 412, 294, 2452, 2539, 575, 668, 3256, 21538, 13, 407, 3256, 21538, 264, 1558, 295, 264, 5633, 307, 300, 321, 362, 364, 3256, 293, 321, 528, 281, 7690, 257, 18161, 3209, 1461, 597, 307, 1075, 281, 6069, 420, 281, 584, 597, 2657, 307, 1974, 294, 341, 3920, 13], "temperature": 0.0, "avg_logprob": -0.06428325176239014, "compression_ratio": 1.6404494382022472, "no_speech_prob": 4.4372798583935946e-05}, {"id": 265, "seek": 200700, "start": 2007.0, "end": 2015.0, "text": " So in this example we see a cat. For example here we have different classes. Let's say we have 3, we have a dog, we have cat and we have deer.", "tokens": [407, 294, 341, 1365, 321, 536, 257, 3857, 13, 1171, 1365, 510, 321, 362, 819, 5359, 13, 961, 311, 584, 321, 362, 805, 11, 321, 362, 257, 3000, 11, 321, 362, 3857, 293, 321, 362, 17120, 13], "temperature": 0.0, "avg_logprob": -0.11647174471900576, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.00012535224959719926}, {"id": 266, "seek": 200700, "start": 2015.0, "end": 2024.0, "text": " And we want this neural network that we see here in our circle to predict whether we see a deer, a dog or a cat in this image. Of course the answer is cat, we know that.", "tokens": [400, 321, 528, 341, 18161, 3209, 300, 321, 536, 510, 294, 527, 6329, 281, 6069, 1968, 321, 536, 257, 17120, 11, 257, 3000, 420, 257, 3857, 294, 341, 3256, 13, 2720, 1164, 264, 1867, 307, 3857, 11, 321, 458, 300, 13], "temperature": 0.0, "avg_logprob": -0.11647174471900576, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.00012535224959719926}, {"id": 267, "seek": 202400, "start": 2024.0, "end": 2045.0, "text": " So first of all because before we can just try to start to process our image we have to transform it in a way that that is ready to process. And this is why I introduced the definition of a tensor because I said already images can be transformed or are essentially tensors.", "tokens": [407, 700, 295, 439, 570, 949, 321, 393, 445, 853, 281, 722, 281, 1399, 527, 3256, 321, 362, 281, 4088, 309, 294, 257, 636, 300, 300, 307, 1919, 281, 1399, 13, 400, 341, 307, 983, 286, 7268, 264, 7123, 295, 257, 40863, 570, 286, 848, 1217, 5267, 393, 312, 16894, 420, 366, 4476, 10688, 830, 13], "temperature": 0.0, "avg_logprob": -0.1274388074874878, "compression_ratio": 1.5964912280701755, "no_speech_prob": 3.313946581329219e-05}, {"id": 268, "seek": 204500, "start": 2045.0, "end": 2057.0, "text": " So like the information of an image is stored in such a tensor. So an image is our tensor and this is the one or this is the structure that we are feeding into our new network.", "tokens": [407, 411, 264, 1589, 295, 364, 3256, 307, 12187, 294, 1270, 257, 40863, 13, 407, 364, 3256, 307, 527, 40863, 293, 341, 307, 264, 472, 420, 341, 307, 264, 3877, 300, 321, 366, 12919, 666, 527, 777, 3209, 13], "temperature": 0.0, "avg_logprob": -0.0971389658310834, "compression_ratio": 1.9263157894736842, "no_speech_prob": 4.899241685052402e-05}, {"id": 269, "seek": 204500, "start": 2057.0, "end": 2071.0, "text": " And this is shown like this. So our new network is taking a tensor which is representing our image and the outcome of our new network is now a vector or a prediction what we are looking at.", "tokens": [400, 341, 307, 4898, 411, 341, 13, 407, 527, 777, 3209, 307, 1940, 257, 40863, 597, 307, 13460, 527, 3256, 293, 264, 9700, 295, 527, 777, 3209, 307, 586, 257, 8062, 420, 257, 17630, 437, 321, 366, 1237, 412, 13], "temperature": 0.0, "avg_logprob": -0.0971389658310834, "compression_ratio": 1.9263157894736842, "no_speech_prob": 4.899241685052402e-05}, {"id": 270, "seek": 207100, "start": 2071.0, "end": 2081.0, "text": " Like a prediction of our new network which says to us we see this object. So as I said we are looking at 3 classes here.", "tokens": [1743, 257, 17630, 295, 527, 777, 3209, 597, 1619, 281, 505, 321, 536, 341, 2657, 13, 407, 382, 286, 848, 321, 366, 1237, 412, 805, 5359, 510, 13], "temperature": 0.0, "avg_logprob": -0.1478561092825497, "compression_ratio": 1.4325842696629214, "no_speech_prob": 5.210854214965366e-05}, {"id": 271, "seek": 207100, "start": 2081.0, "end": 2090.0, "text": " Cat, dog and deer. And you see here it gives us kind of a score like how lightly it is that a cat dog or deer is visible on our image.", "tokens": [9565, 11, 3000, 293, 17120, 13, 400, 291, 536, 510, 309, 2709, 505, 733, 295, 257, 6175, 411, 577, 16695, 309, 307, 300, 257, 3857, 3000, 420, 17120, 307, 8974, 322, 527, 3256, 13], "temperature": 0.0, "avg_logprob": -0.1478561092825497, "compression_ratio": 1.4325842696629214, "no_speech_prob": 5.210854214965366e-05}, {"id": 272, "seek": 209000, "start": 2090.0, "end": 2102.0, "text": " And the prediction that this new network here is now giving to us is really bad because it shows us a 70% probability that on this image we see a deer which is not true.", "tokens": [400, 264, 17630, 300, 341, 777, 3209, 510, 307, 586, 2902, 281, 505, 307, 534, 1578, 570, 309, 3110, 505, 257, 5285, 4, 8482, 300, 322, 341, 3256, 321, 536, 257, 17120, 597, 307, 406, 2074, 13], "temperature": 0.0, "avg_logprob": -0.07714928713711826, "compression_ratio": 1.8949579831932772, "no_speech_prob": 7.916086178738624e-05}, {"id": 273, "seek": 209000, "start": 2102.0, "end": 2118.0, "text": " So in this case what we now have to do is we have to train our new network. We have to show him a lot of images and for each images it gives us a prediction and we have to say him you're good or you're not good and if you're not good we have to change something in our new network.", "tokens": [407, 294, 341, 1389, 437, 321, 586, 362, 281, 360, 307, 321, 362, 281, 3847, 527, 777, 3209, 13, 492, 362, 281, 855, 796, 257, 688, 295, 5267, 293, 337, 1184, 5267, 309, 2709, 505, 257, 17630, 293, 321, 362, 281, 584, 796, 291, 434, 665, 420, 291, 434, 406, 665, 293, 498, 291, 434, 406, 665, 321, 362, 281, 1319, 746, 294, 527, 777, 3209, 13], "temperature": 0.0, "avg_logprob": -0.07714928713711826, "compression_ratio": 1.8949579831932772, "no_speech_prob": 7.916086178738624e-05}, {"id": 274, "seek": 211800, "start": 2118.0, "end": 2128.0, "text": " So this is what we do using loss functions. Loss functions are like we said describing the measurements between two vectors here.", "tokens": [407, 341, 307, 437, 321, 360, 1228, 4470, 6828, 13, 441, 772, 6828, 366, 411, 321, 848, 16141, 264, 15383, 1296, 732, 18875, 510, 13], "temperature": 0.0, "avg_logprob": -0.12955505664532002, "compression_ratio": 1.6628571428571428, "no_speech_prob": 5.710980258299969e-05}, {"id": 275, "seek": 211800, "start": 2128.0, "end": 2137.0, "text": " The one vector is the vector is the outcome of our new network so the prediction and the other one is the truth that we know we just call it ground truth labels.", "tokens": [440, 472, 8062, 307, 264, 8062, 307, 264, 9700, 295, 527, 777, 3209, 370, 264, 17630, 293, 264, 661, 472, 307, 264, 3494, 300, 321, 458, 321, 445, 818, 309, 2727, 3494, 16949, 13], "temperature": 0.0, "avg_logprob": -0.12955505664532002, "compression_ratio": 1.6628571428571428, "no_speech_prob": 5.710980258299969e-05}, {"id": 276, "seek": 213700, "start": 2137.0, "end": 2151.0, "text": " So for this image we know that it should be 100% cat and 0% dog and 0% deer that would be perfect. So then we just calculate the loss between the actual prediction that our new network did and the ground truth.", "tokens": [407, 337, 341, 3256, 321, 458, 300, 309, 820, 312, 2319, 4, 3857, 293, 1958, 4, 3000, 293, 1958, 4, 17120, 300, 576, 312, 2176, 13, 407, 550, 321, 445, 8873, 264, 4470, 1296, 264, 3539, 17630, 300, 527, 777, 3209, 630, 293, 264, 2727, 3494, 13], "temperature": 0.0, "avg_logprob": -0.08400313059488933, "compression_ratio": 1.6940639269406392, "no_speech_prob": 3.2314754207618535e-05}, {"id": 277, "seek": 213700, "start": 2151.0, "end": 2163.0, "text": " And then we can have a look like how wrong has our new network been and the total goal of learning of the learning process is that we want to minimize the loss.", "tokens": [400, 550, 321, 393, 362, 257, 574, 411, 577, 2085, 575, 527, 777, 3209, 668, 293, 264, 3217, 3387, 295, 2539, 295, 264, 2539, 1399, 307, 300, 321, 528, 281, 17522, 264, 4470, 13], "temperature": 0.0, "avg_logprob": -0.08400313059488933, "compression_ratio": 1.6940639269406392, "no_speech_prob": 3.2314754207618535e-05}, {"id": 278, "seek": 216300, "start": 2163.0, "end": 2175.0, "text": " We want to work or we want to train our new network so that it's working pretty good and gives us pretty good predictions.", "tokens": [492, 528, 281, 589, 420, 321, 528, 281, 3847, 527, 777, 3209, 370, 300, 309, 311, 1364, 1238, 665, 293, 2709, 505, 1238, 665, 21264, 13], "temperature": 0.0, "avg_logprob": -0.07380417211731868, "compression_ratio": 1.7102272727272727, "no_speech_prob": 7.410700345644727e-05}, {"id": 279, "seek": 216300, "start": 2175.0, "end": 2186.0, "text": " And the new network essentially what it is doing it's like it's a big structure but it's containing of different matrices and these matrices and tensors are processing our image.", "tokens": [400, 264, 777, 3209, 4476, 437, 309, 307, 884, 309, 311, 411, 309, 311, 257, 955, 3877, 457, 309, 311, 19273, 295, 819, 32284, 293, 613, 32284, 293, 10688, 830, 366, 9007, 527, 3256, 13], "temperature": 0.0, "avg_logprob": -0.07380417211731868, "compression_ratio": 1.7102272727272727, "no_speech_prob": 7.410700345644727e-05}, {"id": 280, "seek": 218600, "start": 2186.0, "end": 2204.0, "text": " They are processing they are just doing matrix matrix multiplication and then we have some nonlinear functions and essentially this is just really easy mathematics that is that is happening here and the outcome is then our prediction of our classes.", "tokens": [814, 366, 9007, 436, 366, 445, 884, 8141, 8141, 27290, 293, 550, 321, 362, 512, 2107, 28263, 6828, 293, 4476, 341, 307, 445, 534, 1858, 18666, 300, 307, 300, 307, 2737, 510, 293, 264, 9700, 307, 550, 527, 17630, 295, 527, 5359, 13], "temperature": 0.0, "avg_logprob": -0.14590208581153383, "compression_ratio": 1.638157894736842, "no_speech_prob": 2.0308327293605544e-05}, {"id": 281, "seek": 220400, "start": 2204.0, "end": 2216.0, "text": " We call the elements of these matrices I call them here W of course we have several of them but like easily speaking we look at the matrix and the weights like the elements of this matrix are the weights of our new network.", "tokens": [492, 818, 264, 4959, 295, 613, 32284, 286, 818, 552, 510, 343, 295, 1164, 321, 362, 2940, 295, 552, 457, 411, 3612, 4124, 321, 574, 412, 264, 8141, 293, 264, 17443, 411, 264, 4959, 295, 341, 8141, 366, 264, 17443, 295, 527, 777, 3209, 13], "temperature": 0.0, "avg_logprob": -0.11199370674465013, "compression_ratio": 1.8986784140969164, "no_speech_prob": 2.9362137865973637e-05}, {"id": 282, "seek": 220400, "start": 2216.0, "end": 2227.0, "text": " And what we now want to do in the training process we want to adjust the weights we want to make them better so that the prediction gets better and better by every training sample that the network is seeing.", "tokens": [400, 437, 321, 586, 528, 281, 360, 294, 264, 3097, 1399, 321, 528, 281, 4369, 264, 17443, 321, 528, 281, 652, 552, 1101, 370, 300, 264, 17630, 2170, 1101, 293, 1101, 538, 633, 3097, 6889, 300, 264, 3209, 307, 2577, 13], "temperature": 0.0, "avg_logprob": -0.11199370674465013, "compression_ratio": 1.8986784140969164, "no_speech_prob": 2.9362137865973637e-05}, {"id": 283, "seek": 222700, "start": 2227.0, "end": 2237.0, "text": " And now is the big question how can we get an accurate matrix W to minimize the loss and minimize is essentially the bird that we need to understand why we need calculus.", "tokens": [400, 586, 307, 264, 955, 1168, 577, 393, 321, 483, 364, 8559, 8141, 343, 281, 17522, 264, 4470, 293, 17522, 307, 4476, 264, 5255, 300, 321, 643, 281, 1223, 983, 321, 643, 33400, 13], "temperature": 0.0, "avg_logprob": -0.1070104297838713, "compression_ratio": 1.887029288702929, "no_speech_prob": 3.32856216118671e-05}, {"id": 284, "seek": 222700, "start": 2237.0, "end": 2254.0, "text": " The loss function has to be minimized that has something to do with the derivative of our last function of course because when we have the derivative we can define like the step the gradient we can define like in which direction do we have to change in order to minimize our loss.", "tokens": [440, 4470, 2445, 575, 281, 312, 4464, 1602, 300, 575, 746, 281, 360, 365, 264, 13760, 295, 527, 1036, 2445, 295, 1164, 570, 562, 321, 362, 264, 13760, 321, 393, 6964, 411, 264, 1823, 264, 16235, 321, 393, 6964, 411, 294, 597, 3513, 360, 321, 362, 281, 1319, 294, 1668, 281, 17522, 527, 4470, 13], "temperature": 0.0, "avg_logprob": -0.1070104297838713, "compression_ratio": 1.887029288702929, "no_speech_prob": 3.32856216118671e-05}, {"id": 285, "seek": 225400, "start": 2254.0, "end": 2270.0, "text": " And this is exactly what I talked about this method which is using derivatives and the chain rule essentially because we have like large new network structures is a method to approximate the best value for the weights of our new network.", "tokens": [400, 341, 307, 2293, 437, 286, 2825, 466, 341, 3170, 597, 307, 1228, 33733, 293, 264, 5021, 4978, 4476, 570, 321, 362, 411, 2416, 777, 3209, 9227, 307, 257, 3170, 281, 30874, 264, 1151, 2158, 337, 264, 17443, 295, 527, 777, 3209, 13], "temperature": 0.0, "avg_logprob": -0.08750814199447632, "compression_ratio": 1.764957264957265, "no_speech_prob": 0.00013937665789853781}, {"id": 286, "seek": 225400, "start": 2270.0, "end": 2281.0, "text": " And this method is called gradient descent and essentially mathematically seen it's pretty easy because it's just a derivative and the chain rule so nothing is happening more.", "tokens": [400, 341, 3170, 307, 1219, 16235, 23475, 293, 4476, 44003, 1612, 309, 311, 1238, 1858, 570, 309, 311, 445, 257, 13760, 293, 264, 5021, 4978, 370, 1825, 307, 2737, 544, 13], "temperature": 0.0, "avg_logprob": -0.08750814199447632, "compression_ratio": 1.764957264957265, "no_speech_prob": 0.00013937665789853781}, {"id": 287, "seek": 228100, "start": 2281.0, "end": 2288.0, "text": " OK, so after this motivation let's start with the second part of our tutorial session which is calculus.", "tokens": [2264, 11, 370, 934, 341, 12335, 718, 311, 722, 365, 264, 1150, 644, 295, 527, 7073, 5481, 597, 307, 33400, 13], "temperature": 0.0, "avg_logprob": -0.26403177938153666, "compression_ratio": 1.5114942528735633, "no_speech_prob": 5.026084545534104e-05}, {"id": 288, "seek": 228100, "start": 2288.0, "end": 2298.0, "text": " I said that we want to mainly talk about derivatives so scale at the relative gradients to covenants and we also want to have a deeper look at the chain rule.", "tokens": [286, 848, 300, 321, 528, 281, 8704, 751, 466, 33733, 370, 4373, 412, 264, 4972, 2771, 2448, 281, 598, 553, 1719, 293, 321, 611, 528, 281, 362, 257, 7731, 574, 412, 264, 5021, 4978, 13], "temperature": 0.0, "avg_logprob": -0.26403177938153666, "compression_ratio": 1.5114942528735633, "no_speech_prob": 5.026084545534104e-05}, {"id": 289, "seek": 229800, "start": 2298.0, "end": 2313.0, "text": " So derivatives in general a derivative is essentially a function which measures the sensibility of changing the function value so the output value and with respect to the input value.", "tokens": [407, 33733, 294, 2674, 257, 13760, 307, 4476, 257, 2445, 597, 8000, 264, 2923, 2841, 295, 4473, 264, 2445, 2158, 370, 264, 5598, 2158, 293, 365, 3104, 281, 264, 4846, 2158, 13], "temperature": 0.0, "avg_logprob": -0.1917375988430447, "compression_ratio": 1.7765957446808511, "no_speech_prob": 4.8227826482616365e-05}, {"id": 290, "seek": 229800, "start": 2313.0, "end": 2322.0, "text": " And it is well known for all of us and this scale at the derivative which is just a derivative of a real valued function f which is going from r to r.", "tokens": [400, 309, 307, 731, 2570, 337, 439, 295, 505, 293, 341, 4373, 412, 264, 13760, 597, 307, 445, 257, 13760, 295, 257, 957, 22608, 2445, 283, 597, 307, 516, 490, 367, 281, 367, 13], "temperature": 0.0, "avg_logprob": -0.1917375988430447, "compression_ratio": 1.7765957446808511, "no_speech_prob": 4.8227826482616365e-05}, {"id": 291, "seek": 232200, "start": 2322.0, "end": 2335.0, "text": " So this is the easy setting. If we go in a higher dimension we need to extend the calculus and also to a higher dimension setting and we call this the matrix calculus.", "tokens": [407, 341, 307, 264, 1858, 3287, 13, 759, 321, 352, 294, 257, 2946, 10139, 321, 643, 281, 10101, 264, 33400, 293, 611, 281, 257, 2946, 10139, 3287, 293, 321, 818, 341, 264, 8141, 33400, 13], "temperature": 0.0, "avg_logprob": -0.168345308303833, "compression_ratio": 1.7373737373737375, "no_speech_prob": 0.00012120250175939873}, {"id": 292, "seek": 232200, "start": 2335.0, "end": 2345.0, "text": " And here we're going to look at function which are similar to the ones that we say or that are denoted here in the second point which are for example functions from r and to r.", "tokens": [400, 510, 321, 434, 516, 281, 574, 412, 2445, 597, 366, 2531, 281, 264, 2306, 300, 321, 584, 420, 300, 366, 1441, 23325, 510, 294, 264, 1150, 935, 597, 366, 337, 1365, 6828, 490, 367, 293, 281, 367, 13], "temperature": 0.0, "avg_logprob": -0.168345308303833, "compression_ratio": 1.7373737373737375, "no_speech_prob": 0.00012120250175939873}, {"id": 293, "seek": 234500, "start": 2345.0, "end": 2359.0, "text": " And functions from r to rn also functions from rn to rm which are even more complex and the maximum here is a function which is starting with a matrix and has as an output a value air.", "tokens": [400, 6828, 490, 367, 281, 367, 77, 611, 6828, 490, 367, 77, 281, 367, 76, 597, 366, 754, 544, 3997, 293, 264, 6674, 510, 307, 257, 2445, 597, 307, 2891, 365, 257, 8141, 293, 575, 382, 364, 5598, 257, 2158, 1988, 13], "temperature": 0.0, "avg_logprob": -0.1528123367664426, "compression_ratio": 1.8556701030927836, "no_speech_prob": 5.243416308076121e-05}, {"id": 294, "seek": 234500, "start": 2359.0, "end": 2370.0, "text": " So function which is going from r power n times m to r. So these are more complex functions and we also want to have a look at how the derivative of these functions look like.", "tokens": [407, 2445, 597, 307, 516, 490, 367, 1347, 297, 1413, 275, 281, 367, 13, 407, 613, 366, 544, 3997, 6828, 293, 321, 611, 528, 281, 362, 257, 574, 412, 577, 264, 13760, 295, 613, 6828, 574, 411, 13], "temperature": 0.0, "avg_logprob": -0.1528123367664426, "compression_ratio": 1.8556701030927836, "no_speech_prob": 5.243416308076121e-05}, {"id": 295, "seek": 237000, "start": 2370.0, "end": 2380.0, "text": " So this is matrix calculus. And all the concepts that we saw so far in linear algebra and also that we're going to see here in the calculus part are not too difficult.", "tokens": [407, 341, 307, 8141, 33400, 13, 400, 439, 264, 10392, 300, 321, 1866, 370, 1400, 294, 8213, 21989, 293, 611, 300, 321, 434, 516, 281, 536, 510, 294, 264, 33400, 644, 366, 406, 886, 2252, 13], "temperature": 0.0, "avg_logprob": -0.1345882886721764, "compression_ratio": 1.6729857819905214, "no_speech_prob": 0.00010662772547220811}, {"id": 296, "seek": 237000, "start": 2380.0, "end": 2391.0, "text": " But what makes it difficult later when we want to apply them to the setting of new networks and deep learning and makes it like all that we have a lot of matrices that are concatenated.", "tokens": [583, 437, 1669, 309, 2252, 1780, 562, 321, 528, 281, 3079, 552, 281, 264, 3287, 295, 777, 9590, 293, 2452, 2539, 293, 1669, 309, 411, 439, 300, 321, 362, 257, 688, 295, 32284, 300, 366, 1588, 7186, 770, 13], "temperature": 0.0, "avg_logprob": -0.1345882886721764, "compression_ratio": 1.6729857819905214, "no_speech_prob": 0.00010662772547220811}, {"id": 297, "seek": 239100, "start": 2391.0, "end": 2406.0, "text": " So we need to use the chain rule and we also have kind of changing dimensions. So we really need to understand the basic setting of a derivative of a derivative in higher dimensions and also of all these matrix multiplication stuff.", "tokens": [407, 321, 643, 281, 764, 264, 5021, 4978, 293, 321, 611, 362, 733, 295, 4473, 12819, 13, 407, 321, 534, 643, 281, 1223, 264, 3875, 3287, 295, 257, 13760, 295, 257, 13760, 294, 2946, 12819, 293, 611, 295, 439, 613, 8141, 27290, 1507, 13], "temperature": 0.0, "avg_logprob": -0.10189675772061912, "compression_ratio": 1.7355371900826446, "no_speech_prob": 2.0991896235500462e-05}, {"id": 298, "seek": 239100, "start": 2406.0, "end": 2418.0, "text": " Because when you're like good to go for these things it will be easy for you to practices and to apply it in a more complex setting where we have a lot of values, a lot of variables, etc.", "tokens": [1436, 562, 291, 434, 411, 665, 281, 352, 337, 613, 721, 309, 486, 312, 1858, 337, 291, 281, 7525, 293, 281, 3079, 309, 294, 257, 544, 3997, 3287, 689, 321, 362, 257, 688, 295, 4190, 11, 257, 688, 295, 9102, 11, 5183, 13], "temperature": 0.0, "avg_logprob": -0.10189675772061912, "compression_ratio": 1.7355371900826446, "no_speech_prob": 2.0991896235500462e-05}, {"id": 299, "seek": 241800, "start": 2418.0, "end": 2426.0, "text": " So try to understand these concepts pretty well and then we can just apply them later in the lecture.", "tokens": [407, 853, 281, 1223, 613, 10392, 1238, 731, 293, 550, 321, 393, 445, 3079, 552, 1780, 294, 264, 7991, 13], "temperature": 0.0, "avg_logprob": -0.1545503817106548, "compression_ratio": 1.6680327868852458, "no_speech_prob": 9.044080798048526e-05}, {"id": 300, "seek": 241800, "start": 2426.0, "end": 2434.0, "text": " So again in overview I said like we look at different settings the easiest setting is the real valued settings. So we have a function from r to r.", "tokens": [407, 797, 294, 12492, 286, 848, 411, 321, 574, 412, 819, 6257, 264, 12889, 3287, 307, 264, 957, 22608, 6257, 13, 407, 321, 362, 257, 2445, 490, 367, 281, 367, 13], "temperature": 0.0, "avg_logprob": -0.1545503817106548, "compression_ratio": 1.6680327868852458, "no_speech_prob": 9.044080798048526e-05}, {"id": 301, "seek": 241800, "start": 2434.0, "end": 2443.0, "text": " We call it the derivative is just called the scalar derivative. I mean call it by the sign f prime of x. This is the thing that we did in high school already.", "tokens": [492, 818, 309, 264, 13760, 307, 445, 1219, 264, 39684, 13760, 13, 286, 914, 818, 309, 538, 264, 1465, 283, 5835, 295, 2031, 13, 639, 307, 264, 551, 300, 321, 630, 294, 1090, 1395, 1217, 13], "temperature": 0.0, "avg_logprob": -0.1545503817106548, "compression_ratio": 1.6680327868852458, "no_speech_prob": 9.044080798048526e-05}, {"id": 302, "seek": 244300, "start": 2443.0, "end": 2461.0, "text": " And the setting the higher dimension is setting one example here is the fine which is going from r and to r and the derivative of such a function is called gradient and we denoted by this nub last symbol which is this triangle but just reversed of f of x.", "tokens": [400, 264, 3287, 264, 2946, 10139, 307, 3287, 472, 1365, 510, 307, 264, 2489, 597, 307, 516, 490, 367, 293, 281, 367, 293, 264, 13760, 295, 1270, 257, 2445, 307, 1219, 16235, 293, 321, 1441, 23325, 538, 341, 297, 836, 1036, 5986, 597, 307, 341, 13369, 457, 445, 30563, 295, 283, 295, 2031, 13], "temperature": 0.0, "avg_logprob": -0.29629917802481814, "compression_ratio": 1.5838509316770186, "no_speech_prob": 8.706528024049476e-05}, {"id": 303, "seek": 246100, "start": 2461.0, "end": 2476.0, "text": " And similar when we look at a function which is taking us input a matrix and going into r so from r m to the power of n times m to r and we also call the derivative of such a function gradient and we use the same symbol this nub last symbol.", "tokens": [400, 2531, 562, 321, 574, 412, 257, 2445, 597, 307, 1940, 505, 4846, 257, 8141, 293, 516, 666, 367, 370, 490, 367, 275, 281, 264, 1347, 295, 297, 1413, 275, 281, 367, 293, 321, 611, 818, 264, 13760, 295, 1270, 257, 2445, 16235, 293, 321, 764, 264, 912, 5986, 341, 297, 836, 1036, 5986, 13], "temperature": 0.0, "avg_logprob": -0.13302283367868198, "compression_ratio": 1.544871794871795, "no_speech_prob": 1.350638376607094e-05}, {"id": 304, "seek": 247600, "start": 2476.0, "end": 2491.0, "text": " And the last setting we want to have look at is a function from r and to r m so both sides we have a higher dimension of setting and here we call the derivative a Jacobian and the notation here will be j and then a subscript we have this f.", "tokens": [400, 264, 1036, 3287, 321, 528, 281, 362, 574, 412, 307, 257, 2445, 490, 367, 293, 281, 367, 275, 370, 1293, 4881, 321, 362, 257, 2946, 10139, 295, 3287, 293, 510, 321, 818, 264, 13760, 257, 14117, 952, 293, 264, 24657, 510, 486, 312, 361, 293, 550, 257, 2325, 662, 321, 362, 341, 283, 13], "temperature": 0.0, "avg_logprob": -0.2114961580796675, "compression_ratio": 1.7843137254901962, "no_speech_prob": 6.3435168158321176e-06}, {"id": 305, "seek": 247600, "start": 2491.0, "end": 2498.0, "text": " So these are the settings that we're going to have a look at and we want to stop at the most easiest one scalar derivative.", "tokens": [407, 613, 366, 264, 6257, 300, 321, 434, 516, 281, 362, 257, 574, 412, 293, 321, 528, 281, 1590, 412, 264, 881, 12889, 472, 39684, 13760, 13], "temperature": 0.0, "avg_logprob": -0.2114961580796675, "compression_ratio": 1.7843137254901962, "no_speech_prob": 6.3435168158321176e-06}, {"id": 306, "seek": 249800, "start": 2498.0, "end": 2508.0, "text": " So this is easy and we have this real value function the notation as I said before as f prime of x or sometimes we also call it the f of the x.", "tokens": [407, 341, 307, 1858, 293, 321, 362, 341, 957, 2158, 2445, 264, 24657, 382, 286, 848, 949, 382, 283, 5835, 295, 2031, 420, 2171, 321, 611, 818, 309, 264, 283, 295, 264, 2031, 13], "temperature": 0.0, "avg_logprob": -0.13765872998184034, "compression_ratio": 1.7565217391304349, "no_speech_prob": 3.8568432501051575e-05}, {"id": 307, "seek": 249800, "start": 2508.0, "end": 2526.0, "text": " And the geometrical interpretation of the derivative of such a real valued function is something that we looked at already a lot and it's representing essentially at a chosen input value the slope of the tangent line to the graph of the function at that point.", "tokens": [400, 264, 12956, 15888, 14174, 295, 264, 13760, 295, 1270, 257, 957, 22608, 2445, 307, 746, 300, 321, 2956, 412, 1217, 257, 688, 293, 309, 311, 13460, 4476, 412, 257, 8614, 4846, 2158, 264, 13525, 295, 264, 27747, 1622, 281, 264, 4295, 295, 264, 2445, 412, 300, 935, 13], "temperature": 0.0, "avg_logprob": -0.13765872998184034, "compression_ratio": 1.7565217391304349, "no_speech_prob": 3.8568432501051575e-05}, {"id": 308, "seek": 252600, "start": 2526.0, "end": 2538.0, "text": " So we just like this red line here is describing the slope at certain input value and this is just a derivative of this function at this input value.", "tokens": [407, 321, 445, 411, 341, 2182, 1622, 510, 307, 16141, 264, 13525, 412, 1629, 4846, 2158, 293, 341, 307, 445, 257, 13760, 295, 341, 2445, 412, 341, 4846, 2158, 13], "temperature": 0.0, "avg_logprob": -0.14243206024169922, "compression_ratio": 1.60431654676259, "no_speech_prob": 3.557861055014655e-05}, {"id": 309, "seek": 252600, "start": 2538.0, "end": 2544.0, "text": " The slope of this tangent line so this is the geometrical interpretation.", "tokens": [440, 13525, 295, 341, 27747, 1622, 370, 341, 307, 264, 12956, 15888, 14174, 13], "temperature": 0.0, "avg_logprob": -0.14243206024169922, "compression_ratio": 1.60431654676259, "no_speech_prob": 3.557861055014655e-05}, {"id": 310, "seek": 254400, "start": 2544.0, "end": 2556.0, "text": " On the following two slides I summarized the most important rule when it comes to derivation. So here on the first slide I gave or included the most common functions and the respective derivatives on the second slide here.", "tokens": [1282, 264, 3480, 732, 9788, 286, 14611, 1602, 264, 881, 1021, 4978, 562, 309, 1487, 281, 10151, 399, 13, 407, 510, 322, 264, 700, 4137, 286, 2729, 420, 5556, 264, 881, 2689, 6828, 293, 264, 23649, 33733, 322, 264, 1150, 4137, 510, 13], "temperature": 0.0, "avg_logprob": -0.13872836791362958, "compression_ratio": 1.926530612244898, "no_speech_prob": 0.00010920753265963867}, {"id": 311, "seek": 254400, "start": 2556.0, "end": 2571.0, "text": " I included the rules when it comes to a combination of two function. So when you have a combination like the multiplication between two functions so f of x times g for example you need to follow the product rule in order to determine the derivative.", "tokens": [286, 5556, 264, 4474, 562, 309, 1487, 281, 257, 6562, 295, 732, 2445, 13, 407, 562, 291, 362, 257, 6562, 411, 264, 27290, 1296, 732, 6828, 370, 283, 295, 2031, 1413, 290, 337, 1365, 291, 643, 281, 1524, 264, 1674, 4978, 294, 1668, 281, 6997, 264, 13760, 13], "temperature": 0.0, "avg_logprob": -0.13872836791362958, "compression_ratio": 1.926530612244898, "no_speech_prob": 0.00010920753265963867}, {"id": 312, "seek": 257100, "start": 2571.0, "end": 2592.0, "text": " So make sure that you remind the most or the derivatives of the most common functions and also all these the relation rules when it comes to a combination of two functions because they will help you to be able to determine every possible or arbitrary and the derivative when it comes to the lecture content in the future.", "tokens": [407, 652, 988, 300, 291, 4160, 264, 881, 420, 264, 33733, 295, 264, 881, 2689, 6828, 293, 611, 439, 613, 264, 9721, 4474, 562, 309, 1487, 281, 257, 6562, 295, 732, 6828, 570, 436, 486, 854, 291, 281, 312, 1075, 281, 6997, 633, 1944, 420, 23211, 293, 264, 13760, 562, 309, 1487, 281, 264, 7991, 2701, 294, 264, 2027, 13], "temperature": 0.0, "avg_logprob": -0.09427833557128906, "compression_ratio": 1.7637362637362637, "no_speech_prob": 9.141385817201808e-05}, {"id": 313, "seek": 259200, "start": 2592.0, "end": 2621.0, "text": " So let's now switch to the higher dimension setting. So first of all we want to have a look at the multivariate function f in this case we start with the setting where our function is going from our end to our on the left side on this slide you can see a visualization of such a function and in this case we start from the space R2 and for each position in R2 we assign with the function f with our multivariate function f.", "tokens": [407, 718, 311, 586, 3679, 281, 264, 2946, 10139, 3287, 13, 407, 700, 295, 439, 321, 528, 281, 362, 257, 574, 412, 264, 2120, 592, 3504, 473, 2445, 283, 294, 341, 1389, 321, 722, 365, 264, 3287, 689, 527, 2445, 307, 516, 490, 527, 917, 281, 527, 322, 264, 1411, 1252, 322, 341, 4137, 291, 393, 536, 257, 25801, 295, 1270, 257, 2445, 293, 294, 341, 1389, 321, 722, 490, 264, 1901, 497, 17, 293, 337, 1184, 2535, 294, 497, 17, 321, 6269, 365, 264, 2445, 283, 365, 527, 2120, 592, 3504, 473, 2445, 283, 13], "temperature": 0.0, "avg_logprob": -0.1761145782470703, "compression_ratio": 1.8883928571428572, "no_speech_prob": 0.00012886595504824072}, {"id": 314, "seek": 262100, "start": 2621.0, "end": 2635.0, "text": " With our multivariate function a function value which is a real value to each of the values in R2 and the graph that you can see here is one example of a graph for such a multi-variate function.", "tokens": [2022, 527, 2120, 592, 3504, 473, 2445, 257, 2445, 2158, 597, 307, 257, 957, 2158, 281, 1184, 295, 264, 4190, 294, 497, 17, 293, 264, 4295, 300, 291, 393, 536, 510, 307, 472, 1365, 295, 257, 4295, 337, 1270, 257, 4825, 12, 34033, 473, 2445, 13], "temperature": 0.0, "avg_logprob": -0.12167070153054227, "compression_ratio": 1.75, "no_speech_prob": 7.392140832962468e-05}, {"id": 315, "seek": 262100, "start": 2635.0, "end": 2646.0, "text": " So you see that the setting is much more complicated than in the real valued world of functions. So the derivative of such a function is now given by a gradient as I mentioned before.", "tokens": [407, 291, 536, 300, 264, 3287, 307, 709, 544, 6179, 813, 294, 264, 957, 22608, 1002, 295, 6828, 13, 407, 264, 13760, 295, 1270, 257, 2445, 307, 586, 2212, 538, 257, 16235, 382, 286, 2835, 949, 13], "temperature": 0.0, "avg_logprob": -0.12167070153054227, "compression_ratio": 1.75, "no_speech_prob": 7.392140832962468e-05}, {"id": 316, "seek": 264600, "start": 2646.0, "end": 2659.0, "text": " And the notation of such a gradient is given by this NABLAS symbol which is just a reversed delta symbol and our gradient function is now going from Rn to Rn.", "tokens": [400, 264, 24657, 295, 1270, 257, 16235, 307, 2212, 538, 341, 426, 13868, 43, 3160, 5986, 597, 307, 445, 257, 30563, 8289, 5986, 293, 527, 16235, 2445, 307, 586, 516, 490, 497, 77, 281, 497, 77, 13], "temperature": 0.0, "avg_logprob": -0.17363875442081028, "compression_ratio": 1.523076923076923, "no_speech_prob": 0.00017102091806009412}, {"id": 317, "seek": 264600, "start": 2659.0, "end": 2671.0, "text": " So it's important to mention here that our output values not a real value anymore but now we have as an output a vector with N components.", "tokens": [407, 309, 311, 1021, 281, 2152, 510, 300, 527, 5598, 4190, 406, 257, 957, 2158, 3602, 457, 586, 321, 362, 382, 364, 5598, 257, 8062, 365, 426, 6677, 13], "temperature": 0.0, "avg_logprob": -0.17363875442081028, "compression_ratio": 1.523076923076923, "no_speech_prob": 0.00017102091806009412}, {"id": 318, "seek": 267100, "start": 2671.0, "end": 2695.0, "text": " And on the second line you can see how the gradient is defined and it's defined like the function is defined you take X as an input which is a vector in Rn and the output is now a vector where you can see that when you look at the first component the first component is given by the partial derivative of f of x with respect to the first variable.", "tokens": [400, 322, 264, 1150, 1622, 291, 393, 536, 577, 264, 16235, 307, 7642, 293, 309, 311, 7642, 411, 264, 2445, 307, 7642, 291, 747, 1783, 382, 364, 4846, 597, 307, 257, 8062, 294, 497, 77, 293, 264, 5598, 307, 586, 257, 8062, 689, 291, 393, 536, 300, 562, 291, 574, 412, 264, 700, 6542, 264, 700, 6542, 307, 2212, 538, 264, 14641, 13760, 295, 283, 295, 2031, 365, 3104, 281, 264, 700, 7006, 13], "temperature": 0.0, "avg_logprob": -0.08329118215120755, "compression_ratio": 1.7794871794871794, "no_speech_prob": 0.0001031609863275662}, {"id": 319, "seek": 269500, "start": 2695.0, "end": 2709.0, "text": " And correspondingly you determine the second up to the end component of our output vector. So it's just a part partial derivative of our function f and then with respect to the variables x1, x2, up to xn.", "tokens": [400, 11760, 356, 291, 6997, 264, 1150, 493, 281, 264, 917, 6542, 295, 527, 5598, 8062, 13, 407, 309, 311, 445, 257, 644, 14641, 13760, 295, 527, 2445, 283, 293, 550, 365, 3104, 281, 264, 9102, 2031, 16, 11, 2031, 17, 11, 493, 281, 2031, 77, 13], "temperature": 0.0, "avg_logprob": -0.09638568517324086, "compression_ratio": 1.528205128205128, "no_speech_prob": 7.999820809345692e-05}, {"id": 320, "seek": 269500, "start": 2709.0, "end": 2714.0, "text": " So that's the definition of our gradient here. Let's have a look at a more complex situation.", "tokens": [407, 300, 311, 264, 7123, 295, 527, 16235, 510, 13, 961, 311, 362, 257, 574, 412, 257, 544, 3997, 2590, 13], "temperature": 0.0, "avg_logprob": -0.09638568517324086, "compression_ratio": 1.528205128205128, "no_speech_prob": 7.999820809345692e-05}, {"id": 321, "seek": 271400, "start": 2714.0, "end": 2726.0, "text": " So when we now want to have a look at a function f which is going from the space R to the power of n times m so our input is essentially a matrix and to the space R.", "tokens": [407, 562, 321, 586, 528, 281, 362, 257, 574, 412, 257, 2445, 283, 597, 307, 516, 490, 264, 1901, 497, 281, 264, 1347, 295, 297, 1413, 275, 370, 527, 4846, 307, 4476, 257, 8141, 293, 281, 264, 1901, 497, 13], "temperature": 0.0, "avg_logprob": -0.14909497174349698, "compression_ratio": 1.375, "no_speech_prob": 6.702532118652016e-05}, {"id": 322, "seek": 272600, "start": 2726.0, "end": 2748.0, "text": " Then our visualization would look much more complicated and it's much more difficult to visualize such a function. But we also call the corresponding derivative gradient and the notation is the same we also use the snap-less symbol and our gradient function is now going from the space R to the power of n times m to R to the power of n times m.", "tokens": [1396, 527, 25801, 576, 574, 709, 544, 6179, 293, 309, 311, 709, 544, 2252, 281, 23273, 1270, 257, 2445, 13, 583, 321, 611, 818, 264, 11760, 13760, 16235, 293, 264, 24657, 307, 264, 912, 321, 611, 764, 264, 13650, 12, 1832, 5986, 293, 527, 16235, 2445, 307, 586, 516, 490, 264, 1901, 497, 281, 264, 1347, 295, 297, 1413, 275, 281, 497, 281, 264, 1347, 295, 297, 1413, 275, 13], "temperature": 0.0, "avg_logprob": -0.1492768107233821, "compression_ratio": 1.751269035532995, "no_speech_prob": 1.5602037819917314e-05}, {"id": 323, "seek": 274800, "start": 2748.0, "end": 2758.0, "text": " So the output again is now not a vector but a matrix and it's pretty similar to the idea of the gradient of the function from R into R.", "tokens": [407, 264, 5598, 797, 307, 586, 406, 257, 8062, 457, 257, 8141, 293, 309, 311, 1238, 2531, 281, 264, 1558, 295, 264, 16235, 295, 264, 2445, 490, 497, 666, 497, 13], "temperature": 0.0, "avg_logprob": -0.10474505323044797, "compression_ratio": 1.808695652173913, "no_speech_prob": 4.742405144497752e-05}, {"id": 324, "seek": 274800, "start": 2758.0, "end": 2777.0, "text": " Because here you can see that the output is a matrix and if you look at one special value of our output matrix for example again the first position 11 and you see that the value over there is the partial derivative of our function with respect to the first or to the variable x11.", "tokens": [1436, 510, 291, 393, 536, 300, 264, 5598, 307, 257, 8141, 293, 498, 291, 574, 412, 472, 2121, 2158, 295, 527, 5598, 8141, 337, 1365, 797, 264, 700, 2535, 2975, 293, 291, 536, 300, 264, 2158, 670, 456, 307, 264, 14641, 13760, 295, 527, 2445, 365, 3104, 281, 264, 700, 420, 281, 264, 7006, 2031, 5348, 13], "temperature": 0.0, "avg_logprob": -0.10474505323044797, "compression_ratio": 1.808695652173913, "no_speech_prob": 4.742405144497752e-05}, {"id": 325, "seek": 277700, "start": 2777.0, "end": 2791.0, "text": " And then you just complete the whole matrix with the partial derivatives of all variables of our input. So x which is in this case a matrix and then you can just build up the function similar to what we saw on the slide before.", "tokens": [400, 550, 291, 445, 3566, 264, 1379, 8141, 365, 264, 14641, 33733, 295, 439, 9102, 295, 527, 4846, 13, 407, 2031, 597, 307, 294, 341, 1389, 257, 8141, 293, 550, 291, 393, 445, 1322, 493, 264, 2445, 2531, 281, 437, 321, 1866, 322, 264, 4137, 949, 13], "temperature": 0.0, "avg_logprob": -0.08516602882972131, "compression_ratio": 1.6171428571428572, "no_speech_prob": 3.3587821235414594e-05}, {"id": 326, "seek": 277700, "start": 2791.0, "end": 2796.0, "text": " So this is just the same idea just for another setting.", "tokens": [407, 341, 307, 445, 264, 912, 1558, 445, 337, 1071, 3287, 13], "temperature": 0.0, "avg_logprob": -0.08516602882972131, "compression_ratio": 1.6171428571428572, "no_speech_prob": 3.3587821235414594e-05}, {"id": 327, "seek": 279600, "start": 2796.0, "end": 2813.0, "text": " On the following two slides we included two examples for you to better understand the concept of a gradient. So we included two functions f and g which are both functions from R to R so pretty simple functions and we determine the gradient for both of the functions.", "tokens": [1282, 264, 3480, 732, 9788, 321, 5556, 732, 5110, 337, 291, 281, 1101, 1223, 264, 3410, 295, 257, 16235, 13, 407, 321, 5556, 732, 6828, 283, 293, 290, 597, 366, 1293, 6828, 490, 497, 281, 497, 370, 1238, 2199, 6828, 293, 321, 6997, 264, 16235, 337, 1293, 295, 264, 6828, 13], "temperature": 0.0, "avg_logprob": -0.09507955204356801, "compression_ratio": 1.9285714285714286, "no_speech_prob": 2.1253184968372807e-05}, {"id": 328, "seek": 281300, "start": 2813.0, "end": 2826.0, "text": " So try to understand the calculation here or maybe better even try to do the calculations on your own and then compare your work to the solutions here on the slides.", "tokens": [407, 853, 281, 1223, 264, 17108, 510, 420, 1310, 1101, 754, 853, 281, 360, 264, 20448, 322, 428, 1065, 293, 550, 6794, 428, 589, 281, 264, 6547, 510, 322, 264, 9788, 13], "temperature": 0.0, "avg_logprob": -0.09148305395375127, "compression_ratio": 1.6813186813186813, "no_speech_prob": 3.452124656178057e-05}, {"id": 329, "seek": 281300, "start": 2826.0, "end": 2834.0, "text": " Okay, so now I want to move on to the last setting that we want to talk about in this part of calculus which is the vector valued functions.", "tokens": [1033, 11, 370, 586, 286, 528, 281, 1286, 322, 281, 264, 1036, 3287, 300, 321, 528, 281, 751, 466, 294, 341, 644, 295, 33400, 597, 307, 264, 8062, 22608, 6828, 13], "temperature": 0.0, "avg_logprob": -0.09148305395375127, "compression_ratio": 1.6813186813186813, "no_speech_prob": 3.452124656178057e-05}, {"id": 330, "seek": 283400, "start": 2834.0, "end": 2851.0, "text": " So what is a vector value function? So a vector value function is defined as a function from R and to Rm in comparison to the other settings before where the input space always have been Rm or even higher and the output space has always been R.", "tokens": [407, 437, 307, 257, 8062, 2158, 2445, 30, 407, 257, 8062, 2158, 2445, 307, 7642, 382, 257, 2445, 490, 497, 293, 281, 497, 76, 294, 9660, 281, 264, 661, 6257, 949, 689, 264, 4846, 1901, 1009, 362, 668, 497, 76, 420, 754, 2946, 293, 264, 5598, 1901, 575, 1009, 668, 497, 13], "temperature": 0.0, "avg_logprob": -0.16904762813023158, "compression_ratio": 1.6266666666666667, "no_speech_prob": 4.4601616536965594e-05}, {"id": 331, "seek": 285100, "start": 2851.0, "end": 2867.0, "text": " In this case now we have both in the input space and in the output space a higher dimensional space. So Rm and Rm that makes this function the most complicated one that we saw before or that we saw in general here in this tutorial session.", "tokens": [682, 341, 1389, 586, 321, 362, 1293, 294, 264, 4846, 1901, 293, 294, 264, 5598, 1901, 257, 2946, 18795, 1901, 13, 407, 497, 76, 293, 497, 76, 300, 1669, 341, 2445, 264, 881, 6179, 472, 300, 321, 1866, 949, 420, 300, 321, 1866, 294, 2674, 510, 294, 341, 7073, 5481, 13], "temperature": 0.0, "avg_logprob": -0.1402826049111106, "compression_ratio": 1.5933333333333333, "no_speech_prob": 5.687948942068033e-06}, {"id": 332, "seek": 286700, "start": 2867.0, "end": 2880.0, "text": " And if you have a look at the second line you can see the definition of the function so it takes as input a vector which is here denoted by x and which is just a tuple with n elements because it's just an element of Rm.", "tokens": [400, 498, 291, 362, 257, 574, 412, 264, 1150, 1622, 291, 393, 536, 264, 7123, 295, 264, 2445, 370, 309, 2516, 382, 4846, 257, 8062, 597, 307, 510, 1441, 23325, 538, 2031, 293, 597, 307, 445, 257, 2604, 781, 365, 297, 4959, 570, 309, 311, 445, 364, 4478, 295, 497, 76, 13], "temperature": 0.0, "avg_logprob": -0.12527133048848904, "compression_ratio": 1.751219512195122, "no_speech_prob": 4.0977778553497046e-05}, {"id": 333, "seek": 286700, "start": 2880.0, "end": 2891.0, "text": " So the elements are demoted from x1 to up to xm and the output of our function f is now a vector again but this vector is an element of Rm.", "tokens": [407, 264, 4959, 366, 1371, 23325, 490, 2031, 16, 281, 493, 281, 2031, 76, 293, 264, 5598, 295, 527, 2445, 283, 307, 586, 257, 8062, 797, 457, 341, 8062, 307, 364, 4478, 295, 497, 76, 13], "temperature": 0.0, "avg_logprob": -0.12527133048848904, "compression_ratio": 1.751219512195122, "no_speech_prob": 4.0977778553497046e-05}, {"id": 334, "seek": 289100, "start": 2891.0, "end": 2898.0, "text": " So it has components which are elements real values and we have M of them in a tuple.", "tokens": [407, 309, 575, 6677, 597, 366, 4959, 957, 4190, 293, 321, 362, 376, 295, 552, 294, 257, 2604, 781, 13], "temperature": 0.0, "avg_logprob": -0.11446757455473965, "compression_ratio": 1.696035242290749, "no_speech_prob": 9.58880627877079e-05}, {"id": 335, "seek": 289100, "start": 2898.0, "end": 2906.0, "text": " So let's have a look at the first of them. So f, the first component of our output vector is given by f1 of x.", "tokens": [407, 718, 311, 362, 257, 574, 412, 264, 700, 295, 552, 13, 407, 283, 11, 264, 700, 6542, 295, 527, 5598, 8062, 307, 2212, 538, 283, 16, 295, 2031, 13], "temperature": 0.0, "avg_logprob": -0.11446757455473965, "compression_ratio": 1.696035242290749, "no_speech_prob": 9.58880627877079e-05}, {"id": 336, "seek": 289100, "start": 2906.0, "end": 2919.0, "text": " And here f1 is determining or defining another function which is going from Rm is it takes x as an input so it's going from Rm and its output value is an element of R so just a real value.", "tokens": [400, 510, 283, 16, 307, 23751, 420, 17827, 1071, 2445, 597, 307, 516, 490, 497, 76, 307, 309, 2516, 2031, 382, 364, 4846, 370, 309, 311, 516, 490, 497, 76, 293, 1080, 5598, 2158, 307, 364, 4478, 295, 497, 370, 445, 257, 957, 2158, 13], "temperature": 0.0, "avg_logprob": -0.11446757455473965, "compression_ratio": 1.696035242290749, "no_speech_prob": 9.58880627877079e-05}, {"id": 337, "seek": 291900, "start": 2919.0, "end": 2934.0, "text": " And this is the setting that we saw before it's a function from Rm to R and we have these functions we have M function of them all concatenated or put in this vector and this is the output of our function f.", "tokens": [400, 341, 307, 264, 3287, 300, 321, 1866, 949, 309, 311, 257, 2445, 490, 497, 76, 281, 497, 293, 321, 362, 613, 6828, 321, 362, 376, 2445, 295, 552, 439, 1588, 7186, 770, 420, 829, 294, 341, 8062, 293, 341, 307, 264, 5598, 295, 527, 2445, 283, 13], "temperature": 0.0, "avg_logprob": -0.13100721285893366, "compression_ratio": 1.5681818181818181, "no_speech_prob": 3.4668784792302176e-05}, {"id": 338, "seek": 293400, "start": 2934.0, "end": 2952.0, "text": " When we now look at the derivative of our vector value function and the derivative is called the Jacobian matrix and it's denoted by j with the n-dxf and it's going from the space Rm to the space R to the power of m times n.", "tokens": [1133, 321, 586, 574, 412, 264, 13760, 295, 527, 8062, 2158, 2445, 293, 264, 13760, 307, 1219, 264, 14117, 952, 8141, 293, 309, 311, 1441, 23325, 538, 361, 365, 264, 297, 12, 67, 87, 69, 293, 309, 311, 516, 490, 264, 1901, 497, 76, 281, 264, 1901, 497, 281, 264, 1347, 295, 275, 1413, 297, 13], "temperature": 0.0, "avg_logprob": -0.19936660130818684, "compression_ratio": 1.5555555555555556, "no_speech_prob": 2.030999348789919e-05}, {"id": 339, "seek": 295200, "start": 2952.0, "end": 2970.0, "text": " And you can see here in the definition in the second line that it takes as an input a value or a vector x which is an element of Rm so it again consists of n elements x1 up to xn like on the left side and the output is now a matrix.", "tokens": [400, 291, 393, 536, 510, 294, 264, 7123, 294, 264, 1150, 1622, 300, 309, 2516, 382, 364, 4846, 257, 2158, 420, 257, 8062, 2031, 597, 307, 364, 4478, 295, 497, 76, 370, 309, 797, 14689, 295, 297, 4959, 2031, 16, 493, 281, 2031, 77, 411, 322, 264, 1411, 1252, 293, 264, 5598, 307, 586, 257, 8141, 13], "temperature": 0.0, "avg_logprob": -0.11240673846885806, "compression_ratio": 1.5064935064935066, "no_speech_prob": 3.0496585168293677e-06}, {"id": 340, "seek": 297000, "start": 2970.0, "end": 2988.0, "text": " And this matrix is just a generalization of the gradient that we have been looking before so look at the first row of this matrix. The first row is now given by the partial derivatives of this function f1 which has been a function from Rm to R.", "tokens": [400, 341, 8141, 307, 445, 257, 2674, 2144, 295, 264, 16235, 300, 321, 362, 668, 1237, 949, 370, 574, 412, 264, 700, 5386, 295, 341, 8141, 13, 440, 700, 5386, 307, 586, 2212, 538, 264, 14641, 33733, 295, 341, 2445, 283, 16, 597, 575, 668, 257, 2445, 490, 497, 76, 281, 497, 13], "temperature": 0.0, "avg_logprob": -0.11075725890042488, "compression_ratio": 1.564102564102564, "no_speech_prob": 7.984880539879669e-06}, {"id": 341, "seek": 298800, "start": 2988.0, "end": 3011.0, "text": " And the row is given by all the partial derivatives with respect to the first variable x1 up to the variable xn and this is the first row. The second row is now doing the same with the function f2 and then we go down and repeat this procedure up to the mth row where we have the partial derivatives of the f of the m's function fm.", "tokens": [400, 264, 5386, 307, 2212, 538, 439, 264, 14641, 33733, 365, 3104, 281, 264, 700, 7006, 2031, 16, 493, 281, 264, 7006, 2031, 77, 293, 341, 307, 264, 700, 5386, 13, 440, 1150, 5386, 307, 586, 884, 264, 912, 365, 264, 2445, 283, 17, 293, 550, 321, 352, 760, 293, 7149, 341, 10747, 493, 281, 264, 275, 392, 5386, 689, 321, 362, 264, 14641, 33733, 295, 264, 283, 295, 264, 275, 311, 2445, 283, 76, 13], "temperature": 0.0, "avg_logprob": -0.06578883528709412, "compression_ratio": 1.8186813186813187, "no_speech_prob": 8.156138392223511e-06}, {"id": 342, "seek": 301100, "start": 3011.0, "end": 3022.0, "text": " So you can see that it's just kind of combining the gradients of the functions f1 up to fm so it's kind of a generalization of the concept of the gradient.", "tokens": [407, 291, 393, 536, 300, 309, 311, 445, 733, 295, 21928, 264, 2771, 2448, 295, 264, 6828, 283, 16, 493, 281, 283, 76, 370, 309, 311, 733, 295, 257, 2674, 2144, 295, 264, 3410, 295, 264, 16235, 13], "temperature": 0.0, "avg_logprob": -0.10612725607956512, "compression_ratio": 1.6243654822335025, "no_speech_prob": 6.174854934215546e-05}, {"id": 343, "seek": 301100, "start": 3022.0, "end": 3035.0, "text": " Again we included an example for you to practice your understanding and here we define the function f which is going from R2 to R2 and we define it on these slides.", "tokens": [3764, 321, 5556, 364, 1365, 337, 291, 281, 3124, 428, 3701, 293, 510, 321, 6964, 264, 2445, 283, 597, 307, 516, 490, 497, 17, 281, 497, 17, 293, 321, 6964, 309, 322, 613, 9788, 13], "temperature": 0.0, "avg_logprob": -0.10612725607956512, "compression_ratio": 1.6243654822335025, "no_speech_prob": 6.174854934215546e-05}, {"id": 344, "seek": 303500, "start": 3035.0, "end": 3045.0, "text": " So just try to understand or maybe try to derive on your own the derivatives of this function and then compare your solutions to the solutions here on this slide.", "tokens": [407, 445, 853, 281, 1223, 420, 1310, 853, 281, 28446, 322, 428, 1065, 264, 33733, 295, 341, 2445, 293, 550, 6794, 428, 6547, 281, 264, 6547, 510, 322, 341, 4137, 13], "temperature": 0.0, "avg_logprob": -0.058987099677324295, "compression_ratio": 1.6384180790960452, "no_speech_prob": 5.2661784138763323e-05}, {"id": 345, "seek": 303500, "start": 3045.0, "end": 3052.0, "text": " So before we finish the second part of our tutorial session today I want to go a little bit more into detail of the chain rule.", "tokens": [407, 949, 321, 2413, 264, 1150, 644, 295, 527, 7073, 5481, 965, 286, 528, 281, 352, 257, 707, 857, 544, 666, 2607, 295, 264, 5021, 4978, 13], "temperature": 0.0, "avg_logprob": -0.058987099677324295, "compression_ratio": 1.6384180790960452, "no_speech_prob": 5.2661784138763323e-05}, {"id": 346, "seek": 305200, "start": 3052.0, "end": 3067.0, "text": " We saw the chain rule already before when I introduced the main or the most important derivation rules but I want to go a little bit more into detail because the chain rule might be the most difficult of these rules and also it will play the most important rule in the item.", "tokens": [492, 1866, 264, 5021, 4978, 1217, 949, 562, 286, 7268, 264, 2135, 420, 264, 881, 1021, 10151, 399, 4474, 457, 286, 528, 281, 352, 257, 707, 857, 544, 666, 2607, 570, 264, 5021, 4978, 1062, 312, 264, 881, 2252, 295, 613, 4474, 293, 611, 309, 486, 862, 264, 881, 1021, 4978, 294, 264, 3174, 13], "temperature": 0.0, "avg_logprob": -0.14448267535159462, "compression_ratio": 1.6580310880829014, "no_speech_prob": 2.40857589233201e-05}, {"id": 347, "seek": 305200, "start": 3067.0, "end": 3068.0, "text": " The setting here.", "tokens": [440, 3287, 510, 13], "temperature": 0.0, "avg_logprob": -0.14448267535159462, "compression_ratio": 1.6580310880829014, "no_speech_prob": 2.40857589233201e-05}, {"id": 348, "seek": 305200, "start": 3068.0, "end": 3070.0, "text": " So let's have a look at it.", "tokens": [407, 718, 311, 362, 257, 574, 412, 309, 13], "temperature": 0.0, "avg_logprob": -0.14448267535159462, "compression_ratio": 1.6580310880829014, "no_speech_prob": 2.40857589233201e-05}, {"id": 349, "seek": 307000, "start": 3070.0, "end": 3082.0, "text": " The setting is now that we are back at a real valued function function so we have a look at a function which is going from R to R and the setting is that we have a composition of two functions.", "tokens": [440, 3287, 307, 586, 300, 321, 366, 646, 412, 257, 957, 22608, 2445, 2445, 370, 321, 362, 257, 574, 412, 257, 2445, 597, 307, 516, 490, 497, 281, 497, 293, 264, 3287, 307, 300, 321, 362, 257, 12686, 295, 732, 6828, 13], "temperature": 0.0, "avg_logprob": -0.10648207786755684, "compression_ratio": 1.906832298136646, "no_speech_prob": 1.3822706932842266e-05}, {"id": 350, "seek": 307000, "start": 3082.0, "end": 3090.0, "text": " So we are given a function h of x which is defined by the composition of f and g so it is defined by f of g of x.", "tokens": [407, 321, 366, 2212, 257, 2445, 276, 295, 2031, 597, 307, 7642, 538, 264, 12686, 295, 283, 293, 290, 370, 309, 307, 7642, 538, 283, 295, 290, 295, 2031, 13], "temperature": 0.0, "avg_logprob": -0.10648207786755684, "compression_ratio": 1.906832298136646, "no_speech_prob": 1.3822706932842266e-05}, {"id": 351, "seek": 309000, "start": 3090.0, "end": 3100.0, "text": " And now we want our task is to determine the derivative of this composition of functions and this is the setting where we use the chain rule.", "tokens": [400, 586, 321, 528, 527, 5633, 307, 281, 6997, 264, 13760, 295, 341, 12686, 295, 6828, 293, 341, 307, 264, 3287, 689, 321, 764, 264, 5021, 4978, 13], "temperature": 0.0, "avg_logprob": -0.04598820209503174, "compression_ratio": 1.7624309392265194, "no_speech_prob": 5.088490070193075e-06}, {"id": 352, "seek": 309000, "start": 3100.0, "end": 3111.0, "text": " And we have now kind of four steps that you need to follow in order to use the chain rule and to determine the correct derivative of this function.", "tokens": [400, 321, 362, 586, 733, 295, 1451, 4439, 300, 291, 643, 281, 1524, 294, 1668, 281, 764, 264, 5021, 4978, 293, 281, 6997, 264, 3006, 13760, 295, 341, 2445, 13], "temperature": 0.0, "avg_logprob": -0.04598820209503174, "compression_ratio": 1.7624309392265194, "no_speech_prob": 5.088490070193075e-06}, {"id": 353, "seek": 309000, "start": 3111.0, "end": 3113.0, "text": " So let's look at these steps.", "tokens": [407, 718, 311, 574, 412, 613, 4439, 13], "temperature": 0.0, "avg_logprob": -0.04598820209503174, "compression_ratio": 1.7624309392265194, "no_speech_prob": 5.088490070193075e-06}, {"id": 354, "seek": 311300, "start": 3113.0, "end": 3122.0, "text": " The first step is the introduction of an intermediate variable so the intermediate variable here will be our variable u which is defined to be g of x.", "tokens": [440, 700, 1823, 307, 264, 9339, 295, 364, 19376, 7006, 370, 264, 19376, 7006, 510, 486, 312, 527, 7006, 344, 597, 307, 7642, 281, 312, 290, 295, 2031, 13], "temperature": 0.0, "avg_logprob": -0.0636697496686663, "compression_ratio": 1.7455621301775148, "no_speech_prob": 8.77308048075065e-06}, {"id": 355, "seek": 311300, "start": 3122.0, "end": 3134.0, "text": " This is our intermediate one because it's the one inside the function of f so we just call it u then we can write our function h of x as f of u.", "tokens": [639, 307, 527, 19376, 472, 570, 309, 311, 264, 472, 1854, 264, 2445, 295, 283, 370, 321, 445, 818, 309, 344, 550, 321, 393, 2464, 527, 2445, 276, 295, 2031, 382, 283, 295, 344, 13], "temperature": 0.0, "avg_logprob": -0.0636697496686663, "compression_ratio": 1.7455621301775148, "no_speech_prob": 8.77308048075065e-06}, {"id": 356, "seek": 313400, "start": 3134.0, "end": 3146.0, "text": " And the second step is now that we have to derive two derivatives namely the first one is the derivative of our function f with respect to our intermediate variable u.", "tokens": [400, 264, 1150, 1823, 307, 586, 300, 321, 362, 281, 28446, 732, 33733, 20926, 264, 700, 472, 307, 264, 13760, 295, 527, 2445, 283, 365, 3104, 281, 527, 19376, 7006, 344, 13], "temperature": 0.0, "avg_logprob": -0.07102200272795442, "compression_ratio": 2.326797385620915, "no_speech_prob": 1.3538076018448919e-05}, {"id": 357, "seek": 313400, "start": 3146.0, "end": 3158.0, "text": " And the second one is the derivative of our function g with respect to our variable x. And you can also call it the derivative of our function of variable u with respect to our variable x.", "tokens": [400, 264, 1150, 472, 307, 264, 13760, 295, 527, 2445, 290, 365, 3104, 281, 527, 7006, 2031, 13, 400, 291, 393, 611, 818, 309, 264, 13760, 295, 527, 2445, 295, 7006, 344, 365, 3104, 281, 527, 7006, 2031, 13], "temperature": 0.0, "avg_logprob": -0.07102200272795442, "compression_ratio": 2.326797385620915, "no_speech_prob": 1.3538076018448919e-05}, {"id": 358, "seek": 315800, "start": 3158.0, "end": 3170.0, "text": " When you did that then you can just compose these two derivatives and then we'll then give you the derivative of h with respect to the variable x.", "tokens": [1133, 291, 630, 300, 550, 291, 393, 445, 35925, 613, 732, 33733, 293, 550, 321, 603, 550, 976, 291, 264, 13760, 295, 276, 365, 3104, 281, 264, 7006, 2031, 13], "temperature": 0.0, "avg_logprob": -0.10956293419946599, "compression_ratio": 1.7435897435897436, "no_speech_prob": 4.438033283804543e-05}, {"id": 359, "seek": 315800, "start": 3170.0, "end": 3174.0, "text": " So you just compose them you just multiply them.", "tokens": [407, 291, 445, 35925, 552, 291, 445, 12972, 552, 13], "temperature": 0.0, "avg_logprob": -0.10956293419946599, "compression_ratio": 1.7435897435897436, "no_speech_prob": 4.438033283804543e-05}, {"id": 360, "seek": 315800, "start": 3174.0, "end": 3185.0, "text": " And what you do then in the last step is you substitute the intermediate variable back to the original function so you go back from u to g of x.", "tokens": [400, 437, 291, 360, 550, 294, 264, 1036, 1823, 307, 291, 15802, 264, 19376, 7006, 646, 281, 264, 3380, 2445, 370, 291, 352, 646, 490, 344, 281, 290, 295, 2031, 13], "temperature": 0.0, "avg_logprob": -0.10956293419946599, "compression_ratio": 1.7435897435897436, "no_speech_prob": 4.438033283804543e-05}, {"id": 361, "seek": 318500, "start": 3185.0, "end": 3191.0, "text": " And we will have a look at an example because that my this might look a little bit abstract but it's not too difficult.", "tokens": [400, 321, 486, 362, 257, 574, 412, 364, 1365, 570, 300, 452, 341, 1062, 574, 257, 707, 857, 12649, 457, 309, 311, 406, 886, 2252, 13], "temperature": 0.0, "avg_logprob": -0.07360132535298665, "compression_ratio": 1.796116504854369, "no_speech_prob": 2.871679316740483e-05}, {"id": 362, "seek": 318500, "start": 3191.0, "end": 3200.0, "text": " So let's have a function here which is given by h of x is the sign of x to the power of two.", "tokens": [407, 718, 311, 362, 257, 2445, 510, 597, 307, 2212, 538, 276, 295, 2031, 307, 264, 1465, 295, 2031, 281, 264, 1347, 295, 732, 13], "temperature": 0.0, "avg_logprob": -0.07360132535298665, "compression_ratio": 1.796116504854369, "no_speech_prob": 2.871679316740483e-05}, {"id": 363, "seek": 318500, "start": 3200.0, "end": 3210.0, "text": " So this is essentially to the composition of two functions namely the first function being the sign function and the second one is the x to the power of two.", "tokens": [407, 341, 307, 4476, 281, 264, 12686, 295, 732, 6828, 20926, 264, 700, 2445, 885, 264, 1465, 2445, 293, 264, 1150, 472, 307, 264, 2031, 281, 264, 1347, 295, 732, 13], "temperature": 0.0, "avg_logprob": -0.07360132535298665, "compression_ratio": 1.796116504854369, "no_speech_prob": 2.871679316740483e-05}, {"id": 364, "seek": 321000, "start": 3210.0, "end": 3215.0, "text": " And our task here is now that we want to determine the derivative of it.", "tokens": [400, 527, 5633, 510, 307, 586, 300, 321, 528, 281, 6997, 264, 13760, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.09909269084101138, "compression_ratio": 1.572289156626506, "no_speech_prob": 1.871576205303427e-05}, {"id": 365, "seek": 321000, "start": 3215.0, "end": 3221.0, "text": " So as I mentioned before we have here like this setting of the composition of two functions f and g.", "tokens": [407, 382, 286, 2835, 949, 321, 362, 510, 411, 341, 3287, 295, 264, 12686, 295, 732, 6828, 283, 293, 290, 13], "temperature": 0.0, "avg_logprob": -0.09909269084101138, "compression_ratio": 1.572289156626506, "no_speech_prob": 1.871576205303427e-05}, {"id": 366, "seek": 321000, "start": 3221.0, "end": 3229.0, "text": " F is here defined as sign of x and g is defined as x to the power of two. So x squared.", "tokens": [479, 307, 510, 7642, 382, 1465, 295, 2031, 293, 290, 307, 7642, 382, 2031, 281, 264, 1347, 295, 732, 13, 407, 2031, 8889, 13], "temperature": 0.0, "avg_logprob": -0.09909269084101138, "compression_ratio": 1.572289156626506, "no_speech_prob": 1.871576205303427e-05}, {"id": 367, "seek": 322900, "start": 3229.0, "end": 3242.0, "text": " And again we follow our first step so the first step is to introduce our intermediate variable as we saw before. So let us define u the variable u to be the variable x squared.", "tokens": [400, 797, 321, 1524, 527, 700, 1823, 370, 264, 700, 1823, 307, 281, 5366, 527, 19376, 7006, 382, 321, 1866, 949, 13, 407, 718, 505, 6964, 344, 264, 7006, 344, 281, 312, 264, 7006, 2031, 8889, 13], "temperature": 0.0, "avg_logprob": -0.09724401103125678, "compression_ratio": 1.707182320441989, "no_speech_prob": 1.1445277777966112e-05}, {"id": 368, "seek": 322900, "start": 3242.0, "end": 3252.0, "text": " And the second step we now want to calculate the two derivatives first of f with respect to u and second one of g with respect to x.", "tokens": [400, 264, 1150, 1823, 321, 586, 528, 281, 8873, 264, 732, 33733, 700, 295, 283, 365, 3104, 281, 344, 293, 1150, 472, 295, 290, 365, 3104, 281, 2031, 13], "temperature": 0.0, "avg_logprob": -0.09724401103125678, "compression_ratio": 1.707182320441989, "no_speech_prob": 1.1445277777966112e-05}, {"id": 369, "seek": 325200, "start": 3252.0, "end": 3265.0, "text": " And you see here the derivative of f with respect to u will now be cosine of u and the derivative of g with respect to x is now two times x.", "tokens": [400, 291, 536, 510, 264, 13760, 295, 283, 365, 3104, 281, 344, 486, 586, 312, 23565, 295, 344, 293, 264, 13760, 295, 290, 365, 3104, 281, 2031, 307, 586, 732, 1413, 2031, 13], "temperature": 0.0, "avg_logprob": -0.08363439315973326, "compression_ratio": 2.286624203821656, "no_speech_prob": 3.558480239007622e-05}, {"id": 370, "seek": 325200, "start": 3265.0, "end": 3281.0, "text": " And what we then have to do is we have to multiply these both derivatives. So the derivative of the function h with respect to x is now the derivative of f with respect to u times the derivative of u with respect to x.", "tokens": [400, 437, 321, 550, 362, 281, 360, 307, 321, 362, 281, 12972, 613, 1293, 33733, 13, 407, 264, 13760, 295, 264, 2445, 276, 365, 3104, 281, 2031, 307, 586, 264, 13760, 295, 283, 365, 3104, 281, 344, 1413, 264, 13760, 295, 344, 365, 3104, 281, 2031, 13], "temperature": 0.0, "avg_logprob": -0.08363439315973326, "compression_ratio": 2.286624203821656, "no_speech_prob": 3.558480239007622e-05}, {"id": 371, "seek": 328100, "start": 3281.0, "end": 3286.0, "text": " So we obtain cosine of u times two times x.", "tokens": [407, 321, 12701, 23565, 295, 344, 1413, 732, 1413, 2031, 13], "temperature": 0.0, "avg_logprob": -0.09086687224251884, "compression_ratio": 1.6588235294117648, "no_speech_prob": 9.828231850406155e-05}, {"id": 372, "seek": 328100, "start": 3286.0, "end": 3296.0, "text": " And the first step now was that we want to substitute the intermediate variable back. So now we can just instead of u we substitute our x squared.", "tokens": [400, 264, 700, 1823, 586, 390, 300, 321, 528, 281, 15802, 264, 19376, 7006, 646, 13, 407, 586, 321, 393, 445, 2602, 295, 344, 321, 15802, 527, 2031, 8889, 13], "temperature": 0.0, "avg_logprob": -0.09086687224251884, "compression_ratio": 1.6588235294117648, "no_speech_prob": 9.828231850406155e-05}, {"id": 373, "seek": 328100, "start": 3296.0, "end": 3303.0, "text": " So then our derivative of h with respect to x is giving us cosine of x squared times two x.", "tokens": [407, 550, 527, 13760, 295, 276, 365, 3104, 281, 2031, 307, 2902, 505, 23565, 295, 2031, 8889, 1413, 732, 2031, 13], "temperature": 0.0, "avg_logprob": -0.09086687224251884, "compression_ratio": 1.6588235294117648, "no_speech_prob": 9.828231850406155e-05}, {"id": 374, "seek": 330300, "start": 3303.0, "end": 3312.0, "text": " And this is an example. So for how you can apply the chain rule to a setting like our setting that we have here.", "tokens": [400, 341, 307, 364, 1365, 13, 407, 337, 577, 291, 393, 3079, 264, 5021, 4978, 281, 257, 3287, 411, 527, 3287, 300, 321, 362, 510, 13], "temperature": 0.0, "avg_logprob": -0.07582308084536822, "compression_ratio": 1.6210526315789473, "no_speech_prob": 4.3223382817814127e-05}, {"id": 375, "seek": 330300, "start": 3312.0, "end": 3321.0, "text": " So I highly recommend you to understand these steps if you don't understand them yet. Just have a look at them. They are not too difficult.", "tokens": [407, 286, 5405, 2748, 291, 281, 1223, 613, 4439, 498, 291, 500, 380, 1223, 552, 1939, 13, 1449, 362, 257, 574, 412, 552, 13, 814, 366, 406, 886, 2252, 13], "temperature": 0.0, "avg_logprob": -0.07582308084536822, "compression_ratio": 1.6210526315789473, "no_speech_prob": 4.3223382817814127e-05}, {"id": 376, "seek": 330300, "start": 3321.0, "end": 3325.0, "text": " But this is a nice idea of how you can solve the chain.", "tokens": [583, 341, 307, 257, 1481, 1558, 295, 577, 291, 393, 5039, 264, 5021, 13], "temperature": 0.0, "avg_logprob": -0.07582308084536822, "compression_ratio": 1.6210526315789473, "no_speech_prob": 4.3223382817814127e-05}, {"id": 377, "seek": 332500, "start": 3325.0, "end": 3334.0, "text": " So the last slide I want to introduce the total derivative chain rule. So this is referring to a little bit more complicated setting than we saw before.", "tokens": [407, 264, 1036, 4137, 286, 528, 281, 5366, 264, 3217, 13760, 5021, 4978, 13, 407, 341, 307, 13761, 281, 257, 707, 857, 544, 6179, 3287, 813, 321, 1866, 949, 13], "temperature": 0.0, "avg_logprob": -0.12708313082471306, "compression_ratio": 1.674757281553398, "no_speech_prob": 6.891645898576826e-05}, {"id": 378, "seek": 332500, "start": 3334.0, "end": 3348.0, "text": " I mean now we not only have a function which is a composition of two functions, but we now have a function which takes more than one input value namely x, but also u one of x until u and of x.", "tokens": [286, 914, 586, 321, 406, 787, 362, 257, 2445, 597, 307, 257, 12686, 295, 732, 6828, 11, 457, 321, 586, 362, 257, 2445, 597, 2516, 544, 813, 472, 4846, 2158, 20926, 2031, 11, 457, 611, 344, 472, 295, 2031, 1826, 344, 293, 295, 2031, 13], "temperature": 0.0, "avg_logprob": -0.12708313082471306, "compression_ratio": 1.674757281553398, "no_speech_prob": 6.891645898576826e-05}, {"id": 379, "seek": 334800, "start": 3348.0, "end": 3358.0, "text": " And then we want to determine the derivative with respect to x. And this is just using or applying the chain rule to all of the different settings here.", "tokens": [400, 550, 321, 528, 281, 6997, 264, 13760, 365, 3104, 281, 2031, 13, 400, 341, 307, 445, 1228, 420, 9275, 264, 5021, 4978, 281, 439, 295, 264, 819, 6257, 510, 13], "temperature": 0.0, "avg_logprob": -0.08581957918532351, "compression_ratio": 2.0485436893203883, "no_speech_prob": 3.9049929910106584e-05}, {"id": 380, "seek": 334800, "start": 3358.0, "end": 3373.0, "text": " So the first part of our summation will be the derivative of f with respect to x. And then you have the summation of all different chain rules applied to all of the compositions that we have here like the composition of f with u one up to the composition of f with u n.", "tokens": [407, 264, 700, 644, 295, 527, 28811, 486, 312, 264, 13760, 295, 283, 365, 3104, 281, 2031, 13, 400, 550, 291, 362, 264, 28811, 295, 439, 819, 5021, 4474, 6456, 281, 439, 295, 264, 43401, 300, 321, 362, 510, 411, 264, 12686, 295, 283, 365, 344, 472, 493, 281, 264, 12686, 295, 283, 365, 344, 297, 13], "temperature": 0.0, "avg_logprob": -0.08581957918532351, "compression_ratio": 2.0485436893203883, "no_speech_prob": 3.9049929910106584e-05}, {"id": 381, "seek": 337300, "start": 3373.0, "end": 3382.0, "text": " And then you have to apply out to some all of the applications of this chain rule and then you get kind of the total derivative chain rule.", "tokens": [400, 550, 291, 362, 281, 3079, 484, 281, 512, 439, 295, 264, 5821, 295, 341, 5021, 4978, 293, 550, 291, 483, 733, 295, 264, 3217, 13760, 5021, 4978, 13], "temperature": 0.0, "avg_logprob": -0.0918189457484654, "compression_ratio": 1.7623762376237624, "no_speech_prob": 5.893687921343371e-05}, {"id": 382, "seek": 337300, "start": 3382.0, "end": 3396.0, "text": " So in case that we will be needing this in the application for our neural networks later, it might be interesting to have this in mind because this is really really useful when you have more than one input variables.", "tokens": [407, 294, 1389, 300, 321, 486, 312, 18006, 341, 294, 264, 3861, 337, 527, 18161, 9590, 1780, 11, 309, 1062, 312, 1880, 281, 362, 341, 294, 1575, 570, 341, 307, 534, 534, 4420, 562, 291, 362, 544, 813, 472, 4846, 9102, 13], "temperature": 0.0, "avg_logprob": -0.0918189457484654, "compression_ratio": 1.7623762376237624, "no_speech_prob": 5.893687921343371e-05}, {"id": 383, "seek": 339600, "start": 3396.0, "end": 3402.0, "text": " Okay, so this has been everything for our calculus part. So let's move on to our probability part.", "tokens": [1033, 11, 370, 341, 575, 668, 1203, 337, 527, 33400, 644, 13, 407, 718, 311, 1286, 322, 281, 527, 8482, 644, 13], "temperature": 0.0, "avg_logprob": -0.10181547844246643, "compression_ratio": 1.712962962962963, "no_speech_prob": 3.083280171267688e-05}, {"id": 384, "seek": 339600, "start": 3402.0, "end": 3419.0, "text": " Okay, probability theory is now forming the last and third part of our tutorial session. And I want to talk about the main concepts of probability theory beginning with the definition of probability spaces random variables and also the probability distribution functions.", "tokens": [1033, 11, 8482, 5261, 307, 586, 15745, 264, 1036, 293, 2636, 644, 295, 527, 7073, 5481, 13, 400, 286, 528, 281, 751, 466, 264, 2135, 10392, 295, 8482, 5261, 2863, 365, 264, 7123, 295, 8482, 7673, 4974, 9102, 293, 611, 264, 8482, 7316, 6828, 13], "temperature": 0.0, "avg_logprob": -0.10181547844246643, "compression_ratio": 1.712962962962963, "no_speech_prob": 3.083280171267688e-05}, {"id": 385, "seek": 341900, "start": 3419.0, "end": 3432.0, "text": " And finally, we want to introduce or review the concept of mean and variances. And we also want to have a look at the most important probability distribution like a normal distribution or uniform distribution.", "tokens": [400, 2721, 11, 321, 528, 281, 5366, 420, 3131, 264, 3410, 295, 914, 293, 1374, 21518, 13, 400, 321, 611, 528, 281, 362, 257, 574, 412, 264, 881, 1021, 8482, 7316, 411, 257, 2710, 7316, 420, 9452, 7316, 13], "temperature": 0.0, "avg_logprob": -0.11626497904459636, "compression_ratio": 1.7014925373134329, "no_speech_prob": 7.4574481914169155e-06}, {"id": 386, "seek": 341900, "start": 3432.0, "end": 3440.0, "text": " Okay, so let us start with the definition of a probability space. A probability space essentially is describing a random experiment.", "tokens": [1033, 11, 370, 718, 505, 722, 365, 264, 7123, 295, 257, 8482, 1901, 13, 316, 8482, 1901, 4476, 307, 16141, 257, 4974, 5120, 13], "temperature": 0.0, "avg_logprob": -0.11626497904459636, "compression_ratio": 1.7014925373134329, "no_speech_prob": 7.4574481914169155e-06}, {"id": 387, "seek": 344000, "start": 3440.0, "end": 3450.0, "text": " Always when we talk in probability theory, we are looking at the random experiment and we work on this setting. So this is kind of the basic setting that we are looking at.", "tokens": [11270, 562, 321, 751, 294, 8482, 5261, 11, 321, 366, 1237, 412, 264, 4974, 5120, 293, 321, 589, 322, 341, 3287, 13, 407, 341, 307, 733, 295, 264, 3875, 3287, 300, 321, 366, 1237, 412, 13], "temperature": 0.0, "avg_logprob": -0.07310492651803153, "compression_ratio": 1.7447916666666667, "no_speech_prob": 4.73546824650839e-05}, {"id": 388, "seek": 344000, "start": 3450.0, "end": 3460.0, "text": " And for everything that we are doing, we can define this basic setting. So let us have a look how it is defined. So it's consisting essentially of three elements.", "tokens": [400, 337, 1203, 300, 321, 366, 884, 11, 321, 393, 6964, 341, 3875, 3287, 13, 407, 718, 505, 362, 257, 574, 577, 309, 307, 7642, 13, 407, 309, 311, 33921, 4476, 295, 1045, 4959, 13], "temperature": 0.0, "avg_logprob": -0.07310492651803153, "compression_ratio": 1.7447916666666667, "no_speech_prob": 4.73546824650839e-05}, {"id": 389, "seek": 346000, "start": 3460.0, "end": 3475.0, "text": " The first element is our sample space and it's denoted with the omega, the big omega. And the omega is describing a set, which is representing the set of all possible outcomes of our random experiment.", "tokens": [440, 700, 4478, 307, 527, 6889, 1901, 293, 309, 311, 1441, 23325, 365, 264, 10498, 11, 264, 955, 10498, 13, 400, 264, 10498, 307, 16141, 257, 992, 11, 597, 307, 13460, 264, 992, 295, 439, 1944, 10070, 295, 527, 4974, 5120, 13], "temperature": 0.0, "avg_logprob": -0.08181573515352995, "compression_ratio": 1.425531914893617, "no_speech_prob": 1.5150949366216082e-05}, {"id": 390, "seek": 347500, "start": 3475.0, "end": 3492.0, "text": " The second element is our event space F. Our event space F is now a set and the elements of our set are again sets. And they are subset of omega describing events. So we are looking at possible events for our random experiment.", "tokens": [440, 1150, 4478, 307, 527, 2280, 1901, 479, 13, 2621, 2280, 1901, 479, 307, 586, 257, 992, 293, 264, 4959, 295, 527, 992, 366, 797, 6352, 13, 400, 436, 366, 25993, 295, 10498, 16141, 3931, 13, 407, 321, 366, 1237, 412, 1944, 3931, 337, 527, 4974, 5120, 13], "temperature": 0.0, "avg_logprob": -0.11173795208786473, "compression_ratio": 1.7546012269938651, "no_speech_prob": 2.059209691651631e-06}, {"id": 391, "seek": 347500, "start": 3492.0, "end": 3498.0, "text": " And these events are all represented in our event space F.", "tokens": [400, 613, 3931, 366, 439, 10379, 294, 527, 2280, 1901, 479, 13], "temperature": 0.0, "avg_logprob": -0.11173795208786473, "compression_ratio": 1.7546012269938651, "no_speech_prob": 2.059209691651631e-06}, {"id": 392, "seek": 349800, "start": 3498.0, "end": 3517.0, "text": " So these are the underlying definitions. And I think this one is known to everybody. It's our probability measure, which is denoted by this P. And P is just a definition or the definition of P is it is a function, which is defined on the set F and going into the real values.", "tokens": [407, 613, 366, 264, 14217, 21988, 13, 400, 286, 519, 341, 472, 307, 2570, 281, 2201, 13, 467, 311, 527, 8482, 3481, 11, 597, 307, 1441, 23325, 538, 341, 430, 13, 400, 430, 307, 445, 257, 7123, 420, 264, 7123, 295, 430, 307, 309, 307, 257, 2445, 11, 597, 307, 7642, 322, 264, 992, 479, 293, 516, 666, 264, 957, 4190, 13], "temperature": 0.0, "avg_logprob": -0.10140570004781087, "compression_ratio": 1.5804597701149425, "no_speech_prob": 5.107761899125762e-05}, {"id": 393, "seek": 351700, "start": 3517.0, "end": 3536.0, "text": " So into R and it satisfies for the following three properties that we want to have a look at. So as it is giving us a probability, it is a non negative function. So on all elements of F, which are sets, it has to be bigger or equal to zero.", "tokens": [407, 666, 497, 293, 309, 44271, 337, 264, 3480, 1045, 7221, 300, 321, 528, 281, 362, 257, 574, 412, 13, 407, 382, 309, 307, 2902, 505, 257, 8482, 11, 309, 307, 257, 2107, 3671, 2445, 13, 407, 322, 439, 4959, 295, 479, 11, 597, 366, 6352, 11, 309, 575, 281, 312, 3801, 420, 2681, 281, 4018, 13], "temperature": 0.0, "avg_logprob": -0.12692301390600985, "compression_ratio": 1.437125748502994, "no_speech_prob": 2.7398416932555847e-05}, {"id": 394, "seek": 353600, "start": 3536.0, "end": 3550.0, "text": " The second property is that our probability function, when we use it on the whole set of all possible outcomes. So that's our omega, which is always an element of our set F, our event space F.", "tokens": [440, 1150, 4707, 307, 300, 527, 8482, 2445, 11, 562, 321, 764, 309, 322, 264, 1379, 992, 295, 439, 1944, 10070, 13, 407, 300, 311, 527, 10498, 11, 597, 307, 1009, 364, 4478, 295, 527, 992, 479, 11, 527, 2280, 1901, 479, 13], "temperature": 0.0, "avg_logprob": -0.1119173435454673, "compression_ratio": 1.4014598540145986, "no_speech_prob": 2.3613969460711814e-05}, {"id": 395, "seek": 355000, "start": 3550.0, "end": 3568.0, "text": " Then the probability always has to be one. So this is the biggest probability that our probability measure can take and it's always defined to be one for the biggest possible set.", "tokens": [1396, 264, 8482, 1009, 575, 281, 312, 472, 13, 407, 341, 307, 264, 3880, 8482, 300, 527, 8482, 3481, 393, 747, 293, 309, 311, 1009, 7642, 281, 312, 472, 337, 264, 3880, 1944, 992, 13], "temperature": 0.0, "avg_logprob": -0.10065960884094238, "compression_ratio": 1.5565217391304347, "no_speech_prob": 1.5728768630651757e-05}, {"id": 396, "seek": 356800, "start": 3568.0, "end": 3585.0, "text": " So in different events, we call them OBD node and by A1 up to A n, which are all elements of our set F. And when we take the union of these sets and the probability of this union must be equal to the sum of the probability of each of these events.", "tokens": [407, 294, 819, 3931, 11, 321, 818, 552, 35538, 35, 9984, 293, 538, 316, 16, 493, 281, 316, 297, 11, 597, 366, 439, 4959, 295, 527, 992, 479, 13, 400, 562, 321, 747, 264, 11671, 295, 613, 6352, 293, 264, 8482, 295, 341, 11671, 1633, 312, 2681, 281, 264, 2408, 295, 264, 8482, 295, 1184, 295, 613, 3931, 13], "temperature": 0.0, "avg_logprob": -0.22608973487975106, "compression_ratio": 1.5246913580246915, "no_speech_prob": 4.6031320380279794e-05}, {"id": 397, "seek": 358500, "start": 3585.0, "end": 3603.0, "text": " Three properties that our function has to satisfy in order to be a probability measure function. So whenever we talk from a random experiment, we're going to have this underlying set up, which is consisting of our set omega, our set F and our probability measure P.", "tokens": [6244, 7221, 300, 527, 2445, 575, 281, 19319, 294, 1668, 281, 312, 257, 8482, 3481, 2445, 13, 407, 5699, 321, 751, 490, 257, 4974, 5120, 11, 321, 434, 516, 281, 362, 341, 14217, 992, 493, 11, 597, 307, 33921, 295, 527, 992, 10498, 11, 527, 992, 479, 293, 527, 8482, 3481, 430, 13], "temperature": 0.0, "avg_logprob": -0.0725660993341814, "compression_ratio": 1.5680473372781065, "no_speech_prob": 1.5638866898370907e-05}, {"id": 398, "seek": 360300, "start": 3603.0, "end": 3619.0, "text": " So as I said in the beginning, the probability space provides a form and model of a random experiment. So whenever you struggle to understand what is going on, try to go back and try to define the probability space and what are you looking at?", "tokens": [407, 382, 286, 848, 294, 264, 2863, 11, 264, 8482, 1901, 6417, 257, 1254, 293, 2316, 295, 257, 4974, 5120, 13, 407, 5699, 291, 7799, 281, 1223, 437, 307, 516, 322, 11, 853, 281, 352, 646, 293, 853, 281, 6964, 264, 8482, 1901, 293, 437, 366, 291, 1237, 412, 30], "temperature": 0.0, "avg_logprob": -0.09389388954246437, "compression_ratio": 1.6680851063829787, "no_speech_prob": 2.5742951038409956e-05}, {"id": 399, "seek": 360300, "start": 3619.0, "end": 3630.0, "text": " Okay, so we want to have a look at one example. It's a really famous and popular example in probability theory. It's namely tossing a six sided die.", "tokens": [1033, 11, 370, 321, 528, 281, 362, 257, 574, 412, 472, 1365, 13, 467, 311, 257, 534, 4618, 293, 3743, 1365, 294, 8482, 5261, 13, 467, 311, 20926, 14432, 278, 257, 2309, 41651, 978, 13], "temperature": 0.0, "avg_logprob": -0.09389388954246437, "compression_ratio": 1.6680851063829787, "no_speech_prob": 2.5742951038409956e-05}, {"id": 400, "seek": 363000, "start": 3630.0, "end": 3648.0, "text": " And I want to look with you together. What is our probability space here? So the sample space was defined as the set of all possible outcomes. Of course, for a die, our outcomes are the numbers one, two, three, four, five, four, six. So this is our sample space.", "tokens": [400, 286, 528, 281, 574, 365, 291, 1214, 13, 708, 307, 527, 8482, 1901, 510, 30, 407, 264, 6889, 1901, 390, 7642, 382, 264, 992, 295, 439, 1944, 10070, 13, 2720, 1164, 11, 337, 257, 978, 11, 527, 10070, 366, 264, 3547, 472, 11, 732, 11, 1045, 11, 1451, 11, 1732, 11, 1451, 11, 2309, 13, 407, 341, 307, 527, 6889, 1901, 13], "temperature": 0.0, "avg_logprob": -0.1239977309952921, "compression_ratio": 1.5232558139534884, "no_speech_prob": 0.0001436045567970723}, {"id": 401, "seek": 364800, "start": 3648.0, "end": 3664.0, "text": " Our event space is now depending on how we want to define it. I defined here three possible event spaces and you can choose on your own depending on how you set up your random experiment, you have to define your own event space.", "tokens": [2621, 2280, 1901, 307, 586, 5413, 322, 577, 321, 528, 281, 6964, 309, 13, 286, 7642, 510, 1045, 1944, 2280, 7673, 293, 291, 393, 2826, 322, 428, 1065, 5413, 322, 577, 291, 992, 493, 428, 4974, 5120, 11, 291, 362, 281, 6964, 428, 1065, 2280, 1901, 13], "temperature": 0.0, "avg_logprob": -0.11113260306564032, "compression_ratio": 1.6888888888888889, "no_speech_prob": 2.2900230760569684e-05}, {"id": 402, "seek": 366400, "start": 3664.0, "end": 3677.0, "text": " The first one that we are looking at is our F one here. It's the smallest possible. So it's containing the empty set and it's containing whole omega. This is kind of the most simple one.", "tokens": [440, 700, 472, 300, 321, 366, 1237, 412, 307, 527, 479, 472, 510, 13, 467, 311, 264, 16998, 1944, 13, 407, 309, 311, 19273, 264, 6707, 992, 293, 309, 311, 19273, 1379, 10498, 13, 639, 307, 733, 295, 264, 881, 2199, 472, 13], "temperature": 0.0, "avg_logprob": -0.11809011347153607, "compression_ratio": 1.6751269035532994, "no_speech_prob": 1.5124520359677263e-05}, {"id": 403, "seek": 366400, "start": 3677.0, "end": 3687.0, "text": " Another one, which is also pretty simple is our F two, which is denoted as P of omega. And this is just a set of all possible subsets of omega.", "tokens": [3996, 472, 11, 597, 307, 611, 1238, 2199, 307, 527, 479, 732, 11, 597, 307, 1441, 23325, 382, 430, 295, 10498, 13, 400, 341, 307, 445, 257, 992, 295, 439, 1944, 2090, 1385, 295, 10498, 13], "temperature": 0.0, "avg_logprob": -0.11809011347153607, "compression_ratio": 1.6751269035532994, "no_speech_prob": 1.5124520359677263e-05}, {"id": 404, "seek": 368700, "start": 3687.0, "end": 3698.0, "text": " So this is the biggest one that we can have. And our F three is now an example for a third possibility, where we don't have the smallest one and not the biggest one, but we now have kind of one in between.", "tokens": [407, 341, 307, 264, 3880, 472, 300, 321, 393, 362, 13, 400, 527, 479, 1045, 307, 586, 364, 1365, 337, 257, 2636, 7959, 11, 689, 321, 500, 380, 362, 264, 16998, 472, 293, 406, 264, 3880, 472, 11, 457, 321, 586, 362, 733, 295, 472, 294, 1296, 13], "temperature": 0.0, "avg_logprob": -0.08742251556910825, "compression_ratio": 1.7142857142857142, "no_speech_prob": 2.201832649006974e-05}, {"id": 405, "seek": 368700, "start": 3698.0, "end": 3707.0, "text": " And here you can see that the elements are defined to be the empty set, the whole set omega. And we also have two other sets, a one and a two.", "tokens": [400, 510, 291, 393, 536, 300, 264, 4959, 366, 7642, 281, 312, 264, 6707, 992, 11, 264, 1379, 992, 10498, 13, 400, 321, 611, 362, 732, 661, 6352, 11, 257, 472, 293, 257, 732, 13], "temperature": 0.0, "avg_logprob": -0.08742251556910825, "compression_ratio": 1.7142857142857142, "no_speech_prob": 2.201832649006974e-05}, {"id": 406, "seek": 370700, "start": 3707.0, "end": 3718.0, "text": " And one is the set of odd numbers, so including one, three and five and a two is the set of even numbers two, four and six.", "tokens": [400, 472, 307, 264, 992, 295, 7401, 3547, 11, 370, 3009, 472, 11, 1045, 293, 1732, 293, 257, 732, 307, 264, 992, 295, 754, 3547, 732, 11, 1451, 293, 2309, 13], "temperature": 0.0, "avg_logprob": -0.1319108896477278, "compression_ratio": 1.7587939698492463, "no_speech_prob": 1.2392795724736061e-05}, {"id": 407, "seek": 370700, "start": 3718.0, "end": 3733.0, "text": " So as I said, the probability measure is now a function, which is going from F to R. And we have to make sure that the probability measure of our empty set equals to zero and the probability measure of our omega equals to one.", "tokens": [407, 382, 286, 848, 11, 264, 8482, 3481, 307, 586, 257, 2445, 11, 597, 307, 516, 490, 479, 281, 497, 13, 400, 321, 362, 281, 652, 988, 300, 264, 8482, 3481, 295, 527, 6707, 992, 6915, 281, 4018, 293, 264, 8482, 3481, 295, 527, 10498, 6915, 281, 472, 13], "temperature": 0.0, "avg_logprob": -0.1319108896477278, "compression_ratio": 1.7587939698492463, "no_speech_prob": 1.2392795724736061e-05}, {"id": 408, "seek": 373300, "start": 3733.0, "end": 3749.0, "text": " And now we can have a look at the third event space, our F three. And in that case, we have more than only these two sets. We have more than the empty set and the whole set omega, but we also have to set a one and a two.", "tokens": [400, 586, 321, 393, 362, 257, 574, 412, 264, 2636, 2280, 1901, 11, 527, 479, 1045, 13, 400, 294, 300, 1389, 11, 321, 362, 544, 813, 787, 613, 732, 6352, 13, 492, 362, 544, 813, 264, 6707, 992, 293, 264, 1379, 992, 10498, 11, 457, 321, 611, 362, 281, 992, 257, 472, 293, 257, 732, 13], "temperature": 0.0, "avg_logprob": -0.0971454938252767, "compression_ratio": 1.5492957746478873, "no_speech_prob": 2.7320696972310543e-05}, {"id": 409, "seek": 374900, "start": 3749.0, "end": 3776.0, "text": " What we can see is that our a one and our a two, if we take the union of these two events, we get the whole set omega. So what we know them with the third probability of our probability measure, you can just go back to the slide before if you don't remind it, you can just derive that you have to know or you have to make sure that the probability of a one plus the probability of a two must be equal to one.", "tokens": [708, 321, 393, 536, 307, 300, 527, 257, 472, 293, 527, 257, 732, 11, 498, 321, 747, 264, 11671, 295, 613, 732, 3931, 11, 321, 483, 264, 1379, 992, 10498, 13, 407, 437, 321, 458, 552, 365, 264, 2636, 8482, 295, 527, 8482, 3481, 11, 291, 393, 445, 352, 646, 281, 264, 4137, 949, 498, 291, 500, 380, 4160, 309, 11, 291, 393, 445, 28446, 300, 291, 362, 281, 458, 420, 291, 362, 281, 652, 988, 300, 264, 8482, 295, 257, 472, 1804, 264, 8482, 295, 257, 732, 1633, 312, 2681, 281, 472, 13], "temperature": 0.0, "avg_logprob": -0.07265728347155513, "compression_ratio": 1.880184331797235, "no_speech_prob": 3.220796770619927e-06}, {"id": 410, "seek": 377600, "start": 3776.0, "end": 3781.0, "text": " So this is what we know that we have to satisfy as a probability measure.", "tokens": [407, 341, 307, 437, 321, 458, 300, 321, 362, 281, 19319, 382, 257, 8482, 3481, 13], "temperature": 0.0, "avg_logprob": -0.11413977897330506, "compression_ratio": 1.718918918918919, "no_speech_prob": 9.34737254283391e-05}, {"id": 411, "seek": 377600, "start": 3781.0, "end": 3798.0, "text": " And now we want to take as an example the event space F three. So the possible probability measure here, like we can look at one possibility where we define the probability of a one equal to the probability of a two, namely both being one half.", "tokens": [400, 586, 321, 528, 281, 747, 382, 364, 1365, 264, 2280, 1901, 479, 1045, 13, 407, 264, 1944, 8482, 3481, 510, 11, 411, 321, 393, 574, 412, 472, 7959, 689, 321, 6964, 264, 8482, 295, 257, 472, 2681, 281, 264, 8482, 295, 257, 732, 11, 20926, 1293, 885, 472, 1922, 13], "temperature": 0.0, "avg_logprob": -0.11413977897330506, "compression_ratio": 1.718918918918919, "no_speech_prob": 9.34737254283391e-05}, {"id": 412, "seek": 379800, "start": 3798.0, "end": 3819.0, "text": " So if we take the sum of both of them, we obtain one, which is then fine with the observation that we derived before another possibility would be that we define our probability measure on a one, be equal to one fourth and our probability measure on a two, be equal to three, four.", "tokens": [407, 498, 321, 747, 264, 2408, 295, 1293, 295, 552, 11, 321, 12701, 472, 11, 597, 307, 550, 2489, 365, 264, 14816, 300, 321, 18949, 949, 1071, 7959, 576, 312, 300, 321, 6964, 527, 8482, 3481, 322, 257, 472, 11, 312, 2681, 281, 472, 6409, 293, 527, 8482, 3481, 322, 257, 732, 11, 312, 2681, 281, 1045, 11, 1451, 13], "temperature": 0.0, "avg_logprob": -0.13927780091762543, "compression_ratio": 1.7721518987341771, "no_speech_prob": 8.830209844745696e-05}, {"id": 413, "seek": 381900, "start": 3819.0, "end": 3829.0, "text": " So this is also adding up to one, but these are two different measure probability measures that you can define on the event space F three.", "tokens": [407, 341, 307, 611, 5127, 493, 281, 472, 11, 457, 613, 366, 732, 819, 3481, 8482, 8000, 300, 291, 393, 6964, 322, 264, 2280, 1901, 479, 1045, 13], "temperature": 0.0, "avg_logprob": -0.11314719373529608, "compression_ratio": 1.6559139784946237, "no_speech_prob": 2.357511402806267e-05}, {"id": 414, "seek": 381900, "start": 3829.0, "end": 3842.0, "text": " So it depends on how you define your probability measure of how your random experiment looks like. And this is kind of the distribution that random variable can take on.", "tokens": [407, 309, 5946, 322, 577, 291, 6964, 428, 8482, 3481, 295, 577, 428, 4974, 5120, 1542, 411, 13, 400, 341, 307, 733, 295, 264, 7316, 300, 4974, 7006, 393, 747, 322, 13], "temperature": 0.0, "avg_logprob": -0.11314719373529608, "compression_ratio": 1.6559139784946237, "no_speech_prob": 2.357511402806267e-05}, {"id": 415, "seek": 384200, "start": 3842.0, "end": 3859.0, "text": " So I said random variable. So what is a random variable now? A random variable is now a function which is defined on the probability space. So this two, two, three elements and it's a mapping from the sample space to the real numbers.", "tokens": [407, 286, 848, 4974, 7006, 13, 407, 437, 307, 257, 4974, 7006, 586, 30, 316, 4974, 7006, 307, 586, 257, 2445, 597, 307, 7642, 322, 264, 8482, 1901, 13, 407, 341, 732, 11, 732, 11, 1045, 4959, 293, 309, 311, 257, 18350, 490, 264, 6889, 1901, 281, 264, 957, 3547, 13], "temperature": 0.0, "avg_logprob": -0.2554688193581321, "compression_ratio": 1.5294117647058822, "no_speech_prob": 6.487182690761983e-05}, {"id": 416, "seek": 385900, "start": 3859.0, "end": 3876.0, "text": " So the definition now is giving our x, which is the notation for our random variable is going from our space omega into the space r. So for each element in our sample space, we assign an element in R.", "tokens": [407, 264, 7123, 586, 307, 2902, 527, 2031, 11, 597, 307, 264, 24657, 337, 527, 4974, 7006, 307, 516, 490, 527, 1901, 10498, 666, 264, 1901, 367, 13, 407, 337, 1184, 4478, 294, 527, 6889, 1901, 11, 321, 6269, 364, 4478, 294, 497, 13], "temperature": 0.0, "avg_logprob": -0.12247563401858012, "compression_ratio": 1.5384615384615385, "no_speech_prob": 1.4635514162364416e-05}, {"id": 417, "seek": 387600, "start": 3876.0, "end": 3892.0, "text": " And for random variables in general, we distinguish between two settings namely the first one being the discrete setting and the second one being the continuous setting. So we're going to have a look at examples for discrete and continuous settings.", "tokens": [400, 337, 4974, 9102, 294, 2674, 11, 321, 20206, 1296, 732, 6257, 20926, 264, 700, 472, 885, 264, 27706, 3287, 293, 264, 1150, 472, 885, 264, 10957, 3287, 13, 407, 321, 434, 516, 281, 362, 257, 574, 412, 5110, 337, 27706, 293, 10957, 6257, 13], "temperature": 0.0, "avg_logprob": -0.0890005364709971, "compression_ratio": 1.638157894736842, "no_speech_prob": 1.2851626706833486e-05}, {"id": 418, "seek": 389200, "start": 3892.0, "end": 3911.0, "text": " So again, I want to talk about tossing a fair six sided die. So this is again the same example, then we saw before for our probability setting and the underlying experiment as we derived before is now that we have our omega or event space, which again is the set from the numbers of one up to six.", "tokens": [407, 797, 11, 286, 528, 281, 751, 466, 14432, 278, 257, 3143, 2309, 41651, 978, 13, 407, 341, 307, 797, 264, 912, 1365, 11, 550, 321, 1866, 949, 337, 527, 8482, 3287, 293, 264, 14217, 5120, 382, 321, 18949, 949, 307, 586, 300, 321, 362, 527, 10498, 420, 2280, 1901, 11, 597, 797, 307, 264, 992, 490, 264, 3547, 295, 472, 493, 281, 2309, 13], "temperature": 0.0, "avg_logprob": -0.13028157275655997, "compression_ratio": 1.563157894736842, "no_speech_prob": 1.617877023818437e-05}, {"id": 419, "seek": 391100, "start": 3911.0, "end": 3930.0, "text": " Let me have our event space, which I now define as being the set of all possible subsets of omega. So our P of omega and we define our probability measure as P of x equals to one sticks for all elements in our sample space.", "tokens": [961, 385, 362, 527, 2280, 1901, 11, 597, 286, 586, 6964, 382, 885, 264, 992, 295, 439, 1944, 2090, 1385, 295, 10498, 13, 407, 527, 430, 295, 10498, 293, 321, 6964, 527, 8482, 3481, 382, 430, 295, 2031, 6915, 281, 472, 12518, 337, 439, 4959, 294, 527, 6889, 1901, 13], "temperature": 0.0, "avg_logprob": -0.1106371349758572, "compression_ratio": 1.448051948051948, "no_speech_prob": 1.9936132957809605e-05}, {"id": 420, "seek": 393000, "start": 3930.0, "end": 3940.0, "text": " So for every number that I can let our die can result in I have the probability one thing. So it's kind of a uniform distribution.", "tokens": [407, 337, 633, 1230, 300, 286, 393, 718, 527, 978, 393, 1874, 294, 286, 362, 264, 8482, 472, 551, 13, 407, 309, 311, 733, 295, 257, 9452, 7316, 13], "temperature": 0.0, "avg_logprob": -0.14393115043640137, "compression_ratio": 1.5789473684210527, "no_speech_prob": 9.187623618345242e-06}, {"id": 421, "seek": 393000, "start": 3940.0, "end": 3953.0, "text": " This is also we choose this one because I call this example tossing a fair six sided side. So of course, we need to have come like our uniform distribution here. Otherwise, it wouldn't be a fair die.", "tokens": [639, 307, 611, 321, 2826, 341, 472, 570, 286, 818, 341, 1365, 14432, 278, 257, 3143, 2309, 41651, 1252, 13, 407, 295, 1164, 11, 321, 643, 281, 362, 808, 411, 527, 9452, 7316, 510, 13, 10328, 11, 309, 2759, 380, 312, 257, 3143, 978, 13], "temperature": 0.0, "avg_logprob": -0.14393115043640137, "compression_ratio": 1.5789473684210527, "no_speech_prob": 9.187623618345242e-06}, {"id": 422, "seek": 395300, "start": 3953.0, "end": 3962.0, "text": " Okay, so what is now our random variable because this is defined on top of our probability space. And here we can choose for different random variables.", "tokens": [1033, 11, 370, 437, 307, 586, 527, 4974, 7006, 570, 341, 307, 7642, 322, 1192, 295, 527, 8482, 1901, 13, 400, 510, 321, 393, 2826, 337, 819, 4974, 9102, 13], "temperature": 0.0, "avg_logprob": -0.08007624110237496, "compression_ratio": 1.611764705882353, "no_speech_prob": 1.3176302672945894e-05}, {"id": 423, "seek": 395300, "start": 3962.0, "end": 3969.0, "text": " And the first one that I want to choose here is the random variable, which represents the number that appears on the die.", "tokens": [400, 264, 700, 472, 300, 286, 528, 281, 2826, 510, 307, 264, 4974, 7006, 11, 597, 8855, 264, 1230, 300, 7038, 322, 264, 978, 13], "temperature": 0.0, "avg_logprob": -0.08007624110237496, "compression_ratio": 1.611764705882353, "no_speech_prob": 1.3176302672945894e-05}, {"id": 424, "seek": 396900, "start": 3969.0, "end": 3984.0, "text": " So our x is defined on the omega, so on our sample space and it's going into R. But in this case, it's going in the set one, two, three, four, five, six. So this is the subset of R. Of course, so that's validified.", "tokens": [407, 527, 2031, 307, 7642, 322, 264, 10498, 11, 370, 322, 527, 6889, 1901, 293, 309, 311, 516, 666, 497, 13, 583, 294, 341, 1389, 11, 309, 311, 516, 294, 264, 992, 472, 11, 732, 11, 1045, 11, 1451, 11, 1732, 11, 2309, 13, 407, 341, 307, 264, 25993, 295, 497, 13, 2720, 1164, 11, 370, 300, 311, 7363, 2587, 13], "temperature": 0.0, "avg_logprob": -0.16833660649318322, "compression_ratio": 1.7256637168141593, "no_speech_prob": 1.3392475921136793e-05}, {"id": 425, "seek": 396900, "start": 3984.0, "end": 3996.0, "text": " And as the subset where we're resulting in is discrete because we only have six elements and it's not continuous. We call this random variable, it is discrete random variable.", "tokens": [400, 382, 264, 25993, 689, 321, 434, 16505, 294, 307, 27706, 570, 321, 787, 362, 2309, 4959, 293, 309, 311, 406, 10957, 13, 492, 818, 341, 4974, 7006, 11, 309, 307, 27706, 4974, 7006, 13], "temperature": 0.0, "avg_logprob": -0.16833660649318322, "compression_ratio": 1.7256637168141593, "no_speech_prob": 1.3392475921136793e-05}, {"id": 426, "seek": 399600, "start": 3996.0, "end": 4013.0, "text": " So this is important for you to remember. And so one example here, it's a really simple random random variable here because it's essentially just giving the outcome of our die of our dice.", "tokens": [407, 341, 307, 1021, 337, 291, 281, 1604, 13, 400, 370, 472, 1365, 510, 11, 309, 311, 257, 534, 2199, 4974, 4974, 7006, 510, 570, 309, 311, 4476, 445, 2902, 264, 9700, 295, 527, 978, 295, 527, 10313, 13], "temperature": 0.0, "avg_logprob": -0.1424631296202194, "compression_ratio": 1.413533834586466, "no_speech_prob": 4.4417818571673706e-05}, {"id": 427, "seek": 401300, "start": 4013.0, "end": 4029.0, "text": " So the example now is for one element in omega, for example, we take our omega equals to four, like the small like the sign which is here being equal to w is called the small omega.", "tokens": [407, 264, 1365, 586, 307, 337, 472, 4478, 294, 10498, 11, 337, 1365, 11, 321, 747, 527, 10498, 6915, 281, 1451, 11, 411, 264, 1359, 411, 264, 1465, 597, 307, 510, 885, 2681, 281, 261, 307, 1219, 264, 1359, 10498, 13], "temperature": 0.0, "avg_logprob": -0.17255584928724502, "compression_ratio": 1.5210084033613445, "no_speech_prob": 4.527106284513138e-05}, {"id": 428, "seek": 402900, "start": 4029.0, "end": 4042.0, "text": " And if we choose the small omega one element in our big omega set, we choose it here, for example, to be equal to four. Then x of omega would be four because we just say, okay, it's kind of the identical function.", "tokens": [400, 498, 321, 2826, 264, 1359, 10498, 472, 4478, 294, 527, 955, 10498, 992, 11, 321, 2826, 309, 510, 11, 337, 1365, 11, 281, 312, 2681, 281, 1451, 13, 1396, 2031, 295, 10498, 576, 312, 1451, 570, 321, 445, 584, 11, 1392, 11, 309, 311, 733, 295, 264, 14800, 2445, 13], "temperature": 0.0, "avg_logprob": -0.15576977449304918, "compression_ratio": 1.6116504854368932, "no_speech_prob": 8.45669910631841e-06}, {"id": 429, "seek": 402900, "start": 4042.0, "end": 4050.0, "text": " We look at what our die is showing us, it's showing us the number four. So the outcome of our random variable is four.", "tokens": [492, 574, 412, 437, 527, 978, 307, 4099, 505, 11, 309, 311, 4099, 505, 264, 1230, 1451, 13, 407, 264, 9700, 295, 527, 4974, 7006, 307, 1451, 13], "temperature": 0.0, "avg_logprob": -0.15576977449304918, "compression_ratio": 1.6116504854368932, "no_speech_prob": 8.45669910631841e-06}, {"id": 430, "seek": 405000, "start": 4050.0, "end": 4065.0, "text": " And now we defined in the first line our probability measure. So now we can see p x equals to four. This is something which we always write like p. And then we use the random variable equals to something.", "tokens": [400, 586, 321, 7642, 294, 264, 700, 1622, 527, 8482, 3481, 13, 407, 586, 321, 393, 536, 280, 2031, 6915, 281, 1451, 13, 639, 307, 746, 597, 321, 1009, 2464, 411, 280, 13, 400, 550, 321, 764, 264, 4974, 7006, 6915, 281, 746, 13], "temperature": 0.0, "avg_logprob": -0.14582069714864096, "compression_ratio": 1.4265734265734267, "no_speech_prob": 4.7376888687722385e-05}, {"id": 431, "seek": 406500, "start": 4065.0, "end": 4083.0, "text": " So here we can see p of the probability that x is equals to four is the same as p like the probability. And then we have to look at the set that I defined in there. So all the omigas in our big omega set such that x of our omega equals to four.", "tokens": [407, 510, 321, 393, 536, 280, 295, 264, 8482, 300, 2031, 307, 6915, 281, 1451, 307, 264, 912, 382, 280, 411, 264, 8482, 13, 400, 550, 321, 362, 281, 574, 412, 264, 992, 300, 286, 7642, 294, 456, 13, 407, 439, 264, 3406, 328, 296, 294, 527, 955, 10498, 992, 1270, 300, 2031, 295, 527, 10498, 6915, 281, 1451, 13], "temperature": 0.0, "avg_logprob": -0.09323522448539734, "compression_ratio": 1.6158940397350994, "no_speech_prob": 8.360204446944408e-06}, {"id": 432, "seek": 408300, "start": 4083.0, "end": 4097.0, "text": " This is essentially what x equals to four means. It's just looking at the set and we're looking at all the elements in our sample space omega such that if we put this element in our random variable x, we obtain the number four.", "tokens": [639, 307, 4476, 437, 2031, 6915, 281, 1451, 1355, 13, 467, 311, 445, 1237, 412, 264, 992, 293, 321, 434, 1237, 412, 439, 264, 4959, 294, 527, 6889, 1901, 10498, 1270, 300, 498, 321, 829, 341, 4478, 294, 527, 4974, 7006, 2031, 11, 321, 12701, 264, 1230, 1451, 13], "temperature": 0.0, "avg_logprob": -0.11821450685199938, "compression_ratio": 1.8577777777777778, "no_speech_prob": 4.564255959849106e-06}, {"id": 433, "seek": 408300, "start": 4097.0, "end": 4109.0, "text": " And we know by definition of our random variable that this set only contains the number four. Because when I take the number four and I put it into my random variable, I get the number four.", "tokens": [400, 321, 458, 538, 7123, 295, 527, 4974, 7006, 300, 341, 992, 787, 8306, 264, 1230, 1451, 13, 1436, 562, 286, 747, 264, 1230, 1451, 293, 286, 829, 309, 666, 452, 4974, 7006, 11, 286, 483, 264, 1230, 1451, 13], "temperature": 0.0, "avg_logprob": -0.11821450685199938, "compression_ratio": 1.8577777777777778, "no_speech_prob": 4.564255959849106e-06}, {"id": 434, "seek": 410900, "start": 4109.0, "end": 4123.0, "text": " If I put anything else in it, I don't get the number four. So this is only a containing the number four. So it's the probability of the set which is only containing number four. And then we know that the probability now is one six.", "tokens": [759, 286, 829, 1340, 1646, 294, 309, 11, 286, 500, 380, 483, 264, 1230, 1451, 13, 407, 341, 307, 787, 257, 19273, 264, 1230, 1451, 13, 407, 309, 311, 264, 8482, 295, 264, 992, 597, 307, 787, 19273, 1230, 1451, 13, 400, 550, 321, 458, 300, 264, 8482, 586, 307, 472, 2309, 13], "temperature": 0.0, "avg_logprob": -0.09405535591973199, "compression_ratio": 1.824390243902439, "no_speech_prob": 0.00010803206532727927}, {"id": 435, "seek": 410900, "start": 4123.0, "end": 4135.0, "text": " So I hope it's not too confusing with this notation, but this essentially is what we mean when we write p of the probability x equals to four.", "tokens": [407, 286, 1454, 309, 311, 406, 886, 13181, 365, 341, 24657, 11, 457, 341, 4476, 307, 437, 321, 914, 562, 321, 2464, 280, 295, 264, 8482, 2031, 6915, 281, 1451, 13], "temperature": 0.0, "avg_logprob": -0.09405535591973199, "compression_ratio": 1.824390243902439, "no_speech_prob": 0.00010803206532727927}, {"id": 436, "seek": 413500, "start": 4135.0, "end": 4145.0, "text": " So let us have a look at another discrete example. It's also a simple example. It's flipping a fair coin two times.", "tokens": [407, 718, 505, 362, 257, 574, 412, 1071, 27706, 1365, 13, 467, 311, 611, 257, 2199, 1365, 13, 467, 311, 26886, 257, 3143, 11464, 732, 1413, 13], "temperature": 0.0, "avg_logprob": -0.08658800522486369, "compression_ratio": 1.586021505376344, "no_speech_prob": 6.314121856121346e-05}, {"id": 437, "seek": 413500, "start": 4145.0, "end": 4157.0, "text": " So now again, we have to define our probability space as I said in the beginning. So our omigas now is the set of all possible outcomes of our experiment of our random experiment.", "tokens": [407, 586, 797, 11, 321, 362, 281, 6964, 527, 8482, 1901, 382, 286, 848, 294, 264, 2863, 13, 407, 527, 3406, 328, 296, 586, 307, 264, 992, 295, 439, 1944, 10070, 295, 527, 5120, 295, 527, 4974, 5120, 13], "temperature": 0.0, "avg_logprob": -0.08658800522486369, "compression_ratio": 1.586021505376344, "no_speech_prob": 6.314121856121346e-05}, {"id": 438, "seek": 415700, "start": 4157.0, "end": 4171.0, "text": " And if I flip a coin, I can get either hat or tail. And if I do it twice, I can have like the outcomes that I can have is either I get two times a hat where I get in the first flip a hat and then a tail.", "tokens": [400, 498, 286, 7929, 257, 11464, 11, 286, 393, 483, 2139, 2385, 420, 6838, 13, 400, 498, 286, 360, 309, 6091, 11, 286, 393, 362, 411, 264, 10070, 300, 286, 393, 362, 307, 2139, 286, 483, 732, 1413, 257, 2385, 689, 286, 483, 294, 264, 700, 7929, 257, 2385, 293, 550, 257, 6838, 13], "temperature": 0.0, "avg_logprob": -0.12487899023911049, "compression_ratio": 1.6111111111111112, "no_speech_prob": 1.5995579815353267e-05}, {"id": 439, "seek": 417100, "start": 4171.0, "end": 4188.0, "text": " And the second or I get in my second flip ahead and my first one, a tail. So head tail and tail head are also two elements or I get two times tail. So this is the possible outcomes of our random variable here.", "tokens": [400, 264, 1150, 420, 286, 483, 294, 452, 1150, 7929, 2286, 293, 452, 700, 472, 11, 257, 6838, 13, 407, 1378, 6838, 293, 6838, 1378, 366, 611, 732, 4959, 420, 286, 483, 732, 1413, 6838, 13, 407, 341, 307, 264, 1944, 10070, 295, 527, 4974, 7006, 510, 13], "temperature": 0.0, "avg_logprob": -0.16515520902780387, "compression_ratio": 1.548148148148148, "no_speech_prob": 7.334813290071907e-07}, {"id": 440, "seek": 418800, "start": 4188.0, "end": 4204.0, "text": " Again, we choose for our event space, and p of omega, which is just all possible subsets of our omega, which is always the most simple, which is just the most simple choice for event space f.", "tokens": [3764, 11, 321, 2826, 337, 527, 2280, 1901, 11, 293, 280, 295, 10498, 11, 597, 307, 445, 439, 1944, 2090, 1385, 295, 527, 10498, 11, 597, 307, 1009, 264, 881, 2199, 11, 597, 307, 445, 264, 881, 2199, 3922, 337, 2280, 1901, 283, 13], "temperature": 0.0, "avg_logprob": -0.17293920119603476, "compression_ratio": 1.6324786324786325, "no_speech_prob": 3.685525825858349e-06}, {"id": 441, "seek": 420400, "start": 4204.0, "end": 4220.0, "text": " And again, as I said, flipping a fair coin two times, we need to define, otherwise we wouldn't be a fair coin to be our probability measure to be equal to p of omega equals to one fourth for all possible elements in our sample space omega.", "tokens": [400, 797, 11, 382, 286, 848, 11, 26886, 257, 3143, 11464, 732, 1413, 11, 321, 643, 281, 6964, 11, 5911, 321, 2759, 380, 312, 257, 3143, 11464, 281, 312, 527, 8482, 3481, 281, 312, 2681, 281, 280, 295, 10498, 6915, 281, 472, 6409, 337, 439, 1944, 4959, 294, 527, 6889, 1901, 10498, 13], "temperature": 0.0, "avg_logprob": -0.1276015545948442, "compression_ratio": 1.7428571428571429, "no_speech_prob": 5.548832177737495e-06}, {"id": 442, "seek": 420400, "start": 4220.0, "end": 4230.0, "text": " So for all possible outcomes, I have the same probability, 91 fourth because our sample space omega consists of four elements.", "tokens": [407, 337, 439, 1944, 10070, 11, 286, 362, 264, 912, 8482, 11, 31064, 6409, 570, 527, 6889, 1901, 10498, 14689, 295, 1451, 4959, 13], "temperature": 0.0, "avg_logprob": -0.1276015545948442, "compression_ratio": 1.7428571428571429, "no_speech_prob": 5.548832177737495e-06}, {"id": 443, "seek": 423000, "start": 4230.0, "end": 4239.0, "text": " Okay, so this is the setup for the random experiment, which I call flipping a fair coin two times.", "tokens": [1033, 11, 370, 341, 307, 264, 8657, 337, 264, 4974, 5120, 11, 597, 286, 818, 26886, 257, 3143, 11464, 732, 1413, 13], "temperature": 0.0, "avg_logprob": -0.1491437408159364, "compression_ratio": 1.4090909090909092, "no_speech_prob": 1.368771063425811e-05}, {"id": 444, "seek": 423000, "start": 4239.0, "end": 4247.0, "text": " Now we can define a more complex random variable here our x is now the number of heads that appeared in the two flips.", "tokens": [823, 321, 393, 6964, 257, 544, 3997, 4974, 7006, 510, 527, 2031, 307, 586, 264, 1230, 295, 8050, 300, 8516, 294, 264, 732, 40249, 13], "temperature": 0.0, "avg_logprob": -0.1491437408159364, "compression_ratio": 1.4090909090909092, "no_speech_prob": 1.368771063425811e-05}, {"id": 445, "seek": 424700, "start": 4247.0, "end": 4268.0, "text": " So our x is again function defined on our sample space omega into R, but now like the outcome space is only containing three elements, having zero, one and two because the outcome can be either that I don't have any head, I get one head or in my flips, I get two times ahead.", "tokens": [407, 527, 2031, 307, 797, 2445, 7642, 322, 527, 6889, 1901, 10498, 666, 497, 11, 457, 586, 411, 264, 9700, 1901, 307, 787, 19273, 1045, 4959, 11, 1419, 4018, 11, 472, 293, 732, 570, 264, 9700, 393, 312, 2139, 300, 286, 500, 380, 362, 604, 1378, 11, 286, 483, 472, 1378, 420, 294, 452, 40249, 11, 286, 483, 732, 1413, 2286, 13], "temperature": 0.0, "avg_logprob": -0.15844436125321823, "compression_ratio": 1.5277777777777777, "no_speech_prob": 6.138795015431242e-06}, {"id": 446, "seek": 426800, "start": 4268.0, "end": 4281.0, "text": " So the outcome like the possible outcomes here are zero, one and two. And again, the outcome space or the target space of my random variable is again a discrete set.", "tokens": [407, 264, 9700, 411, 264, 1944, 10070, 510, 366, 4018, 11, 472, 293, 732, 13, 400, 797, 11, 264, 9700, 1901, 420, 264, 3779, 1901, 295, 452, 4974, 7006, 307, 797, 257, 27706, 992, 13], "temperature": 0.0, "avg_logprob": -0.1373255517747667, "compression_ratio": 1.661764705882353, "no_speech_prob": 5.1295523007865995e-05}, {"id": 447, "seek": 426800, "start": 4281.0, "end": 4286.0, "text": " So again, here we are looking at a discrete random variable.", "tokens": [407, 797, 11, 510, 321, 366, 1237, 412, 257, 27706, 4974, 7006, 13], "temperature": 0.0, "avg_logprob": -0.1373255517747667, "compression_ratio": 1.661764705882353, "no_speech_prob": 5.1295523007865995e-05}, {"id": 448, "seek": 428600, "start": 4286.0, "end": 4301.0, "text": " So this is the second example of one, but here the random variable is a little bit more complex than in the one before.", "tokens": [407, 341, 307, 264, 1150, 1365, 295, 472, 11, 457, 510, 264, 4974, 7006, 307, 257, 707, 857, 544, 3997, 813, 294, 264, 472, 949, 13], "temperature": 0.0, "avg_logprob": -0.13418321499879332, "compression_ratio": 1.699530516431925, "no_speech_prob": 2.78717798209982e-05}, {"id": 449, "seek": 428600, "start": 4301.0, "end": 4311.0, "text": " Okay, so now let's have again, let's have a look again at one specific example or outcome in our sample space. And here we choose for an outcome small omega, the element where the first flip gives us a tail and the second one gives us a head.", "tokens": [1033, 11, 370, 586, 718, 311, 362, 797, 11, 718, 311, 362, 257, 574, 797, 412, 472, 2685, 1365, 420, 9700, 294, 527, 6889, 1901, 13, 400, 510, 321, 2826, 337, 364, 9700, 1359, 10498, 11, 264, 4478, 689, 264, 700, 7929, 2709, 505, 257, 6838, 293, 264, 1150, 472, 2709, 505, 257, 1378, 13], "temperature": 0.0, "avg_logprob": -0.13418321499879332, "compression_ratio": 1.699530516431925, "no_speech_prob": 2.78717798209982e-05}, {"id": 450, "seek": 431100, "start": 4311.0, "end": 4323.0, "text": " And then we know if we put this one into our random variable, which is representing the number of heads of our outcome, we know that the outcome of our random variable would be one.", "tokens": [400, 550, 321, 458, 498, 321, 829, 341, 472, 666, 527, 4974, 7006, 11, 597, 307, 13460, 264, 1230, 295, 8050, 295, 527, 9700, 11, 321, 458, 300, 264, 9700, 295, 527, 4974, 7006, 576, 312, 472, 13], "temperature": 0.0, "avg_logprob": -0.0841054537079551, "compression_ratio": 1.6288209606986899, "no_speech_prob": 2.7681726351147518e-05}, {"id": 451, "seek": 431100, "start": 4323.0, "end": 4335.0, "text": " So this is more complex and here you can also have a look at our probability measure that we defined up there. And again, I want to talk about what does it mean to write p of x equals to one.", "tokens": [407, 341, 307, 544, 3997, 293, 510, 291, 393, 611, 362, 257, 574, 412, 527, 8482, 3481, 300, 321, 7642, 493, 456, 13, 400, 797, 11, 286, 528, 281, 751, 466, 437, 775, 309, 914, 281, 2464, 280, 295, 2031, 6915, 281, 472, 13], "temperature": 0.0, "avg_logprob": -0.0841054537079551, "compression_ratio": 1.6288209606986899, "no_speech_prob": 2.7681726351147518e-05}, {"id": 452, "seek": 433500, "start": 4335.0, "end": 4349.0, "text": " And here again, x equals to one is just a description for a set. And the set here is the set of all omigas in our sample space omega such that when I put this omega in my random variable, I get the number one.", "tokens": [400, 510, 797, 11, 2031, 6915, 281, 472, 307, 445, 257, 3855, 337, 257, 992, 13, 400, 264, 992, 510, 307, 264, 992, 295, 439, 3406, 328, 296, 294, 527, 6889, 1901, 10498, 1270, 300, 562, 286, 829, 341, 10498, 294, 452, 4974, 7006, 11, 286, 483, 264, 1230, 472, 13], "temperature": 0.0, "avg_logprob": -0.1040375449440696, "compression_ratio": 1.4315068493150684, "no_speech_prob": 8.316099410876632e-06}, {"id": 453, "seek": 434900, "start": 4349.0, "end": 4370.0, "text": " So I have to look in all possible outcomes in my sample space and look which one result in my random variable to the number one. And this is definitely just two of the outcomes namely the one where the first flip gives us a head and the second one a tail or the other way around the first flip gives us a tail and the second flip ahead.", "tokens": [407, 286, 362, 281, 574, 294, 439, 1944, 10070, 294, 452, 6889, 1901, 293, 574, 597, 472, 1874, 294, 452, 4974, 7006, 281, 264, 1230, 472, 13, 400, 341, 307, 2138, 445, 732, 295, 264, 10070, 20926, 264, 472, 689, 264, 700, 7929, 2709, 505, 257, 1378, 293, 264, 1150, 472, 257, 6838, 420, 264, 661, 636, 926, 264, 700, 7929, 2709, 505, 257, 6838, 293, 264, 1150, 7929, 2286, 13], "temperature": 0.0, "avg_logprob": -0.11655980428059896, "compression_ratio": 1.7409326424870466, "no_speech_prob": 1.8732370108409668e-06}, {"id": 454, "seek": 437000, "start": 4370.0, "end": 4388.0, "text": " Because then always I just have one head and the other one is a tail. So I have here in this set, which I described in the second term, I can just define it as the set, including the elements h and t and the two and h.", "tokens": [1436, 550, 1009, 286, 445, 362, 472, 1378, 293, 264, 661, 472, 307, 257, 6838, 13, 407, 286, 362, 510, 294, 341, 992, 11, 597, 286, 7619, 294, 264, 1150, 1433, 11, 286, 393, 445, 6964, 309, 382, 264, 992, 11, 3009, 264, 4959, 276, 293, 256, 293, 264, 732, 293, 276, 13], "temperature": 0.0, "avg_logprob": -0.25865079149787806, "compression_ratio": 1.687830687830688, "no_speech_prob": 4.8568872443865985e-05}, {"id": 455, "seek": 437000, "start": 4388.0, "end": 4397.0, "text": " So this is two of the four to the probability here as we have a uniform probability is now one half.", "tokens": [407, 341, 307, 732, 295, 264, 1451, 281, 264, 8482, 510, 382, 321, 362, 257, 9452, 8482, 307, 586, 472, 1922, 13], "temperature": 0.0, "avg_logprob": -0.25865079149787806, "compression_ratio": 1.687830687830688, "no_speech_prob": 4.8568872443865985e-05}, {"id": 456, "seek": 439700, "start": 4397.0, "end": 4408.0, "text": " Okay, so this is another example. I really quick want to look at also a continuous example. So let us have a look at radioactive decay.", "tokens": [1033, 11, 370, 341, 307, 1071, 1365, 13, 286, 534, 1702, 528, 281, 574, 412, 611, 257, 10957, 1365, 13, 407, 718, 505, 362, 257, 574, 412, 35844, 21039, 13], "temperature": 0.0, "avg_logprob": -0.11916213650857249, "compression_ratio": 1.6689655172413793, "no_speech_prob": 0.00010274995293002576}, {"id": 457, "seek": 439700, "start": 4408.0, "end": 4415.0, "text": " So when we want to have a look at a particle, which is radioactive and we want to look at the decay of it.", "tokens": [407, 562, 321, 528, 281, 362, 257, 574, 412, 257, 12359, 11, 597, 307, 35844, 293, 321, 528, 281, 574, 412, 264, 21039, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.11916213650857249, "compression_ratio": 1.6689655172413793, "no_speech_prob": 0.00010274995293002576}, {"id": 458, "seek": 441500, "start": 4415.0, "end": 4429.0, "text": " And the underlying experiment that we are looking at is a set omega, which is now defined to be the real valued or the set of all real values, greater or equal to zero.", "tokens": [400, 264, 14217, 5120, 300, 321, 366, 1237, 412, 307, 257, 992, 10498, 11, 597, 307, 586, 7642, 281, 312, 264, 957, 22608, 420, 264, 992, 295, 439, 957, 4190, 11, 5044, 420, 2681, 281, 4018, 13], "temperature": 0.0, "avg_logprob": -0.14339689774946732, "compression_ratio": 1.6559139784946237, "no_speech_prob": 2.933023642981425e-05}, {"id": 459, "seek": 441500, "start": 4429.0, "end": 4439.0, "text": " So the positive part of the real values. And again, we choose our event space f to be the p of omega. So all possible subsets of our omega.", "tokens": [407, 264, 3353, 644, 295, 264, 957, 4190, 13, 400, 797, 11, 321, 2826, 527, 2280, 1901, 283, 281, 312, 264, 280, 295, 10498, 13, 407, 439, 1944, 2090, 1385, 295, 527, 10498, 13], "temperature": 0.0, "avg_logprob": -0.14339689774946732, "compression_ratio": 1.6559139784946237, "no_speech_prob": 2.933023642981425e-05}, {"id": 460, "seek": 443900, "start": 4439.0, "end": 4453.0, "text": " And we choose in general a probability measure p. This is much more complicated to define the probability measure here. So I don't do it by hand, but there are possible choices for probability measures.", "tokens": [400, 321, 2826, 294, 2674, 257, 8482, 3481, 280, 13, 639, 307, 709, 544, 6179, 281, 6964, 264, 8482, 3481, 510, 13, 407, 286, 500, 380, 360, 309, 538, 1011, 11, 457, 456, 366, 1944, 7994, 337, 8482, 8000, 13], "temperature": 0.0, "avg_logprob": -0.10591025048113883, "compression_ratio": 1.756198347107438, "no_speech_prob": 3.2590927730780095e-05}, {"id": 461, "seek": 443900, "start": 4453.0, "end": 4468.0, "text": " Okay, so now we want to have a look at the continuous random variable. So let now let's now have a look at the random variable here X, which is indicating the amount of time that it takes for radioactive particle to decay.", "tokens": [1033, 11, 370, 586, 321, 528, 281, 362, 257, 574, 412, 264, 10957, 4974, 7006, 13, 407, 718, 586, 718, 311, 586, 362, 257, 574, 412, 264, 4974, 7006, 510, 1783, 11, 597, 307, 25604, 264, 2372, 295, 565, 300, 309, 2516, 337, 35844, 12359, 281, 21039, 13], "temperature": 0.0, "avg_logprob": -0.10591025048113883, "compression_ratio": 1.756198347107438, "no_speech_prob": 3.2590927730780095e-05}, {"id": 462, "seek": 446800, "start": 4468.0, "end": 4478.0, "text": " And so the definition of our random variable is now a function X, which is going from R greater or equal to zero into R greater or equal to zero.", "tokens": [400, 370, 264, 7123, 295, 527, 4974, 7006, 307, 586, 257, 2445, 1783, 11, 597, 307, 516, 490, 497, 5044, 420, 2681, 281, 4018, 666, 497, 5044, 420, 2681, 281, 4018, 13], "temperature": 0.0, "avg_logprob": -0.09835699159805089, "compression_ratio": 1.7374301675977655, "no_speech_prob": 1.1349709893693216e-05}, {"id": 463, "seek": 446800, "start": 4478.0, "end": 4489.0, "text": " Because the outcome only can be positive because we're talking about time. And the input is our omega. And this is also our definition of R greater or equal to zero.", "tokens": [1436, 264, 9700, 787, 393, 312, 3353, 570, 321, 434, 1417, 466, 565, 13, 400, 264, 4846, 307, 527, 10498, 13, 400, 341, 307, 611, 527, 7123, 295, 497, 5044, 420, 2681, 281, 4018, 13], "temperature": 0.0, "avg_logprob": -0.09835699159805089, "compression_ratio": 1.7374301675977655, "no_speech_prob": 1.1349709893693216e-05}, {"id": 464, "seek": 448900, "start": 4489.0, "end": 4501.0, "text": " And this one is now as you see that the outcome space of our X is a continuous set because it's like the real values all greater or equal to zero. We have a continuous random variable.", "tokens": [400, 341, 472, 307, 586, 382, 291, 536, 300, 264, 9700, 1901, 295, 527, 1783, 307, 257, 10957, 992, 570, 309, 311, 411, 264, 957, 4190, 439, 5044, 420, 2681, 281, 4018, 13, 492, 362, 257, 10957, 4974, 7006, 13], "temperature": 0.0, "avg_logprob": -0.10632686373553699, "compression_ratio": 1.5660377358490567, "no_speech_prob": 6.024845333740814e-06}, {"id": 465, "seek": 448900, "start": 4501.0, "end": 4510.0, "text": " Okay, so this is one example here. The probability measure is now defined on the set of events f. And it's now used for random variable as follows.", "tokens": [1033, 11, 370, 341, 307, 472, 1365, 510, 13, 440, 8482, 3481, 307, 586, 7642, 322, 264, 992, 295, 3931, 283, 13, 400, 309, 311, 586, 1143, 337, 4974, 7006, 382, 10002, 13], "temperature": 0.0, "avg_logprob": -0.10632686373553699, "compression_ratio": 1.5660377358490567, "no_speech_prob": 6.024845333740814e-06}, {"id": 466, "seek": 451000, "start": 4510.0, "end": 4522.0, "text": " So again, we have this usage of our probability measure in relation with our X. So we can say what is the probability of our random variable being between a and b.", "tokens": [407, 797, 11, 321, 362, 341, 14924, 295, 527, 8482, 3481, 294, 9721, 365, 527, 1783, 13, 407, 321, 393, 584, 437, 307, 264, 8482, 295, 527, 4974, 7006, 885, 1296, 257, 293, 272, 13], "temperature": 0.0, "avg_logprob": -0.1315164686758307, "compression_ratio": 1.683673469387755, "no_speech_prob": 1.1843289030366577e-05}, {"id": 467, "seek": 451000, "start": 4522.0, "end": 4536.0, "text": " And this again is just looking at our sample space omega big omega and it's taking all the small omigas in the sample space for which X of omega lies between a and b.", "tokens": [400, 341, 797, 307, 445, 1237, 412, 527, 6889, 1901, 10498, 955, 10498, 293, 309, 311, 1940, 439, 264, 1359, 3406, 328, 296, 294, 264, 6889, 1901, 337, 597, 1783, 295, 10498, 9134, 1296, 257, 293, 272, 13], "temperature": 0.0, "avg_logprob": -0.1315164686758307, "compression_ratio": 1.683673469387755, "no_speech_prob": 1.1843289030366577e-05}, {"id": 468, "seek": 453600, "start": 4536.0, "end": 4546.0, "text": " And this is just giving us the probability of this set. And then it's depending on how we defined our probability measure depends on what this gives us.", "tokens": [400, 341, 307, 445, 2902, 505, 264, 8482, 295, 341, 992, 13, 400, 550, 309, 311, 5413, 322, 577, 321, 7642, 527, 8482, 3481, 5946, 322, 437, 341, 2709, 505, 13], "temperature": 0.0, "avg_logprob": -0.0912823918499524, "compression_ratio": 1.6278026905829597, "no_speech_prob": 3.51883027178701e-05}, {"id": 469, "seek": 453600, "start": 4546.0, "end": 4559.0, "text": " Okay, so this is an example of a continuous setting. And I hope that you better understood like when we talk about a random variable that we always have this underlying probability space on top or on the basis.", "tokens": [1033, 11, 370, 341, 307, 364, 1365, 295, 257, 10957, 3287, 13, 400, 286, 1454, 300, 291, 1101, 7320, 411, 562, 321, 751, 466, 257, 4974, 7006, 300, 321, 1009, 362, 341, 14217, 8482, 1901, 322, 1192, 420, 322, 264, 5143, 13], "temperature": 0.0, "avg_logprob": -0.0912823918499524, "compression_ratio": 1.6278026905829597, "no_speech_prob": 3.51883027178701e-05}, {"id": 470, "seek": 455900, "start": 4559.0, "end": 4566.0, "text": " The probability or the random variable is just defined on top of such an random experiment.", "tokens": [440, 8482, 420, 264, 4974, 7006, 307, 445, 7642, 322, 1192, 295, 1270, 364, 4974, 5120, 13], "temperature": 0.0, "avg_logprob": -0.08824975906856476, "compression_ratio": 1.7752808988764044, "no_speech_prob": 4.690205241786316e-05}, {"id": 471, "seek": 455900, "start": 4566.0, "end": 4578.0, "text": " Okay, after having defined the most important basic concepts of probability theory, which are including the probability space and the random variable, we now want to have a look at probability measure of the random variable.", "tokens": [1033, 11, 934, 1419, 7642, 264, 881, 1021, 3875, 10392, 295, 8482, 5261, 11, 597, 366, 3009, 264, 8482, 1901, 293, 264, 4974, 7006, 11, 321, 586, 528, 281, 362, 257, 574, 412, 8482, 3481, 295, 264, 4974, 7006, 13], "temperature": 0.0, "avg_logprob": -0.08824975906856476, "compression_ratio": 1.7752808988764044, "no_speech_prob": 4.690205241786316e-05}, {"id": 472, "seek": 457800, "start": 4578.0, "end": 4592.0, "text": " And we can specify the probability measure of a random variable with alternative functions. And we call them here cumulative density function CDF, probability mass function PMF and the probability density function.", "tokens": [400, 321, 393, 16500, 264, 8482, 3481, 295, 257, 4974, 7006, 365, 8535, 6828, 13, 400, 321, 818, 552, 510, 38379, 10305, 2445, 6743, 37, 11, 8482, 2758, 2445, 12499, 37, 293, 264, 8482, 10305, 2445, 13], "temperature": 0.0, "avg_logprob": -0.1577957931317781, "compression_ratio": 1.8177777777777777, "no_speech_prob": 7.887266292527784e-06}, {"id": 473, "seek": 457800, "start": 4592.0, "end": 4604.0, "text": " And essentially, we have to make a difference between discrete and continuous random variables. And I included a short overview over these function, which are describing our probability measure.", "tokens": [400, 4476, 11, 321, 362, 281, 652, 257, 2649, 1296, 27706, 293, 10957, 4974, 9102, 13, 400, 286, 5556, 257, 2099, 12492, 670, 613, 2445, 11, 597, 366, 16141, 527, 8482, 3481, 13], "temperature": 0.0, "avg_logprob": -0.1577957931317781, "compression_ratio": 1.8177777777777777, "no_speech_prob": 7.887266292527784e-06}, {"id": 474, "seek": 460400, "start": 4604.0, "end": 4615.0, "text": " And the first column or second column is giving us the cumulative density function. This is something which is the same for both discrete and continuous random variable.", "tokens": [400, 264, 700, 7738, 420, 1150, 7738, 307, 2902, 505, 264, 38379, 10305, 2445, 13, 639, 307, 746, 597, 307, 264, 912, 337, 1293, 27706, 293, 10957, 4974, 7006, 13], "temperature": 0.0, "avg_logprob": -0.11200641680367385, "compression_ratio": 1.751219512195122, "no_speech_prob": 3.99278687837068e-05}, {"id": 475, "seek": 460400, "start": 4615.0, "end": 4627.0, "text": " And this is a function which is denoted by a big F with a subscript x for our random variable x. And it's giving us the probability that our random variable is smaller equal to the value x.", "tokens": [400, 341, 307, 257, 2445, 597, 307, 1441, 23325, 538, 257, 955, 479, 365, 257, 2325, 662, 2031, 337, 527, 4974, 7006, 2031, 13, 400, 309, 311, 2902, 505, 264, 8482, 300, 527, 4974, 7006, 307, 4356, 2681, 281, 264, 2158, 2031, 13], "temperature": 0.0, "avg_logprob": -0.11200641680367385, "compression_ratio": 1.751219512195122, "no_speech_prob": 3.99278687837068e-05}, {"id": 476, "seek": 462700, "start": 4627.0, "end": 4634.0, "text": " And this can be defined as I said before for discrete and continuous probability or random variables.", "tokens": [400, 341, 393, 312, 7642, 382, 286, 848, 949, 337, 27706, 293, 10957, 8482, 420, 4974, 9102, 13], "temperature": 0.0, "avg_logprob": -0.06853276345787979, "compression_ratio": 1.7880184331797235, "no_speech_prob": 1.4800362805544864e-05}, {"id": 477, "seek": 462700, "start": 4634.0, "end": 4643.0, "text": " The third column is now what makes the difference between a discrete and a continuous case. In the discrete case, you can define the probability mass function.", "tokens": [440, 2636, 7738, 307, 586, 437, 1669, 264, 2649, 1296, 257, 27706, 293, 257, 10957, 1389, 13, 682, 264, 27706, 1389, 11, 291, 393, 6964, 264, 8482, 2758, 2445, 13], "temperature": 0.0, "avg_logprob": -0.06853276345787979, "compression_ratio": 1.7880184331797235, "no_speech_prob": 1.4800362805544864e-05}, {"id": 478, "seek": 462700, "start": 4643.0, "end": 4651.0, "text": " This is now giving the probability that our random variable is taking the value x. So it's just looking at one specific value.", "tokens": [639, 307, 586, 2902, 264, 8482, 300, 527, 4974, 7006, 307, 1940, 264, 2158, 2031, 13, 407, 309, 311, 445, 1237, 412, 472, 2685, 2158, 13], "temperature": 0.0, "avg_logprob": -0.06853276345787979, "compression_ratio": 1.7880184331797235, "no_speech_prob": 1.4800362805544864e-05}, {"id": 479, "seek": 465100, "start": 4651.0, "end": 4658.0, "text": " This is not really possible for continuous case. So here we define in the continuous case the probability density function.", "tokens": [639, 307, 406, 534, 1944, 337, 10957, 1389, 13, 407, 510, 321, 6964, 294, 264, 10957, 1389, 264, 8482, 10305, 2445, 13], "temperature": 0.0, "avg_logprob": -0.061409926995998475, "compression_ratio": 1.979899497487437, "no_speech_prob": 2.6359375624451786e-05}, {"id": 480, "seek": 465100, "start": 4658.0, "end": 4666.0, "text": " And the probability density function is essentially just the derivative of our cumulative density function in the continuous case.", "tokens": [400, 264, 8482, 10305, 2445, 307, 4476, 445, 264, 13760, 295, 527, 38379, 10305, 2445, 294, 264, 10957, 1389, 13], "temperature": 0.0, "avg_logprob": -0.061409926995998475, "compression_ratio": 1.979899497487437, "no_speech_prob": 2.6359375624451786e-05}, {"id": 481, "seek": 465100, "start": 4666.0, "end": 4676.0, "text": " In the continuous case, most of the time the CDF is differentiable, which makes it possible to define our PDF as the derivative of our CDF.", "tokens": [682, 264, 10957, 1389, 11, 881, 295, 264, 565, 264, 6743, 37, 307, 819, 9364, 11, 597, 1669, 309, 1944, 281, 6964, 527, 17752, 382, 264, 13760, 295, 527, 6743, 37, 13], "temperature": 0.0, "avg_logprob": -0.061409926995998475, "compression_ratio": 1.979899497487437, "no_speech_prob": 2.6359375624451786e-05}, {"id": 482, "seek": 467600, "start": 4676.0, "end": 4685.0, "text": " Okay, so let's just shortly review the definition of these functions. I said the cumulative distribution function is for both settings the same.", "tokens": [1033, 11, 370, 718, 311, 445, 13392, 3131, 264, 7123, 295, 613, 6828, 13, 286, 848, 264, 38379, 7316, 2445, 307, 337, 1293, 6257, 264, 912, 13], "temperature": 0.0, "avg_logprob": -0.10232494450822661, "compression_ratio": 1.6118721461187215, "no_speech_prob": 4.005918526672758e-05}, {"id": 483, "seek": 467600, "start": 4685.0, "end": 4700.0, "text": " And it's essentially a function F of x which is defined from R into the interval between 0 and 1. And it's defined to be the probability that our random variable is smaller or equal to the value of x small x.", "tokens": [400, 309, 311, 4476, 257, 2445, 479, 295, 2031, 597, 307, 7642, 490, 497, 666, 264, 15035, 1296, 1958, 293, 502, 13, 400, 309, 311, 7642, 281, 312, 264, 8482, 300, 527, 4974, 7006, 307, 4356, 420, 2681, 281, 264, 2158, 295, 2031, 1359, 2031, 13], "temperature": 0.0, "avg_logprob": -0.10232494450822661, "compression_ratio": 1.6118721461187215, "no_speech_prob": 4.005918526672758e-05}, {"id": 484, "seek": 470000, "start": 4700.0, "end": 4711.0, "text": " And it also has four properties. I won't read through all of them, but you can just have a look. But essentially it just means that it always have to end in one.", "tokens": [400, 309, 611, 575, 1451, 7221, 13, 286, 1582, 380, 1401, 807, 439, 295, 552, 11, 457, 291, 393, 445, 362, 257, 574, 13, 583, 4476, 309, 445, 1355, 300, 309, 1009, 362, 281, 917, 294, 472, 13], "temperature": 0.0, "avg_logprob": -0.11576735973358154, "compression_ratio": 1.5194174757281553, "no_speech_prob": 4.7021821956150234e-05}, {"id": 485, "seek": 470000, "start": 4711.0, "end": 4717.0, "text": " This is the biggest possible value that it can take and it starts at 0.", "tokens": [639, 307, 264, 3880, 1944, 2158, 300, 309, 393, 747, 293, 309, 3719, 412, 1958, 13], "temperature": 0.0, "avg_logprob": -0.11576735973358154, "compression_ratio": 1.5194174757281553, "no_speech_prob": 4.7021821956150234e-05}, {"id": 486, "seek": 470000, "start": 4717.0, "end": 4719.0, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.11576735973358154, "compression_ratio": 1.5194174757281553, "no_speech_prob": 4.7021821956150234e-05}, {"id": 487, "seek": 470000, "start": 4719.0, "end": 4727.0, "text": " So there you also have an image of a possible sample cumulative function.", "tokens": [407, 456, 291, 611, 362, 364, 3256, 295, 257, 1944, 6889, 38379, 2445, 13], "temperature": 0.0, "avg_logprob": -0.11576735973358154, "compression_ratio": 1.5194174757281553, "no_speech_prob": 4.7021821956150234e-05}, {"id": 488, "seek": 472700, "start": 4727.0, "end": 4737.0, "text": " In the discrete case, I said that we can have a look at the probability mass function. And here the probability mass function is denoted by a small p with a subscript x for the random variable x.", "tokens": [682, 264, 27706, 1389, 11, 286, 848, 300, 321, 393, 362, 257, 574, 412, 264, 8482, 2758, 2445, 13, 400, 510, 264, 8482, 2758, 2445, 307, 1441, 23325, 538, 257, 1359, 280, 365, 257, 2325, 662, 2031, 337, 264, 4974, 7006, 2031, 13], "temperature": 0.0, "avg_logprob": -0.08335752165719365, "compression_ratio": 1.821256038647343, "no_speech_prob": 6.386436143657193e-05}, {"id": 489, "seek": 472700, "start": 4737.0, "end": 4750.0, "text": " And it is defined to go from the sample space into R. And I said before it's just giving us the probability that our random variable in the discrete case is equal to the value of x.", "tokens": [400, 309, 307, 7642, 281, 352, 490, 264, 6889, 1901, 666, 497, 13, 400, 286, 848, 949, 309, 311, 445, 2902, 505, 264, 8482, 300, 527, 4974, 7006, 294, 264, 27706, 1389, 307, 2681, 281, 264, 2158, 295, 2031, 13], "temperature": 0.0, "avg_logprob": -0.08335752165719365, "compression_ratio": 1.821256038647343, "no_speech_prob": 6.386436143657193e-05}, {"id": 490, "seek": 475000, "start": 4750.0, "end": 4767.0, "text": " And again, we have some properties which I just derived here. It has to be between 0 and 1 which is natural because it just representing a probability and it also has to sum up up to 1.", "tokens": [400, 797, 11, 321, 362, 512, 7221, 597, 286, 445, 18949, 510, 13, 467, 575, 281, 312, 1296, 1958, 293, 502, 597, 307, 3303, 570, 309, 445, 13460, 257, 8482, 293, 309, 611, 575, 281, 2408, 493, 493, 281, 502, 13], "temperature": 0.0, "avg_logprob": -0.12764181560940213, "compression_ratio": 1.3805970149253732, "no_speech_prob": 3.390700294403359e-05}, {"id": 491, "seek": 476700, "start": 4767.0, "end": 4792.0, "text": " Okay. Now we have one example of a PMF and a CDF. So this is the discrete case if we want to have a look at the sum of two dices. So you see that the cumulative density function is summing up up to 1 and the probability mass function is essentially giving you the probability for all of the possible outcomes which are here ranging between 2 and 12.", "tokens": [1033, 13, 823, 321, 362, 472, 1365, 295, 257, 12499, 37, 293, 257, 6743, 37, 13, 407, 341, 307, 264, 27706, 1389, 498, 321, 528, 281, 362, 257, 574, 412, 264, 2408, 295, 732, 274, 1473, 13, 407, 291, 536, 300, 264, 38379, 10305, 2445, 307, 2408, 2810, 493, 493, 281, 502, 293, 264, 8482, 2758, 2445, 307, 4476, 2902, 291, 264, 8482, 337, 439, 295, 264, 1944, 10070, 597, 366, 510, 25532, 1296, 568, 293, 2272, 13], "temperature": 0.0, "avg_logprob": -0.11734592623826934, "compression_ratio": 1.5863636363636364, "no_speech_prob": 1.240636902366532e-05}, {"id": 492, "seek": 479200, "start": 4792.0, "end": 4805.0, "text": " Okay. And in the continuous case, I said that we're going to have a look or we have the probability density function which is now defined as the derivative of our cumulative distribution function f of x.", "tokens": [1033, 13, 400, 294, 264, 10957, 1389, 11, 286, 848, 300, 321, 434, 516, 281, 362, 257, 574, 420, 321, 362, 264, 8482, 10305, 2445, 597, 307, 586, 7642, 382, 264, 13760, 295, 527, 38379, 7316, 2445, 283, 295, 2031, 13], "temperature": 0.0, "avg_logprob": -0.07833173102939252, "compression_ratio": 1.8158995815899581, "no_speech_prob": 8.455084753222764e-05}, {"id": 493, "seek": 479200, "start": 4805.0, "end": 4820.0, "text": " So it's essentially just the definition here. It's given by a small f with a subscript x for the random variable x. And it's going from omega into R. And it's just defined as the derivative of our cumulative distribution function.", "tokens": [407, 309, 311, 4476, 445, 264, 7123, 510, 13, 467, 311, 2212, 538, 257, 1359, 283, 365, 257, 2325, 662, 2031, 337, 264, 4974, 7006, 2031, 13, 400, 309, 311, 516, 490, 10498, 666, 497, 13, 400, 309, 311, 445, 7642, 382, 264, 13760, 295, 527, 38379, 7316, 2445, 13], "temperature": 0.0, "avg_logprob": -0.07833173102939252, "compression_ratio": 1.8158995815899581, "no_speech_prob": 8.455084753222764e-05}, {"id": 494, "seek": 482000, "start": 4820.0, "end": 4840.0, "text": " Again, we have like very nice properties and I also included a figure so that you can better have a look at this one. So this is describing the continuous case. And it's important to mention here that the value of a PDF so probability density function at any given point x is not the probability of that event.", "tokens": [3764, 11, 321, 362, 411, 588, 1481, 7221, 293, 286, 611, 5556, 257, 2573, 370, 300, 291, 393, 1101, 362, 257, 574, 412, 341, 472, 13, 407, 341, 307, 16141, 264, 10957, 1389, 13, 400, 309, 311, 1021, 281, 2152, 510, 300, 264, 2158, 295, 257, 17752, 370, 8482, 10305, 2445, 412, 604, 2212, 935, 2031, 307, 406, 264, 8482, 295, 300, 2280, 13], "temperature": 0.0, "avg_logprob": -0.10455320161931655, "compression_ratio": 1.5048543689320388, "no_speech_prob": 2.299428524565883e-05}, {"id": 495, "seek": 484000, "start": 4840.0, "end": 4857.0, "text": " Because here we are in the continuous case and that makes it more more difficult. So we can't really describe it as the probability of the input value, but it's more kind of the space under the graph of giving you the probability.", "tokens": [1436, 510, 321, 366, 294, 264, 10957, 1389, 293, 300, 1669, 309, 544, 544, 2252, 13, 407, 321, 393, 380, 534, 6786, 309, 382, 264, 8482, 295, 264, 4846, 2158, 11, 457, 309, 311, 544, 733, 295, 264, 1901, 833, 264, 4295, 295, 2902, 291, 264, 8482, 13], "temperature": 0.0, "avg_logprob": -0.11368881372305063, "compression_ratio": 1.5904255319148937, "no_speech_prob": 1.537512980576139e-05}, {"id": 496, "seek": 484000, "start": 4857.0, "end": 4861.0, "text": " So this is the relationship to the cumulative distribution function.", "tokens": [407, 341, 307, 264, 2480, 281, 264, 38379, 7316, 2445, 13], "temperature": 0.0, "avg_logprob": -0.11368881372305063, "compression_ratio": 1.5904255319148937, "no_speech_prob": 1.537512980576139e-05}, {"id": 497, "seek": 486100, "start": 4861.0, "end": 4878.0, "text": " Okay, so now finally, let's have a look at the concept of expectation and variance of a random variable. So let us start with the expectation. So the idea of the expectation is that it's given or it's defining a weighted average of the values that the random variable can take on.", "tokens": [1033, 11, 370, 586, 2721, 11, 718, 311, 362, 257, 574, 412, 264, 3410, 295, 14334, 293, 21977, 295, 257, 4974, 7006, 13, 407, 718, 505, 722, 365, 264, 14334, 13, 407, 264, 1558, 295, 264, 14334, 307, 300, 309, 311, 2212, 420, 309, 311, 17827, 257, 32807, 4274, 295, 264, 4190, 300, 264, 4974, 7006, 393, 747, 322, 13], "temperature": 0.0, "avg_logprob": -0.07482913784358812, "compression_ratio": 1.7872340425531914, "no_speech_prob": 4.425540828378871e-05}, {"id": 498, "seek": 486100, "start": 4878.0, "end": 4887.0, "text": " And the weights that we are using here are just given by the probability measure function, which is corresponding to our random variable x.", "tokens": [400, 264, 17443, 300, 321, 366, 1228, 510, 366, 445, 2212, 538, 264, 8482, 3481, 2445, 11, 597, 307, 11760, 281, 527, 4974, 7006, 2031, 13], "temperature": 0.0, "avg_logprob": -0.07482913784358812, "compression_ratio": 1.7872340425531914, "no_speech_prob": 4.425540828378871e-05}, {"id": 499, "seek": 488700, "start": 4887.0, "end": 4898.0, "text": " So I said before that we always have to make a difference between the discrete and continuous setting. This is the same here for our expectation. So first of all, we want to look at the definition in the discrete case.", "tokens": [407, 286, 848, 949, 300, 321, 1009, 362, 281, 652, 257, 2649, 1296, 264, 27706, 293, 10957, 3287, 13, 639, 307, 264, 912, 510, 337, 527, 14334, 13, 407, 700, 295, 439, 11, 321, 528, 281, 574, 412, 264, 7123, 294, 264, 27706, 1389, 13], "temperature": 0.0, "avg_logprob": -0.0895487285795666, "compression_ratio": 1.6802973977695168, "no_speech_prob": 5.9369202062953264e-05}, {"id": 500, "seek": 488700, "start": 4898.0, "end": 4915.0, "text": " So let us assume that x is a discrete random variable and we have a probability mass function p of x. And now the expectation of x is defined as e from x, e of x, which is equal to the sum over all elements in our sample space omega.", "tokens": [407, 718, 505, 6552, 300, 2031, 307, 257, 27706, 4974, 7006, 293, 321, 362, 257, 8482, 2758, 2445, 280, 295, 2031, 13, 400, 586, 264, 14334, 295, 2031, 307, 7642, 382, 308, 490, 2031, 11, 308, 295, 2031, 11, 597, 307, 2681, 281, 264, 2408, 670, 439, 4959, 294, 527, 6889, 1901, 10498, 13], "temperature": 0.0, "avg_logprob": -0.0895487285795666, "compression_ratio": 1.6802973977695168, "no_speech_prob": 5.9369202062953264e-05}, {"id": 501, "seek": 491500, "start": 4915.0, "end": 4931.0, "text": " And over the function x times the probability mass function of x of p x of x. So here you can easily see this is the weighted average of all our elements in our sample space and the weights are given by our probability mass function.", "tokens": [400, 670, 264, 2445, 2031, 1413, 264, 8482, 2758, 2445, 295, 2031, 295, 280, 2031, 295, 2031, 13, 407, 510, 291, 393, 3612, 536, 341, 307, 264, 32807, 4274, 295, 439, 527, 4959, 294, 527, 6889, 1901, 293, 264, 17443, 366, 2212, 538, 527, 8482, 2758, 2445, 13], "temperature": 0.0, "avg_logprob": -0.11120538528148945, "compression_ratio": 1.6293706293706294, "no_speech_prob": 1.4749326510354877e-05}, {"id": 502, "seek": 493100, "start": 4931.0, "end": 4946.0, "text": " So similar in the continuous setting, the definition is quite the same. So here we assume that x is now continuous random variable and then instead of a probability mass function, we have our probability distribution function f of x.", "tokens": [407, 2531, 294, 264, 10957, 3287, 11, 264, 7123, 307, 1596, 264, 912, 13, 407, 510, 321, 6552, 300, 2031, 307, 586, 10957, 4974, 7006, 293, 550, 2602, 295, 257, 8482, 2758, 2445, 11, 321, 362, 527, 8482, 7316, 2445, 283, 295, 2031, 13], "temperature": 0.0, "avg_logprob": -0.10370230674743652, "compression_ratio": 1.563758389261745, "no_speech_prob": 8.548484402126633e-06}, {"id": 503, "seek": 494600, "start": 4946.0, "end": 4962.0, "text": " And now our expectation is defined by an integral instead of a sum. So e of x equals the integral from minus infinity to infinity over the function x times f of x where f of x is our probability distribution function.", "tokens": [400, 586, 527, 14334, 307, 7642, 538, 364, 11573, 2602, 295, 257, 2408, 13, 407, 308, 295, 2031, 6915, 264, 11573, 490, 3175, 13202, 281, 13202, 670, 264, 2445, 2031, 1413, 283, 295, 2031, 689, 283, 295, 2031, 307, 527, 8482, 7316, 2445, 13], "temperature": 0.0, "avg_logprob": -0.0718017319838206, "compression_ratio": 1.5390070921985815, "no_speech_prob": 2.2839138182462193e-05}, {"id": 504, "seek": 496200, "start": 4962.0, "end": 4977.0, "text": " So this is like very much the same as just in the discrete setting and in the continuous setting. So now I include it and XR before expectation. And again, I included first of all the definition in our discrete case.", "tokens": [407, 341, 307, 411, 588, 709, 264, 912, 382, 445, 294, 264, 27706, 3287, 293, 294, 264, 10957, 3287, 13, 407, 586, 286, 4090, 309, 293, 1783, 49, 949, 14334, 13, 400, 797, 11, 286, 5556, 700, 295, 439, 264, 7123, 294, 527, 27706, 1389, 13], "temperature": 0.0, "avg_logprob": -0.19246347427368163, "compression_ratio": 1.4794520547945205, "no_speech_prob": 4.754589099320583e-06}, {"id": 505, "seek": 497700, "start": 4977.0, "end": 4994.0, "text": " And now we want to look at our random experiment, experiment of tossing a six sided die. So again, as we knew from the previous slides, our omigas now the set of one, two, three, four, five, and six. Because this is all the possible outcomes that our experiment has.", "tokens": [400, 586, 321, 528, 281, 574, 412, 527, 4974, 5120, 11, 5120, 295, 14432, 278, 257, 2309, 41651, 978, 13, 407, 797, 11, 382, 321, 2586, 490, 264, 3894, 9788, 11, 527, 3406, 328, 296, 586, 264, 992, 295, 472, 11, 732, 11, 1045, 11, 1451, 11, 1732, 11, 293, 2309, 13, 1436, 341, 307, 439, 264, 1944, 10070, 300, 527, 5120, 575, 13], "temperature": 0.0, "avg_logprob": -0.14633339583271682, "compression_ratio": 1.7142857142857142, "no_speech_prob": 6.028800271451473e-06}, {"id": 506, "seek": 497700, "start": 4994.0, "end": 5003.0, "text": " And our x is the random variable, which represents the outcome of the toss. So just kind of the number which is shown on the die.", "tokens": [400, 527, 2031, 307, 264, 4974, 7006, 11, 597, 8855, 264, 9700, 295, 264, 14432, 13, 407, 445, 733, 295, 264, 1230, 597, 307, 4898, 322, 264, 978, 13], "temperature": 0.0, "avg_logprob": -0.14633339583271682, "compression_ratio": 1.7142857142857142, "no_speech_prob": 6.028800271451473e-06}, {"id": 507, "seek": 500300, "start": 5003.0, "end": 5016.0, "text": " And we know that our probability mass function here, like the probability that our random variable equals to a specific element in our omigas equals to one six because we have a fair die.", "tokens": [400, 321, 458, 300, 527, 8482, 2758, 2445, 510, 11, 411, 264, 8482, 300, 527, 4974, 7006, 6915, 281, 257, 2685, 4478, 294, 527, 3406, 328, 296, 6915, 281, 472, 2309, 570, 321, 362, 257, 3143, 978, 13], "temperature": 0.0, "avg_logprob": -0.11054760012133368, "compression_ratio": 1.588235294117647, "no_speech_prob": 5.477798549691215e-05}, {"id": 508, "seek": 500300, "start": 5016.0, "end": 5021.0, "text": " So this is for all elements in our omigas sample space.", "tokens": [407, 341, 307, 337, 439, 4959, 294, 527, 3406, 328, 296, 6889, 1901, 13], "temperature": 0.0, "avg_logprob": -0.11054760012133368, "compression_ratio": 1.588235294117647, "no_speech_prob": 5.477798549691215e-05}, {"id": 509, "seek": 502100, "start": 5021.0, "end": 5035.0, "text": " And now we can just take the sum, which is defining our expectation of our random variable, which is just summing or calculating the weighted average of all possible outcomes. So the outcomes are one, two, three, four, five, and six.", "tokens": [400, 586, 321, 393, 445, 747, 264, 2408, 11, 597, 307, 17827, 527, 14334, 295, 527, 4974, 7006, 11, 597, 307, 445, 2408, 2810, 420, 28258, 264, 32807, 4274, 295, 439, 1944, 10070, 13, 407, 264, 10070, 366, 472, 11, 732, 11, 1045, 11, 1451, 11, 1732, 11, 293, 2309, 13], "temperature": 0.0, "avg_logprob": -0.09019831489114201, "compression_ratio": 1.7230046948356808, "no_speech_prob": 7.200693289632909e-06}, {"id": 510, "seek": 502100, "start": 5035.0, "end": 5043.0, "text": " And the probability for all of them, so the probability mass function as we have a fair die is now one six for all possible outcomes.", "tokens": [400, 264, 8482, 337, 439, 295, 552, 11, 370, 264, 8482, 2758, 2445, 382, 321, 362, 257, 3143, 978, 307, 586, 472, 2309, 337, 439, 1944, 10070, 13], "temperature": 0.0, "avg_logprob": -0.09019831489114201, "compression_ratio": 1.7230046948356808, "no_speech_prob": 7.200693289632909e-06}, {"id": 511, "seek": 504300, "start": 5043.0, "end": 5056.0, "text": " So if you take this sum, then you get a 3.5 as result, which definitely makes sense because we have a fair die. So it has to be the center of all possible outcomes.", "tokens": [407, 498, 291, 747, 341, 2408, 11, 550, 291, 483, 257, 805, 13, 20, 382, 1874, 11, 597, 2138, 1669, 2020, 570, 321, 362, 257, 3143, 978, 13, 407, 309, 575, 281, 312, 264, 3056, 295, 439, 1944, 10070, 13], "temperature": 0.0, "avg_logprob": -0.09301167726516724, "compression_ratio": 1.5528846153846154, "no_speech_prob": 1.4744585314474534e-05}, {"id": 512, "seek": 504300, "start": 5056.0, "end": 5060.0, "text": " So this is one example for our expectation.", "tokens": [407, 341, 307, 472, 1365, 337, 527, 14334, 13], "temperature": 0.0, "avg_logprob": -0.09301167726516724, "compression_ratio": 1.5528846153846154, "no_speech_prob": 1.4744585314474534e-05}, {"id": 513, "seek": 504300, "start": 5060.0, "end": 5067.0, "text": " Okay, the expectation of a random variable has two important properties that we want to have a look on this slide.", "tokens": [1033, 11, 264, 14334, 295, 257, 4974, 7006, 575, 732, 1021, 7221, 300, 321, 528, 281, 362, 257, 574, 322, 341, 4137, 13], "temperature": 0.0, "avg_logprob": -0.09301167726516724, "compression_ratio": 1.5528846153846154, "no_speech_prob": 1.4744585314474534e-05}, {"id": 514, "seek": 506700, "start": 5067.0, "end": 5079.0, "text": " And the first property is related to a constant value. So if you have a constant a, which is just an element of r, then the expectation of this constant is just the constant itself.", "tokens": [400, 264, 700, 4707, 307, 4077, 281, 257, 5754, 2158, 13, 407, 498, 291, 362, 257, 5754, 257, 11, 597, 307, 445, 364, 4478, 295, 367, 11, 550, 264, 14334, 295, 341, 5754, 307, 445, 264, 5754, 2564, 13], "temperature": 0.0, "avg_logprob": -0.12350002664034485, "compression_ratio": 1.6358024691358024, "no_speech_prob": 4.536226697382517e-05}, {"id": 515, "seek": 506700, "start": 5079.0, "end": 5084.0, "text": " And that makes total distance because in a constant you don't have anything random.", "tokens": [400, 300, 1669, 3217, 4560, 570, 294, 257, 5754, 291, 500, 380, 362, 1340, 4974, 13], "temperature": 0.0, "avg_logprob": -0.12350002664034485, "compression_ratio": 1.6358024691358024, "no_speech_prob": 4.536226697382517e-05}, {"id": 516, "seek": 508400, "start": 5084.0, "end": 5104.0, "text": " The second property now is linearity. So the expectation, the operation of expectation is linear. So if we have two random variables here denoted by x and y, and we also have two scalars a and b, and we take a new random variable, which is a times x plus b times y, then we can take the expectation of that.", "tokens": [440, 1150, 4707, 586, 307, 8213, 507, 13, 407, 264, 14334, 11, 264, 6916, 295, 14334, 307, 8213, 13, 407, 498, 321, 362, 732, 4974, 9102, 510, 1441, 23325, 538, 2031, 293, 288, 11, 293, 321, 611, 362, 732, 15664, 685, 257, 293, 272, 11, 293, 321, 747, 257, 777, 4974, 7006, 11, 597, 307, 257, 1413, 2031, 1804, 272, 1413, 288, 11, 550, 321, 393, 747, 264, 14334, 295, 300, 13], "temperature": 0.0, "avg_logprob": -0.07650387914557207, "compression_ratio": 1.7953216374269005, "no_speech_prob": 2.390524241491221e-05}, {"id": 517, "seek": 510400, "start": 5104.0, "end": 5116.0, "text": " And this one has to be equal to a times the expectation of x plus b times the expectation of y. And this shows you the linearity of the expectation operation.", "tokens": [400, 341, 472, 575, 281, 312, 2681, 281, 257, 1413, 264, 14334, 295, 2031, 1804, 272, 1413, 264, 14334, 295, 288, 13, 400, 341, 3110, 291, 264, 8213, 507, 295, 264, 14334, 6916, 13], "temperature": 0.0, "avg_logprob": -0.07913072689159496, "compression_ratio": 1.8224852071005917, "no_speech_prob": 1.2328461707511451e-05}, {"id": 518, "seek": 510400, "start": 5116.0, "end": 5125.0, "text": " And this is very important. So as a hint, you have to use that in the exercise sheet and it will be also very important in the future in the lecture.", "tokens": [400, 341, 307, 588, 1021, 13, 407, 382, 257, 12075, 11, 291, 362, 281, 764, 300, 294, 264, 5380, 8193, 293, 309, 486, 312, 611, 588, 1021, 294, 264, 2027, 294, 264, 7991, 13], "temperature": 0.0, "avg_logprob": -0.07913072689159496, "compression_ratio": 1.8224852071005917, "no_speech_prob": 1.2328461707511451e-05}, {"id": 519, "seek": 512500, "start": 5125.0, "end": 5142.0, "text": " Okay, so now we want to have a look at the variance of a random variable and the idea of the variances, roughly speaking, is just a measure how concentrated the distribution of a random variable x is around its expectation or mean.", "tokens": [1033, 11, 370, 586, 321, 528, 281, 362, 257, 574, 412, 264, 21977, 295, 257, 4974, 7006, 293, 264, 1558, 295, 264, 1374, 21518, 11, 9810, 4124, 11, 307, 445, 257, 3481, 577, 21321, 264, 7316, 295, 257, 4974, 7006, 2031, 307, 926, 1080, 14334, 420, 914, 13], "temperature": 0.0, "avg_logprob": -0.163480648627648, "compression_ratio": 1.582191780821918, "no_speech_prob": 6.961370672797784e-05}, {"id": 520, "seek": 514200, "start": 5142.0, "end": 5158.0, "text": " The expectation and mean is just the same. So you want to know how much how dense it is around its mean and how much it's spreading also. And the definition of a variance of the variance of a random variable is now given by this term.", "tokens": [440, 14334, 293, 914, 307, 445, 264, 912, 13, 407, 291, 528, 281, 458, 577, 709, 577, 18011, 309, 307, 926, 1080, 914, 293, 577, 709, 309, 311, 15232, 611, 13, 400, 264, 7123, 295, 257, 21977, 295, 264, 21977, 295, 257, 4974, 7006, 307, 586, 2212, 538, 341, 1433, 13], "temperature": 0.0, "avg_logprob": -0.12749094529585406, "compression_ratio": 1.5194805194805194, "no_speech_prob": 1.4496083167614415e-05}, {"id": 521, "seek": 515800, "start": 5158.0, "end": 5173.0, "text": " You just need to compute the expectation of x squared, then you need to know the expectation of x and you need to take this to the power of two. And then you just combine it and subtract it and then you get the variance of x.", "tokens": [509, 445, 643, 281, 14722, 264, 14334, 295, 2031, 8889, 11, 550, 291, 643, 281, 458, 264, 14334, 295, 2031, 293, 291, 643, 281, 747, 341, 281, 264, 1347, 295, 732, 13, 400, 550, 291, 445, 10432, 309, 293, 16390, 309, 293, 550, 291, 483, 264, 21977, 295, 2031, 13], "temperature": 0.0, "avg_logprob": -0.13297795366357873, "compression_ratio": 1.6917293233082706, "no_speech_prob": 1.1217681276320945e-05}, {"id": 522, "seek": 517300, "start": 5173.0, "end": 5188.0, "text": " On the figure that I'm including here, you can see different probability measures of a normal distribution. And here the red and the blue one are both normal distributions with a mean around zero.", "tokens": [1282, 264, 2573, 300, 286, 478, 3009, 510, 11, 291, 393, 536, 819, 8482, 8000, 295, 257, 2710, 7316, 13, 400, 510, 264, 2182, 293, 264, 3344, 472, 366, 1293, 2710, 37870, 365, 257, 914, 926, 4018, 13], "temperature": 0.0, "avg_logprob": -0.08973474618865222, "compression_ratio": 1.727699530516432, "no_speech_prob": 2.2972194528847467e-06}, {"id": 523, "seek": 517300, "start": 5188.0, "end": 5199.0, "text": " And you can see that the red one has a variance which is much bigger than the one and the blue one. So it's much less concentrated around its mean and it's spreading more.", "tokens": [400, 291, 393, 536, 300, 264, 2182, 472, 575, 257, 21977, 597, 307, 709, 3801, 813, 264, 472, 293, 264, 3344, 472, 13, 407, 309, 311, 709, 1570, 21321, 926, 1080, 914, 293, 309, 311, 15232, 544, 13], "temperature": 0.0, "avg_logprob": -0.08973474618865222, "compression_ratio": 1.727699530516432, "no_speech_prob": 2.2972194528847467e-06}, {"id": 524, "seek": 519900, "start": 5199.0, "end": 5208.0, "text": " The blue one is more dense around the mean. So here you can see kind of the difference what it is when you have a bigger variance or smaller variance.", "tokens": [440, 3344, 472, 307, 544, 18011, 926, 264, 914, 13, 407, 510, 291, 393, 536, 733, 295, 264, 2649, 437, 309, 307, 562, 291, 362, 257, 3801, 21977, 420, 4356, 21977, 13], "temperature": 0.0, "avg_logprob": -0.061916961902525366, "compression_ratio": 1.721951219512195, "no_speech_prob": 8.308805263368413e-06}, {"id": 525, "seek": 519900, "start": 5208.0, "end": 5219.0, "text": " And now let's have a look also at an example of the computation of the variance of a random variable. And again, we want to have a look at the same example that we have been looking for the expectation.", "tokens": [400, 586, 718, 311, 362, 257, 574, 611, 412, 364, 1365, 295, 264, 24903, 295, 264, 21977, 295, 257, 4974, 7006, 13, 400, 797, 11, 321, 528, 281, 362, 257, 574, 412, 264, 912, 1365, 300, 321, 362, 668, 1237, 337, 264, 14334, 13], "temperature": 0.0, "avg_logprob": -0.061916961902525366, "compression_ratio": 1.721951219512195, "no_speech_prob": 8.308805263368413e-06}, {"id": 526, "seek": 521900, "start": 5219.0, "end": 5229.0, "text": " So tossing a fair, six sided die. So the setting is exactly the same. We have our sample space, omega, we have our random variable, which is a discrete random variable.", "tokens": [407, 14432, 278, 257, 3143, 11, 2309, 41651, 978, 13, 407, 264, 3287, 307, 2293, 264, 912, 13, 492, 362, 527, 6889, 1901, 11, 10498, 11, 321, 362, 527, 4974, 7006, 11, 597, 307, 257, 27706, 4974, 7006, 13], "temperature": 0.0, "avg_logprob": -0.17002795578597427, "compression_ratio": 1.6868686868686869, "no_speech_prob": 1.3072280125925317e-05}, {"id": 527, "seek": 521900, "start": 5229.0, "end": 5239.0, "text": " And we already know the probability mass function, which was a uniform distribution. And we also know the expectation, which we have been computing a few slides ago.", "tokens": [400, 321, 1217, 458, 264, 8482, 2758, 2445, 11, 597, 390, 257, 9452, 7316, 13, 400, 321, 611, 458, 264, 14334, 11, 597, 321, 362, 668, 15866, 257, 1326, 9788, 2057, 13], "temperature": 0.0, "avg_logprob": -0.17002795578597427, "compression_ratio": 1.6868686868686869, "no_speech_prob": 1.3072280125925317e-05}, {"id": 528, "seek": 523900, "start": 5239.0, "end": 5250.0, "text": " So what we now have to do is we have to compute the expectation of x squared. And then we have to combine it in order to obtain our variance of our random variable x.", "tokens": [407, 437, 321, 586, 362, 281, 360, 307, 321, 362, 281, 14722, 264, 14334, 295, 2031, 8889, 13, 400, 550, 321, 362, 281, 10432, 309, 294, 1668, 281, 12701, 527, 21977, 295, 527, 4974, 7006, 2031, 13], "temperature": 0.0, "avg_logprob": -0.10908932156032985, "compression_ratio": 1.620253164556962, "no_speech_prob": 2.6902253011940047e-05}, {"id": 529, "seek": 523900, "start": 5250.0, "end": 5257.0, "text": " So I just let it to you to understand these steps, but please try to understand them and.", "tokens": [407, 286, 445, 718, 309, 281, 291, 281, 1223, 613, 4439, 11, 457, 1767, 853, 281, 1223, 552, 293, 13], "temperature": 0.0, "avg_logprob": -0.10908932156032985, "compression_ratio": 1.620253164556962, "no_speech_prob": 2.6902253011940047e-05}, {"id": 530, "seek": 525700, "start": 5257.0, "end": 5270.0, "text": " Okay, so last but not least also the variance has some important properties that I want to name here pretty quickly. So the first one again, we're looking at a constant, which is nothing random about it.", "tokens": [1033, 11, 370, 1036, 457, 406, 1935, 611, 264, 21977, 575, 512, 1021, 7221, 300, 286, 528, 281, 1315, 510, 1238, 2661, 13, 407, 264, 700, 472, 797, 11, 321, 434, 1237, 412, 257, 5754, 11, 597, 307, 1825, 4974, 466, 309, 13], "temperature": 0.0, "avg_logprob": -0.11653558278487901, "compression_ratio": 1.496969696969697, "no_speech_prob": 6.257814675336704e-05}, {"id": 531, "seek": 525700, "start": 5270.0, "end": 5274.0, "text": " So the variance of a constant is just zero.", "tokens": [407, 264, 21977, 295, 257, 5754, 307, 445, 4018, 13], "temperature": 0.0, "avg_logprob": -0.11653558278487901, "compression_ratio": 1.496969696969697, "no_speech_prob": 6.257814675336704e-05}, {"id": 532, "seek": 527400, "start": 5274.0, "end": 5295.0, "text": " Which makes also interpretation wise a lot of sense. And now if you look at random variable x and you take a new random variable, which is defined as a times x plus b, you obtain for the variance of this new variable that it's equal to a to the power of two times the variance of x.", "tokens": [3013, 1669, 611, 14174, 10829, 257, 688, 295, 2020, 13, 400, 586, 498, 291, 574, 412, 4974, 7006, 2031, 293, 291, 747, 257, 777, 4974, 7006, 11, 597, 307, 7642, 382, 257, 1413, 2031, 1804, 272, 11, 291, 12701, 337, 264, 21977, 295, 341, 777, 7006, 300, 309, 311, 2681, 281, 257, 281, 264, 1347, 295, 732, 1413, 264, 21977, 295, 2031, 13], "temperature": 0.0, "avg_logprob": -0.12252999775445284, "compression_ratio": 1.6588235294117648, "no_speech_prob": 2.4904442398110405e-05}, {"id": 533, "seek": 529500, "start": 5295.0, "end": 5312.0, "text": " So the constant part in our term is just vanishing and the scalar, which is multiplied or the scalar, which is used to be multiplied to our random variable is getting outside, but you have to take it to the power of two.", "tokens": [407, 264, 5754, 644, 294, 527, 1433, 307, 445, 3161, 3807, 293, 264, 39684, 11, 597, 307, 17207, 420, 264, 39684, 11, 597, 307, 1143, 281, 312, 17207, 281, 527, 4974, 7006, 307, 1242, 2380, 11, 457, 291, 362, 281, 747, 309, 281, 264, 1347, 295, 732, 13], "temperature": 0.0, "avg_logprob": -0.08809912772405715, "compression_ratio": 1.7695852534562213, "no_speech_prob": 1.9291703210910782e-05}, {"id": 534, "seek": 529500, "start": 5312.0, "end": 5322.0, "text": " Okay, so again, I also included the image of our normal distributions here, which is showing you different normal distributions with different means and variances.", "tokens": [1033, 11, 370, 797, 11, 286, 611, 5556, 264, 3256, 295, 527, 2710, 37870, 510, 11, 597, 307, 4099, 291, 819, 2710, 37870, 365, 819, 1355, 293, 1374, 21518, 13], "temperature": 0.0, "avg_logprob": -0.08809912772405715, "compression_ratio": 1.7695852534562213, "no_speech_prob": 1.9291703210910782e-05}, {"id": 535, "seek": 532200, "start": 5322.0, "end": 5335.0, "text": " On this final slide, we summarized the most important probability distribution and we included to discrete ones, the Bernoulli and the Vynomial distribution and to continuous one in the uniform and the normal distribution.", "tokens": [1282, 341, 2572, 4137, 11, 321, 14611, 1602, 264, 881, 1021, 8482, 7316, 293, 321, 5556, 281, 27706, 2306, 11, 264, 10781, 263, 16320, 293, 264, 691, 2534, 47429, 7316, 293, 281, 10957, 472, 294, 264, 9452, 293, 264, 2710, 7316, 13], "temperature": 0.0, "avg_logprob": -0.15221005565715287, "compression_ratio": 1.8778625954198473, "no_speech_prob": 2.236825275758747e-05}, {"id": 536, "seek": 532200, "start": 5335.0, "end": 5343.0, "text": " In our lecture, the normal distribution will by far be the most important one. So make sure that you have the information in this table in mind.", "tokens": [682, 527, 7991, 11, 264, 2710, 7316, 486, 538, 1400, 312, 264, 881, 1021, 472, 13, 407, 652, 988, 300, 291, 362, 264, 1589, 294, 341, 3199, 294, 1575, 13], "temperature": 0.0, "avg_logprob": -0.15221005565715287, "compression_ratio": 1.8778625954198473, "no_speech_prob": 2.236825275758747e-05}, {"id": 537, "seek": 534300, "start": 5343.0, "end": 5352.0, "text": " And with this slide, we reach the end of our second tutorial session. Thank you so much for listening and see you next week.", "tokens": [50364, 400, 365, 341, 4137, 11, 321, 2524, 264, 917, 295, 527, 1150, 7073, 5481, 13, 1044, 291, 370, 709, 337, 4764, 293, 536, 291, 958, 1243, 13, 50814], "temperature": 0.0, "avg_logprob": -0.0897550900777181, "compression_ratio": 1.2156862745098038, "no_speech_prob": 2.2677748347632587e-05}], "language": "en"}